<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>《动手学深度学习》 Pytorch ver. Part B</title>
      <link href="/2022/09/15/dive-into-dl-pytorch-B/"/>
      <url>/2022/09/15/dive-into-dl-pytorch-B/</url>
      
        <content type="html"><![CDATA[<ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构：</p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg"><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="封面"></a></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域，reta 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>Part B 包含：</p><ul><li>§ 5 CNN<ul><li>基本概念：卷积层、填充与步长、多通道、池化、批量归一化</li><li>模型的例子：LeNet、AlexNet、VGG、NiN、GoogLeNet、ResNet、DenseNet</li></ul></li><li>§ 6 RNN<ul><li>语言模型及其计算，N-gram 的概念</li><li>RNN 基本模型及其实现，字符数据集的制作</li><li>GRU, LSTM 的原理</li><li>Deep-RNN, bi-RNN</li></ul></li></ul><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><ul><li><strong>二维互相关运算</strong></li></ul><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.1_correlation.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.1_correlation.svg" alt="img"></a></p><p>如图所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为 $3×3$ 或$（3，3）$。</p><p>核数组的高和宽分别为 2。该数组在卷积计算中又称卷积核或过滤器（filter）。</p><p>卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即 $2×2$。</p><p>图中的阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：</p><p>$0×0+1×1+3×2+4×3=19$。</p><p>在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，按照特定的步长，依次在输入数组上滑动。</p><p>当卷积窗口滑动到某一位置时，窗口中的输入子数组与核数组按元素相乘并求和，得到输出数组中相应位置的元素。</p><ul><li><strong>从互相关运算到卷积运算</strong></li></ul><p>实际上，卷积运算与互相关运算类似。<strong>为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算</strong>。</p><ul><li><strong>Feature Map 与 Receptive Field</strong></li></ul><p>二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。</p><p>影响元素 x 的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的 x 感受野（receptive field）。</p><p>以图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。</p><p>我们将图中形状为 $2×2$ 的输出记为 $Y$，并考虑一个更深的卷积神经网络：将 $Y$ 与另一个形状为 $2×2$ 的核数组做互相关运算，输出单个元素$z$。那么，$z$ 在 $Y$ 上的 Receptive Field 为 $Y$ 的全部四个元素，在 $x$ 上的感受野包括其中全部 9 个元素。</p><p>可见，我们可以<strong>通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征</strong>。</p><h4 id="Padding-amp-Stride"><a href="#Padding-amp-Stride" class="headerlink" title="Padding &amp; Stride"></a>Padding &amp; Stride</h4><p>本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。</p><ul><li><strong>Padding</strong></li></ul><p>填充（padding）是指在输入高和宽的两侧填充元素（通常是 0 元素）。</p><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_pad.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_pad.svg" alt="img"></a></p><p>图中我们在原输入高和宽的两侧分别添加了值为 0 的元素，使得输入高和宽从 3 变成了 5 ，并导致输出高和宽由 2 增加到 4。</p><p>一般来说，如果在高的两侧一共填充 $p_h$ 行，在宽的两侧一共填充 $p_w$ 列，在很多情况下，我们会设置 $p_h=k_h−1$ 和 $p_w=k_w−1$ 来使输入和输出具有相同的高和宽，其中 $k_h×k_w$ 是卷积核窗口形状。这样会方便在构造网络时推测每个层的输出形状。</p><p>假设这里 $k_h$ 是奇数，我们会在高的两侧分别填充 $p_h/2$ 行。如果 $k_h$ 是偶数，一种可能是在输入的顶端一侧填充 $⌈p_h/2⌉$ 行，而在底端一侧填充 $⌊p_h/2⌋$ 行。在宽的两侧填充同理。卷积神经网络经常使用<strong>奇数高宽的卷积核</strong> $k_h×k_w$，如 1、3、5 和 7，所以两端上的填充个数相等。</p><p>对任意的二维数组 <code>X</code>，设它的第 <code>i</code> 行第 <code>j</code> 列的元素为 <code>X[i,j]</code>。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出 <code>Y[i,j]</code> 是由输入以 <code>X[i,j]</code> 为中心的窗口同卷积核进行互相关计算得到的。</p><ul><li><strong>Stride</strong></li></ul><p>在上一节里我们介绍了二维互相关运算。卷积窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。</p><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_stride.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.2_conv_stride.svg" alt="img"></a></p><p>目前我们看到的例子里，在高和宽两个方向上步幅均为1。我们也可以使用更大步幅。</p><p>图中展示了在高上步幅为 3、在宽上步幅为 2 的二维互相关运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了 3 行，而在输出第一行第二个元素时卷积窗口向右滑动了 2 列。当卷积窗口在输入上再向右滑动 2 列时，由于输入元素无法填满窗口，无结果输出。</p><h4 id="多通道"><a href="#多通道" class="headerlink" title="多通道"></a>多通道</h4><p>前面两节里我们用到的输入和输出都是二维数组，但真实数据的维度经常更高。</p><p>例如，彩色图像在高和宽 2 个维度外还有 RGB（红、绿、蓝）3 个颜色通道。</p><p>假设彩色图像的高和宽分别是 $h$ 和 $w$（像素），那么它可以表示为一个 $3×h×w$ 的多维数组。</p><p>我们将大小为 3 的这一维称为通道（channel）维。</p><p>本节我们将介绍含多个输入通道或多个输出通道的卷积核。</p><ul><li><strong>多输入通道</strong></li></ul><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_multi_in.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_multi_in.svg" alt="img"></a></p><p>当输入数据含多个通道时，我们需要构造一个输入通道数与输入数据的通道数相同的卷积核，从而能够与含多通道的输入数据做互相关运算。</p><p>含多个通道的输入数据与多输入通道的卷积核做二维互相关运算的输出：在各个通道上对输入的二维数组和卷积核的二维核数组做互相关运算，再将这些互相关运算的输出相加。</p><ul><li><strong>多输出通道</strong></li></ul><p>当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为 1。设卷积核输入通道数和输出通道数分别为 $c_i$ 和 $c_o$，高和宽分别为 $k_h$ 和 $k_w$。</p><p>如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为 $c_i×k_h×k_w$ 的核数组。将它们在输出通道维上连结，卷积核的形状即 $c_o×c_i×k_h×k_w$。</p><ul><li><strong>1 x 1 卷积层</strong></li></ul><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_1x1.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.3_conv_1x1.svg" alt="img"></a></p><p>因为使用了最小窗口，$1×1$ 卷积失去了卷积层可以识别高和宽维度上相邻元素构成的模式的功能。实际上，$1×1$ 卷积的主要计算发生在通道维上。</p><p>值得注意的是，输入和输出具有相同的高和宽。输出中的每个元素来自输入中在高和宽上相同位置的元素<strong>在不同通道之间的按权重累加</strong>。</p><p>假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，<strong>那么 $1×1$ 卷积层的作用与全连接层等价</strong>。</p><h4 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h4><p>设任意二维数组 <code>X</code> 的 <code>i</code>行 <code>j</code> 列的元素为 <code>X[i, j]</code>。如果我们构造的 $1×2$卷积核 [1,−1] 输出 <code>Y[i, j]=1</code>，那么说明输入中 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 数值不一样。这可能意味着物体边缘通过这两个元素之间。</p><p>实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出 <code>Y</code> 中的不同位置，进而对后面的模式识别造成不便。</p><p>在本节中我们介绍池化（pooling）层，它的提出是<strong>为了缓解卷积层对位置的过度敏感性</strong>。</p><ul><li><strong>2D-MaxPooling &amp; Mean Pooling</strong></li></ul><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.4_pooling.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.4_pooling.svg" alt="img"></a></p><p>同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。</p><p>不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。</p><p>让我们再次回到本节开始提到的物体边缘检测的例子。现在我们将卷积层的输出作为 $2×2$ 最大池化的输入。设该卷积层输入是 <code>X</code>、池化层输出为 <code>Y</code>。无论是 <code>X[i, j]</code> 和 <code>X[i, j+1]</code> 值不同，还是 <code>X[i, j+1]</code> 和 <code>X[i, j+2]</code> 不同，池化层输出均有 <code>Y[i, j]=1</code>。也就是说，使用 $2×2$ 最大池化层时，只要卷积层识别的模式在高和宽上移动不超过一个元素，我们依然可以将它检测出来。</p><ul><li><strong>Padding &amp; Stride</strong></li></ul><p>同卷积层一样，池化层也可以在输入的高和宽两侧的填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。我们将通过 <code>nn</code> 模块里的二维最大池化层 <code>MaxPool2d</code> 来演示池化层填充和步幅的工作机制。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.<span class="built_in">float</span>).view((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)) <span class="comment"># C_o * C_i * K_h * K_w</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 0.,  1.,  2.,  3.],</span><br><span class="line">          [ 4.,  5.,  6.,  7.],</span><br><span class="line">          [ 8.,  9., 10., 11.],</span><br><span class="line">          [12., 13., 14., 15.]]]])</span><br></pre></td></tr></table></figure><p>默认情况下，<code>MaxPool2d</code> 实例里步幅和池化窗口形状相同。下面使用形状为 (3,3) 的池化窗口，默认获得形状为 (3,3) 的步幅。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X) </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[10.]]]])</span><br></pre></td></tr></table></figure><p>我们可以手动指定步幅和填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]]]])</span><br></pre></td></tr></table></figure><p>当然，我们也可以指定非正方形的池化窗口，并分别指定高和宽上的填充和步幅。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">4</span>), padding=(<span class="number">1</span>, <span class="number">2</span>), stride=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 1.,  3.],</span><br><span class="line">          [ 9., 11.],</span><br><span class="line">          [13., 15.]]]])</span><br></pre></td></tr></table></figure><ul><li><strong>多通道</strong></li></ul><p>在处理多通道输入数据时，<strong>池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加</strong>。</p><p>这意味着池化层的输出通道数与输入通道数相等。下面将数组 <code>X</code> 和 <code>X+1</code> 在通道维上连结来构造通道数为 2 的输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = torch.cat((X, X + <span class="number">1</span>), dim=<span class="number">1</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 0.,  1.,  2.,  3.],</span><br><span class="line">          [ 4.,  5.,  6.,  7.],</span><br><span class="line">          [ 8.,  9., 10., 11.],</span><br><span class="line">          [12., 13., 14., 15.]],</span><br><span class="line">         [[ 1.,  2.,  3.,  4.],</span><br><span class="line">          [ 5.,  6.,  7.,  8.],</span><br><span class="line">          [ 9., 10., 11., 12.],</span><br><span class="line">          [13., 14., 15., 16.]]]])</span><br></pre></td></tr></table></figure><p>池化后，我们发现输出通道数仍然是2。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>, padding=<span class="number">1</span>, stride=<span class="number">2</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[ 5.,  7.],</span><br><span class="line">          [13., 15.]],</span><br><span class="line">         [[ 6.,  8.],</span><br><span class="line">          [14., 16.]]]])</span><br></pre></td></tr></table></figure><h4 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h4><p>本节我们介绍批量归一化（batch normalization）层，它能让较深的神经网络的训练变得更加容易。</p><ul><li>为什么要有 Batch Normalization?</li></ul><p>在预测回归问题里，我们对输入数据做了标准化处理：处理后的任意一个特征在数据集中所有样本上的均值为 0、标准差为 1。标准化处理输入数据使各个特征的分布相近：这往往更容易训练出有效的模型。</p><p>通常来说，数据标准化预处理对于浅层模型就足够有效了。随着模型训练的进行，当每层中参数更新时，靠近输出层的输出较难出现剧烈变化。但对深层神经网络来说，即使输入数据已做标准化，训练中模型参数的更新依然很容易造成靠近输出层输出的剧烈变化。这种计算数值的不稳定性通常令我们难以训练出有效的深度模型。</p><p>批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。<strong>批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路</strong>。</p><ul><li>怎么做 Batch Normalization?</li></ul><p>对全连接层和卷积层做批量归一化的方法稍有不同。下面我们将分别介绍这两种情况下的批量归一化。</p><p><strong>对 Fully Connected Layer 的 Batch Normalization</strong></p><p>我们先考虑如何对全连接层做批量归一化。通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。设全连接层的输入为 $u$，权重参数和偏差参数分别为 $W$ 和 $b$，激活函数为 $ϕ$。设批量归一化的运算符为 $BN$。那么，使用批量归一化的全连接层的输出为 $ϕ(BN(Wu+b))$。</p><p>下面我们解释 $BN$ 算符是什么。</p><p>考虑一个由 $m$ 个样本组成的 Mini-batch，仿射变换的输出为一个新的 Mini-batch $\mathcal{B}=x^{(1)},…,x^{(m)}$。它们正是批量归一化层的输入。对于小批量 $\mathcal{B}$ 中任意样本 $x^{(i)}∈\mathbb{R}^d,1≤i≤m$，批量归一化层的输出同样是 $d$ 维向量$y^{(i)}=BN(x^{(i)})$，并由以下几步求得。</p><script type="math/tex; mode=display">μ_\mathcal{B}←\frac{1}{m}\sum_{i=1}^{m}x^{(i)}σ_\mathcal{B}^2←\frac{1}{m}\sum_{i=1}^{m}(x^{(i)}−μ_\mathcal{B})^2\hat{x}^{(i)}←\frac{x^{(i)}−μ_\mathcal{B}}{\sqrt{σ_\mathcal{B}^2+ϵ}}</script><p>这里 $ϵ&gt;0$ 是一个很小的常数，是为了保证分母大于 0。在上面标准化的基础上，批量归一化层引入了两个可以学习的模型参数，拉伸（scale）参数 $γ$ 和偏移（shift）参数 $β$。这两个参数和 $x^{(i)}$ 形状相同，皆为 $d$ 维向量。它们与 $x^{(i)}$ 分别做 Hadamard Product（符号$⊙$）和加法计算：</p><script type="math/tex; mode=display">y^{(i)}←γ⊙\hat{x}^{(i)}+β</script><p>至此，我们得到了 $x^{(i)}$ 的批量归一化的输出 $y^{(i)}$。值得注意的是，可学习的拉伸和偏移参数保留了不对 $\hat{x}^{(i)}$ 做批量归一化的可能：此时只需学出 $γ=\sqrt{σ_\mathcal{B}^2+ϵ}$ 和 $β=μB$。我们可以对此这样理解：如果批量归一化无益，理论上，学出的模型可以不使用批量归一化。</p><p><strong>对 Conv. Layer 的 Batch Normalization</strong></p><p>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。</p><p>如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且<strong>每个通道都拥有独立的拉伸和偏移参数，并均为标量</strong>。</p><p>设小批量中有 $m$ 个样本，在单个通道上，假设卷积计算输出的高和宽分别为 $p$ 和 $q$。我们需要对该通道中 $m×p×q$ 个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中 $m×p×q$ 个元素的均值和方差。</p><p><strong>预测时的 Batch Normalization</strong></p><p>使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。将训练好的模型用于预测时，我们希望模型对于任意输入都有确定的输出。因此，<strong>单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差</strong>。一种常用的方法是通过移动平均<strong>估算整个训练数据集的样本均值和方差</strong>，并在预测时使用它们得到确定的输出。可见，和丢弃层一样，批量归一化层在训练模式和预测模式下的计算结果也是不一样的。</p><ul><li>实现（Simple ver.）</li></ul><p>与我们刚刚自己定义的 <code>BatchNorm</code> 类相比，Pytorch 中 <code>nn</code> 模块定义的 <code>BatchNorm1d</code> 和 <code>BatchNorm2d</code> 类使用起来更加简单，二者分别用于全连接层和卷积层，都需要指定输入的 <code>num_features</code> 参数值。下面我们用 PyTorch 实现使用批量归一化的 LeNet。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.BatchNorm2d(<span class="number">6</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">    </span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">    </span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">    </span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    </span><br><span class="line">            d2l.FlattenLayer(),</span><br><span class="line">    </span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">    </span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.BatchNorm1d(<span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">    </span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><h3 id="CNN-的例子"><a href="#CNN-的例子" class="headerlink" title="CNN 的例子"></a>CNN 的例子</h3><h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>之前我们曾使用 MLP 对 Fashion-MNIST 数据集中的图像进行分类。每张图像高和宽均是 28 像素。我们将图像中的像素逐行展开，得到长度为 784 的向量，并输入进全连接层中。然而，这种分类方法有一定的局限性。</p><ol><li>图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别。</li><li>对于大尺寸的输入图像，使用全连接层容易造成模型过大。假设输入是高和宽均为 1000 像素的彩色照片（含 3 个通道）。即使全连接层输出个数仍是 256，该层权重参数的形状是$3,000,000×256$：它占用了大约 3 GB 的内存或显存。这带来过复杂的模型和过高的存储开销。</li></ol><p>卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。</p><p>卷积神经网络就是含卷积层的网络。本节里我们将介绍一个早期用来识别手写数字图像的卷积神经网络：LeNet。</p><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.5_lenet.png"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.5_lenet.png" alt="img"></a></p><p>LeNet分为卷积层块和全连接层块两个部分。下面我们分别介绍这两个模块。</p><p>卷积层块里的基本单位是<strong>卷积层后接最大池化层</strong>：</p><ul><li>卷积层用来识别图像里的空间模式，如线条和物体局部</li><li>之后的最大池化层则用来降低卷积层对位置的敏感性</li></ul><p>卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用 5×5 的窗口，并在输出上使用 sigmoid 激活函数。第一个卷积层输出通道数为 6，第二个卷积层输出通道数则增加到 16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为 2×2，且步幅为 2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。</p><p>卷积层块的输出形状为 (批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含 3 个全连接层。它们的输出个数分别是 120、84 和 10，其中 10 为输出的类别个数。</p><p>下面我们通过 <code>Sequential</code> 类来实现 LeNet 模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet, self).__init__()</span><br><span class="line">        self.conv = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>), <span class="comment"># in_channels, out_channels, kernel_size</span></span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>), <span class="comment"># kernel_size, stride</span></span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">120</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.Sigmoid(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, img</span>):</span></span><br><span class="line">        feature = self.conv(img)</span><br><span class="line">        output = self.fc(feature.view(img.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>这里使用 GPU 进行计算，对相关函数的修改如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_ch5</span>(<span class="params">net, train_iter, test_iter, batch_size, optimizer, device, num_epochs</span>):</span></span><br><span class="line">    net = net.to(device)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;training on &quot;</span>, device)</span><br><span class="line">    loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum, train_acc_sum, n, batch_count, start = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span>, <span class="number">0</span>, time.time()</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            y = y.to(device)</span><br><span class="line">            y_hat = net(X)</span><br><span class="line">            l = loss(y_hat, y)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            l.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            train_l_sum += l.cpu().item()</span><br><span class="line">            train_acc_sum += (y_hat.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">            batch_count += <span class="number">1</span></span><br><span class="line">        test_acc = evaluate_accuracy(test_iter, net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用。该函数将被逐步改进。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">data_iter, net, device=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> device <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">        <span class="comment"># 如果没指定device就使用net的device</span></span><br><span class="line">        device = <span class="built_in">list</span>(net.parameters())[<span class="number">0</span>].device</span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(net, torch.nn.Module):</span><br><span class="line">                net.<span class="built_in">eval</span>() <span class="comment"># 评估模式, 这会关闭 dropout</span></span><br><span class="line">                acc_sum += (net(X.to(device)).argmax(dim=<span class="number">1</span>) == y.to(device)).<span class="built_in">float</span>().<span class="built_in">sum</span>().cpu().item()</span><br><span class="line">                net.train() <span class="comment"># 改回训练模式</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment"># 自定义的模型, 3.13节之后不会用到, 不考虑GPU</span></span><br><span class="line">                <span class="keyword">if</span>(<span class="string">&#x27;is_training&#x27;</span> <span class="keyword">in</span> net.__code__.co_varnames): <span class="comment"># 如果有 is_training 这个参数</span></span><br><span class="line">                    <span class="comment"># 将 is_training 设置成 False</span></span><br><span class="line">                    acc_sum += (net(X, is_training=<span class="literal">False</span>).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item() </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item() </span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br></pre></td></tr></table></figure><h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.6_alexnet.png"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.6_alexnet.png" alt="img"></a></p><ul><li>Larger parameter size</li><li>Use ReLU instead of sigmoid</li><li>Introducing Dropout</li><li>Data augmentation</li></ul><h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><p>VGG 提出了可以通过重复使用简单的基础块来构建深度模型的思路。</p><p>VGG块的组成规律是：连续使用数个相同的填充为 1、窗口形状为 3×3 的卷积层后接上一个步幅为 2、窗口形状为 $2×2$ 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。我们使用<code>vgg_block</code>函数来实现这个基础的VGG 块，它可以指定卷积层的数量和输入输出通道数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg_block</span>(<span class="params">num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">        blk.append(nn.ReLU())</span><br><span class="line">    blk.append(nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)) <span class="comment"># 这里会使宽高减半</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br></pre></td></tr></table></figure><p>现在我们构造一个 VGG 网络。它有 5 个 <code>vgg_block</code>，前 2 块使用单卷积层 <code>num_convs=1</code>，而后 3 块使用双卷积层 <code>num_convs=2</code>。第一块的输入输出通道分别是 1（因为下面要使用的 Fashion-MNIST 数据的通道数为 1）和 64，之后每次对输出通道数翻倍，直到变为 512。因为这个网络使用了 8 个卷积层和 3 个全连接层，所以经常被称为 VGG-11。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">conv_arch = ((<span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>), (<span class="number">1</span>, <span class="number">64</span>, <span class="number">128</span>), (<span class="number">2</span>, <span class="number">128</span>, <span class="number">256</span>), (<span class="number">2</span>, <span class="number">256</span>, <span class="number">512</span>), (<span class="number">2</span>, <span class="number">512</span>, <span class="number">512</span>))</span><br><span class="line"><span class="comment"># 经过5个 vgg_block, 宽高会减半5次, 变成 224/32 = 7</span></span><br><span class="line">fc_features = <span class="number">512</span> * <span class="number">7</span> * <span class="number">7</span> <span class="comment"># c * w * h</span></span><br><span class="line">fc_hidden_units = <span class="number">4096</span> <span class="comment"># 任意</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vgg</span>(<span class="params">conv_arch, fc_features, fc_hidden_units=<span class="number">4096</span></span>):</span></span><br><span class="line">    net = nn.Sequential()</span><br><span class="line">    <span class="comment"># 卷积层部分</span></span><br><span class="line">    <span class="keyword">for</span> i, (num_convs, in_channels, out_channels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(conv_arch):</span><br><span class="line">        <span class="comment"># 每经过一个vgg_block都会使宽高减半</span></span><br><span class="line">        net.add_module(<span class="string">&quot;vgg_block_&quot;</span> + <span class="built_in">str</span>(i+<span class="number">1</span>), vgg_block(num_convs, in_channels, out_channels))</span><br><span class="line">    <span class="comment"># 全连接层部分</span></span><br><span class="line">    net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(d2l.FlattenLayer(),</span><br><span class="line">                                 nn.Linear(fc_features, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, fc_hidden_units),</span><br><span class="line">                                 nn.ReLU(),</span><br><span class="line">                                 nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">                                 nn.Linear(fc_hidden_units, <span class="number">10</span>)</span><br><span class="line">                                ))</span><br><span class="line">    <span class="keyword">return</span> net</span><br></pre></td></tr></table></figure><h4 id="NiN"><a href="#NiN" class="headerlink" title="NiN"></a>NiN</h4><p>前几节介绍的 LeNet、AlexNet 和 VGG 在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。其中，AlexNet 和 VGG 对 LeNet 的改进主要在于如何对这两个模块加宽（增加通道数）和加深。</p><p>本节我们介绍网络中的网络（NiN）。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。NiN使用 $1×1$ 卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层中去。</p><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.8_nin.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.8_nin.svg" alt="img"></a></p><p>左图是 AlexNet 和 VGG 的网络结构局部，右图是 NiN 的网络结构局部。</p><p><strong>NiN 块</strong>是 NiN 中的基础块。它由一个卷积层加两个充当全连接层的 $1×1$ 卷积层串联而成。其中第一个卷积层的超参数可以自行设置，而第二和第三个卷积层的超参数一般是固定的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nin_block</span>(<span class="params">in_channels, out_channels, kernel_size, stride, padding</span>):</span></span><br><span class="line">    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">                        nn.ReLU())</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><p><strong>NiN 模型</strong>使用卷积窗口形状分别为 11×11、5×5 和 3×3 的卷积层，相应的输出通道数也与 AlexNet 中的一致。每个 NiN 块后接一个步幅为 2、窗口形状为 3×3 的最大池化层。</p><p>除使用 NiN 块以外，NiN 还有一个设计与 AlexNet 显著不同：NiN 去掉了 AlexNet 最后的3个全连接层，取而代之地，NiN 使用了输出通道数等于标签类别数的NiN 块，然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。NiN 的这个设计的好处是可以显著减小模型参数尺寸，从而缓解过拟合。然而，该设计有时会造成获得有效模型的训练时间的增加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GlobalAvgPool2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(GlobalAvgPool2d, self).__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.avg_pool2d(x, kernel_size=x.size()[<span class="number">2</span>:])</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nin_block(<span class="number">1</span>, <span class="number">96</span>, kernel_size=<span class="number">11</span>, stride=<span class="number">4</span>, padding=<span class="number">0</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">96</span>, <span class="number">256</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="number">2</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">    nin_block(<span class="number">256</span>, <span class="number">384</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>), </span><br><span class="line">    nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    <span class="comment"># 标签类别数是10</span></span><br><span class="line">    nin_block(<span class="number">384</span>, <span class="number">10</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">    GlobalAvgPool2d(), </span><br><span class="line">    <span class="comment"># 将四维的输出转成二维的输出，其形状为(批量大小, 10)</span></span><br><span class="line">    d2l.FlattenLayer())</span><br></pre></td></tr></table></figure><h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.9_inception.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.9_inception.svg" alt="img"></a></p><p>图为 Inception 块的结构。从这里我们可以意识到的一点是，以 Block 为单位来拼凑模型的这种方法逐渐火热…</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>让我们先思考一个问题：对神经网络模型添加新的层，充分训练后的模型是否只可能更有效地降低训练误差？</p><p>理论上，原模型解的空间只是新模型解的空间的子空间。也就是说，如果我们能将新添加的层训练成恒等映射 $f(x)=x$，新模型和原模型将同样有效。由于新模型可能得出更优的解来拟合训练数据集，因此添加层似乎更容易降低训练误差。</p><p>然而在实践中，添加过多的层后训练误差往往不降反升。即使利用批量归一化带来的数值稳定性使训练深层模型更加容易，该问题仍然存在。针对这一问题，残差网络被提出。</p><ul><li>残差块</li></ul><p>让我们聚焦于神经网络局部。如图所示，设输入为 $x$。假设我们希望学出的理想映射为 $f(x)$，从而作为图上方激活函数的输入。左图虚线框中的部分需要直接拟合出该映射 $f(x)$，而右图虚线框中的部分则需要拟合出有关恒等映射的残差映射 $f(x)−x$。</p><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.11_residual-block.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.11_residual-block.svg" alt="img"></a></p><p>残差映射在实际中往往更容易优化。以本节开头提到的恒等映射作为我们希望学出的理想映射 $f(x)$。我们只需将图中右图虚线框内上方的加权运算（如仿射）的权重和偏差参数学成 $0$，那么 $f(x)$ 即为恒等映射。</p><p>实际中，当理想映射 $f(x)$ 极接近于恒等映射时，残差映射也易于捕捉恒等映射的细微波动。右图也是 ResNet 的基础块，即<strong>残差块</strong>（residual block）。在残差块中，输入可通过跨层的数据线路更快地向前传播。</p><p>ResNet 沿用了 VGG 全 $3×3$ 卷积层的设计。残差块里首先有 2 个有相同输出通道数的 $3×3$ 卷积层。每个卷积层后接一个批量归一化层和ReLU激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的ReLU激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 $1×1$ 卷积层来将输入变换成需要的形状后再做相加运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Residual</span>(<span class="params">nn.Module</span>):</span>  <span class="comment"># 本类已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channels, out_channels, use_1x1conv=<span class="literal">False</span>, stride=<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Residual, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, stride=stride)</span><br><span class="line">        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> use_1x1conv:</span><br><span class="line">            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, stride=stride)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv3 = <span class="literal">None</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channels)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        Y = F.relu(self.bn1(self.conv1(X)))</span><br><span class="line">        Y = self.bn2(self.conv2(Y))</span><br><span class="line">        <span class="keyword">if</span> self.conv3:</span><br><span class="line">            X = self.conv3(X)</span><br><span class="line">        <span class="keyword">return</span> F.relu(Y + X)</span><br></pre></td></tr></table></figure><ul><li>ResNet 模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet_block</span>(<span class="params">in_channels, out_channels, num_residuals, first_block=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> first_block:</span><br><span class="line">        <span class="keyword">assert</span> in_channels == out_channels <span class="comment"># 第一个模块的通道数同输入通道数一致</span></span><br><span class="line">    blk = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_residuals):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> first_block:</span><br><span class="line">            blk.append(Residual(in_channels, out_channels, use_1x1conv=<span class="literal">True</span>, stride=<span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            blk.append(Residual(out_channels, out_channels))</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(*blk)</span><br><span class="line"></span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block1&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">64</span>, <span class="number">2</span>, first_block=<span class="literal">True</span>))</span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block2&quot;</span>, resnet_block(<span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block3&quot;</span>, resnet_block(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>))</span><br><span class="line">net.add_module(<span class="string">&quot;resnet_block4&quot;</span>, resnet_block(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, 512, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(<span class="number">512</span>, <span class="number">10</span>))) </span><br></pre></td></tr></table></figure><h4 id="DenseNet"><a href="#DenseNet" class="headerlink" title="DenseNet"></a>DenseNet</h4><p>ResNet 中的跨层连接设计引申出了数个后续工作。本节我们介绍其中的一个：稠密连接网络。</p><p><a href="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.12_densenet.svg"><img src="http://tangshusen.me/Dive-into-DL-PyTorch/img/chapter05/5.12_densenet.svg" alt="img"></a></p><p>图中将部分前后相邻的运算抽象为模块 A 和模块 B。</p><p>与 ResNet 的主要区别在于，DenseNet 里模块 B 的输出不是像 ResNet 那样和模块 A 的输出相加，而是在通道维上连结。这样模块 A 的输出可以直接传入模块 B 后面的层。在这个设计里，模块 A 直接跟模块 B 后面的所有层连接在了一起。这也是它被称为“稠密连接”的原因。</p><ul><li>DenseBlock</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_block</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = nn.Sequential(nn.BatchNorm2d(in_channels), </span><br><span class="line">                        nn.ReLU(),</span><br><span class="line">                        nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DenseBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_convs, in_channels, out_channels</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DenseBlock, self).__init__()</span><br><span class="line">        net = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_convs):</span><br><span class="line">            in_c = in_channels + i * out_channels</span><br><span class="line">            net.append(conv_block(in_c, out_channels))</span><br><span class="line">        self.net = nn.ModuleList(net)</span><br><span class="line">        self.out_channels = in_channels + num_convs * out_channels <span class="comment"># 计算输出通道数</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.net:</span><br><span class="line">            Y = blk(X)</span><br><span class="line">            X = torch.cat((X, Y), dim=<span class="number">1</span>)  <span class="comment"># 在通道维上将输入和输出连结</span></span><br><span class="line">        <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><ul><li>过渡层</li></ul><p>由于每个稠密块都会带来通道数的增加，使用过多则会带来过于复杂的模型。过渡层用来控制模型复杂度。它通过 $1×1$ 卷积层来减小通道数，并使用步幅为 2 的平均池化层减半高和宽，从而进一步降低模型复杂度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transition_block</span>(<span class="params">in_channels, out_channels</span>):</span></span><br><span class="line">    blk = nn.Sequential(</span><br><span class="line">            nn.BatchNorm2d(in_channels), </span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>),</span><br><span class="line">            nn.AvgPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> blk</span><br></pre></td></tr></table></figure><ul><li>DenseNet 模型</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>),</span><br><span class="line">        nn.BatchNorm2d(<span class="number">64</span>), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">num_channels, growth_rate = <span class="number">64</span>, <span class="number">32</span>  <span class="comment"># num_channels为当前的通道数</span></span><br><span class="line">num_convs_in_dense_blocks = [<span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, num_convs <span class="keyword">in</span> <span class="built_in">enumerate</span>(num_convs_in_dense_blocks):</span><br><span class="line">    DB = DenseBlock(num_convs, num_channels, growth_rate)</span><br><span class="line">    net.add_module(<span class="string">&quot;DenseBlock_%d&quot;</span> % i, DB)</span><br><span class="line">    <span class="comment"># 上一个稠密块的输出通道数</span></span><br><span class="line">    num_channels = DB.out_channels</span><br><span class="line">    <span class="comment"># 在稠密块之间加入通道数减半的过渡层</span></span><br><span class="line">    <span class="keyword">if</span> i != <span class="built_in">len</span>(num_convs_in_dense_blocks) - <span class="number">1</span>:</span><br><span class="line">        net.add_module(<span class="string">&quot;transition_block_%d&quot;</span> % i, transition_block(num_channels, num_channels // <span class="number">2</span>))</span><br><span class="line">        num_channels = num_channels // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">net.add_module(<span class="string">&quot;BN&quot;</span>, nn.BatchNorm2d(num_channels))</span><br><span class="line">net.add_module(<span class="string">&quot;relu&quot;</span>, nn.ReLU())</span><br><span class="line">net.add_module(<span class="string">&quot;global_avg_pool&quot;</span>, d2l.GlobalAvgPool2d()) <span class="comment"># GlobalAvgPool2d的输出: (Batch, num_channels, 1, 1)</span></span><br><span class="line">net.add_module(<span class="string">&quot;fc&quot;</span>, nn.Sequential(d2l.FlattenLayer(), nn.Linear(num_channels, <span class="number">10</span>))) </span><br></pre></td></tr></table></figure><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型（language model）是自然语言处理的重要技术。自然语言处理中最常见的数据是文本数据。</p><p>我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为 $T$ 的文本中的词依次为 $w_1,w_2,…,w_T$，那么在离散的时间序列中，$w_t（1≤t≤T）$可看作在时间步（time step）$t$ 的输出或标签。</p><p>给定一个长度为 $T$ 的词的序列 $w_1,w_2,…,w_T$，语言模型将计算该序列的概率：$P(w_1,w_2,…,w_T)$.</p><p>语言模型可用于提升语音识别和机器翻译的性能。</p><p>例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。</p><p>在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。</p><h4 id="语言模型的计算"><a href="#语言模型的计算" class="headerlink" title="语言模型的计算"></a>语言模型的计算</h4><p>根据《概率论》课程学过的有关知识，我们不难理解：</p><script type="math/tex; mode=display">P(w_1,w_2,…,w_T)=\prod_{t=1}^{T}P(w_t∣w_1,…,w_{t−1})</script><p>那么，这些概率该如何获得呢？</p><p>设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过<strong>该词在训练数据集中的相对词频来计算</strong>。例如，$P(w_1)$ 可以计算为 $w_1$ 在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。再例如，$P(w_2∣w_1)$ 可以计算为 $w_1,w_2$ 两词相邻的频率与 $w_1$ 词频的比值，因为该比值即 $P(w_1,w_2)$ 与 $P(w_1)$ 之比；而 $P(w_3∣w_1,w_2)$ 同理可以计算为 $w_1$、$w_2$ 和 $w_3$ 三词相邻的频率与 $w_1$ 和 $w_2$ 两词相邻的频率的比值。以此类推。</p><h4 id="N-grams"><a href="#N-grams" class="headerlink" title="N-grams"></a>N-grams</h4><p>当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$ 元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的马尔可夫假设是指，<strong>一个词的出现只与前面 n 个词相关</strong>。</p><p>如果 $n=1$，那么有 $P(w_3∣w_1,w_2)=P(w_3∣w_2)$。</p><p>如果基于 $n−1$ 阶马尔可夫链，我们可以将语言模型改写为：$P(w_1,w_2,…,w_T)\prod_{t=1}^{T}P(w_t∣w_{t−(n−1)},…,w_{t−1})$。</p><p>当 $n$ 分别为 1、2 和 3 时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。</p><p>当 $n$ 较小时，$n$ 元语法往往并不准确。然而，当 $n$ 较大时，$n$ 元语法需要计算并存储大量的词频和多词相邻频率。那么，有没有方法在语言模型中更好地平衡以上这两点呢？我们将在本章探究这样的方法。</p><h3 id="RNN-1"><a href="#RNN-1" class="headerlink" title="RNN"></a>RNN</h3><p>本节将介绍循环神经网络。它并非刚性地记忆所有固定长度的序列，而是通过隐藏状态来存储之前时间步的信息。首先我们回忆一下前面介绍过的多层感知机，然后描述如何添加隐藏状态来将它变成循环神经网络。</p><ul><li>不含隐藏状态的神经网络</li></ul><p>让我们考虑一个含单隐藏层的多层感知机。给定样本数为 $n$、输入个数（特征数或特征向量维度）为 $d$ 的小批量数据样本 $X∈\mathbb{R}^{n×d}$。设隐藏层的激活函数为 $ϕ$，那么隐藏层的输出 $H∈\mathbb{R}^{n×h}$ 计算为</p><script type="math/tex; mode=display">H=ϕ(XW_{xh}+b_h)</script><p>其中隐藏层权重参数 $W_{xh}∈\mathbb{R}^{d×h}$，隐藏层偏差参数 $b_h∈\mathbb{R}^{1×h}$，$h$ 为隐藏单元个数。上式相加的两项形状不同，因此将按照广播机制相加。把隐藏变量 $H$ 作为输出层的输入，且设输出个数为 $q$（如分类问题中的类别数），输出层的输出为 $O=HW_{hq}+b_q$. 其中输出变量 $O∈\mathbb{R}^{n×q}$, 输出层权重参数 $W_{hq}∈\mathbb{R}^{h×q}$, 输出层偏差参数 $b_q∈\mathbb{R}^{1×q}$。如果是分类问题，我们可以使用 $\mathbf{softmax}(O)$ 来计算输出类别的概率分布。</p><ul><li>含隐藏状态的 RNN</li></ul><p>现在我们考虑输入数据存在时间相关性的情况。假设 $X_t∈\mathbb{R}^{n×d}$ 是序列中时间步 $t$ 的小批量输入，$H_t∈\mathbb{R}^{n×h}$ 是该时间步的隐藏变量。</p><p>与多层感知机不同的是，这里我们保存上一时间步的隐藏变量 $H_{t−1}$，并引入一个新的权重参数 $W_{hh}∈\mathbb{R}^{h×h}$，该参数用来描述在当前时间步如何使用上一时间步的隐藏变量。</p><p>具体来说，时间步 t 的隐藏变量的计算由当前时间步的输入和上一时间步的隐藏变量共同决定：</p><script type="math/tex; mode=display">H_t=ϕ(X_tW_{xh}+H_{t−1}W_{hh}+b_h)</script><p>与多层感知机相比，我们在这里添加了 $H_{t−1}W_{hh}$ 一项。由上式中相邻时间步的隐藏变量 $H_t$ 和 $H_{t−1}$ 之间的关系可知，这里的隐藏变量能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。因此，该隐藏变量也称为隐藏状态。</p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.2_rnn.svg"><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.2_rnn.svg" alt="img"></a></p><p>由于隐藏状态在当前时间步的定义使用了上一时间步的隐藏状态，上式的计算是循环的。使用循环计算的网络即循环神经网络（recurrent neural network）。</p><p>而在时间步 $t$，输出层的输出和多层感知机中的计算类似：$O_t=H_tW_{hq}+b_q$.</p><h3 id="字符数据集的制作"><a href="#字符数据集的制作" class="headerlink" title="字符数据集的制作"></a>字符数据集的制作</h3><ul><li>读取数据集</li><li>建立字符索引 idx_to_char 与 char_to_idx</li><li>时序数据的采样<ul><li>随机采样：在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。</li><li>相邻采样：令相邻的两个随机小批量在原始序列上的位置相毗邻。这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。<ul><li>在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态。</li><li>当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。为了使模型参数的梯度计算只依赖一次迭代读取的小批量序列，我们可以在每次读取小批量前将隐藏状态从计算图中分离出来。</li></ul></li></ul></li></ul><h3 id="RNN-的实现"><a href="#RNN-的实现" class="headerlink" title="RNN 的实现"></a>RNN 的实现</h3><h4 id="From-scratch"><a href="#From-scratch" class="headerlink" title="From scratch"></a>From scratch</h4><ul><li>单个词的表示：One-hot 向量</li><li>初始化模型参数与模型定义</li><li>预测函数的定义</li><li>裁剪梯度</li><li>模型评估：困惑度<ul><li>困惑度是对交叉熵损失函数做指数运算后得到的值<ul><li>最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</li><li>最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</li><li>基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</li></ul></li></ul></li></ul><h4 id="Simple"><a href="#Simple" class="headerlink" title="Simple"></a>Simple</h4><p>Pytorch 实现：大调库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens = <span class="number">256</span></span><br><span class="line"><span class="comment"># rnn_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens) # 已测试</span></span><br><span class="line">rnn_layer = nn.RNN(input_size=vocab_size, hidden_size=num_hiddens)</span><br></pre></td></tr></table></figure><p>与上一节中实现的循环神经网络不同，这里 <code>rnn_layer</code> 的输入形状为 <code>(时间步数, 批量大小, 输入个数)</code>。其中输入个数即 one-hot 向量长度（词典大小）。</p><p>此外，<code>rnn_layer</code> 作为 <code>nn.RNN</code> 实例，在前向计算后会分别返回<strong>隐藏层的输出 H</strong> 和<strong>隐藏状态 h</strong>。</p><ul><li>$H$ 指的是隐藏层在<strong>各个时间步</strong>上计算并输出的隐藏状态，它们通常作为后续输出层的输入，形状为 <code>(时间步数, 批量大小, 隐藏单元个数)</code>。</li><li>$h$ 指的是隐藏层在<strong>最后时间步</strong>的隐藏状态：当隐藏层有多层时，每一层的隐藏状态都会记录在该变量中；对于像长短期记忆（LSTM），隐藏状态是一个元组 $(h,c)$，即 hidden state 和 cell state。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本类已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rnn_layer, vocab_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(RNNModel, self).__init__()</span><br><span class="line">        self.rnn = rnn_layer</span><br><span class="line">        self.hidden_size = rnn_layer.hidden_size * (<span class="number">2</span> <span class="keyword">if</span> rnn_layer.bidirectional <span class="keyword">else</span> <span class="number">1</span>) </span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.dense = nn.Linear(self.hidden_size, vocab_size)</span><br><span class="line">        self.state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, state</span>):</span> <span class="comment"># inputs: (batch, seq_len)</span></span><br><span class="line">        <span class="comment"># 获取 one-hot 向量表示</span></span><br><span class="line">        X = d2l.to_onehot(inputs, self.vocab_size) <span class="comment"># X 是个 list</span></span><br><span class="line">        Y, self.state = self.rnn(torch.stack(X), state)</span><br><span class="line">        <span class="comment"># 全连接层会首先将 Y 的形状变成 (num_steps * batch_size, num_hiddens)，它的输出</span></span><br><span class="line">        <span class="comment"># 形状为 (num_steps * batch_size, vocab_size)</span></span><br><span class="line">        output = self.dense(Y.view(-<span class="number">1</span>, Y.shape[-<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">return</span> output, self.state</span><br></pre></td></tr></table></figure><p>在前面两节中，如果不裁剪梯度，模型将无法正常训练。当总的时间步数较大或者当前时间步较小时，循环神经网络的梯度较容易出现衰减或爆炸。</p><h3 id="RNN-的改进"><a href="#RNN-的改进" class="headerlink" title="RNN 的改进"></a>RNN 的改进</h3><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>虽然裁剪梯度可以应对梯度爆炸，但无法解决梯度衰减的问题。通常由于这个原因，循环神经网络在实际中较难捕捉时间序列中时间步距离较大的依赖关系。</p><p>门控循环神经网络（gated recurrent neural network）的提出，正是为了更好地捕捉时间序列中时间步距离较大的依赖关系。它通过可以学习的门来控制信息的流动。</p><ul><li>门控循环单元</li></ul><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.7_gru_3.svg"><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.7_gru_3.svg" alt="img"></a></p><script type="math/tex; mode=display">R_t=σ(X_tW_{xr}+H_{t−1}W_{hr}+b_r),\\Z_t=σ(X_tW_{xz}+H_{t−1}W_{hz}+b_z),\\\tilde{H}_t=tanh(X_tW_{xh}+(R_t⊙H_{t−1})W_{hh}+b_h),\\H_t=Z_t⊙H_{t−1}+(1−Z_t)⊙\tilde{H}_t.</script><ul><li>重置门有助于捕捉时间序列里短期的依赖关系<ul><li>重置门控制了上一时间步的隐藏状态如何流入当前时间步的候选隐藏状态</li><li>上一时间步的隐藏状态可能包含了时间序列截至上一时间步的全部历史信息</li><li>重置门可以用来丢弃与预测无关的历史信息</li></ul></li><li>更新门有助于捕捉时间序列里长期的依赖关系<ul><li>更新门可以控制隐藏状态应该如何被包含当前时间步信息的候选隐藏状态所更新</li><li>假设更新门在时间步 $t^′$ 到 $t$（$t^′&lt;t$）之间一直近似 1。那么，在时间步 $t^′$ 到 $t$ 之间的输入信息几乎没有流入时间步 $t$ 的隐藏状态 $H_t$。实际上，这可以看作是较早时刻的隐藏状态 $H_{t^′−1}$ 一直通过时间保存并传递至当前时间步 $t$。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</li></ul></li></ul><p>实现也可以直接大调库：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.8_lstm_3.svg"><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.8_lstm_3.svg" alt="img"></a></p><script type="math/tex; mode=display">I_t=σ(X_tW_{xi}+H_{t−1}W_{hi}+b_i),\\F_t=σ(X_tW_{xf}+H_{t−1}W_{hf}+b_f),\\O_t=σ(X_tW_{xo}+H_{t−1}W_{ho}+b_o),\\\tilde{C}_t=tanh(X_tW_{xc}+H_{t−1}W_{hc}+b_c),\\C_t=F_t⊙C_{t−1}+I_t⊙\tilde{C}_t,\\H_t=O_t⊙tanh(C_t).</script><ul><li><p>遗忘门控制上一时间步的记忆细胞 $C_{t−1}$ 中的信息是否传递到当前时间步，而输入门则控制当前时间步的输入 $X_t$ 通过候选记忆细胞 $\tilde{C}_t$ 如何流入当前时间步的记忆细胞。</p><ul><li>如果遗忘门一直近似 1 且输入门一直近似 0，过去的记忆细胞将一直通过时间保存并传递至当前时间步。这个设计可以应对循环神经网络中的梯度衰减问题，并更好地捕捉时间序列中时间步距离较大的依赖关系。</li></ul></li><li><p>当输出门近似 1 时，记忆细胞信息将传递到隐藏状态供输出层使用；当输出门近似 0 时，记忆细胞信息只自己保留。</p></li></ul><p>实现也是大调库。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><h4 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep-RNN"></a>Deep-RNN</h4><p>本章到目前为止介绍的循环神经网络只有一个单向的隐藏层，在深度学习应用里，我们通常会用到含有多个隐藏层的循环神经网络，也称作深度循环神经网络。</p><p>图中演示了一个有 $L$ 个隐藏层的深度循环神经网络，每个隐藏状态不断传递至当前层的下一时间步和当前时间步的下一层。</p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.9_deep-rnn.svg"><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.9_deep-rnn.svg" alt="img"></a></p><script type="math/tex; mode=display">H_t^{(1)}=ϕ(X_tW_{xh}^{(1)}+H_{t−1}^{(1)}W_{hh}^{(1)}+b_h^{(1)}),\\H_t^{(ℓ)}=ϕ(H_t^{(ℓ−1)}W_{xh}^{(ℓ)}+H_{t−1}^{(ℓ)}W_{hh}^{(ℓ)}+b_h^{(ℓ)}),1<ℓ≤L,\\O_t=H_t^{(L)}W_{hq}+b_q.</script><h4 id="bi-RNN"><a href="#bi-RNN" class="headerlink" title="bi-RNN"></a>bi-RNN</h4><p>之前介绍的循环神经网络模型都是假设当前时间步是由前面的较早时间步的序列决定的，因此它们都将信息通过隐藏状态从前往后传递。</p><p>有时候，当前时间步也可能由后面时间步决定。例如，当我们写下一个句子时，可能会根据句子后面的词来修改句子前面的用词。双向循环神经网络通过增加从后往前传递信息的隐藏层来更灵活地处理这类信息。</p><p><a href="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.10_birnn.svg"><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/chapter06/6.10_birnn.svg" alt="img"></a></p><script type="math/tex; mode=display">\overrightarrow{H}_t=ϕ(X_tW_{xh}^{(f)}+\overrightarrow{H}^{t−1}W_{hh}^{(f)}+b_h^{(f)}),\\\overrightarrow{H}_t=ϕ(X_tW_{xh}^{(b)}+\overrightarrow{H}_{t+1}W_{hh}^P{(b)}+b_h^{(b)}),</script><p>然后我们连结两个方向的隐藏状态 $\overrightarrow{H}_t$ 和 $\overleftarrow{H}_t$ 来得到隐藏状态 $H_t∈\mathbb{R}^{n×2h}$，并将其输入到输出层。</p>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
          <category> 理论/机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《动手学深度学习》 Pytorch ver. Part A</title>
      <link href="/2022/07/10/dive-into-dl-pytorch-A/"/>
      <url>/2022/07/10/dive-into-dl-pytorch-A/</url>
      
        <content type="html"><![CDATA[<p><img src="https://s2.loli.net/2022/07/10/8Z5AxfpC9aN1Yu2.png" alt="cover.png"></p><ul><li>《动手学深度学习》原书地址：<a href="https://github.com/d2l-ai/d2l-zh">https://github.com/d2l-ai/d2l-zh</a></li><li>《动手学深度学习》(Pytorch ver.)：<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">https://tangshusen.me/Dive-into-DL-PyTorch/#/</a></li></ul><p>知识架构:</p><p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" alt="知识架构"></p><p>本文的主要作用是在阅读过程中做一些摘录。对于「机器学习」领域，reta 虽然曾尝试从各个领域入门，也尝试训过一些模型，但是还是缺少系统性、结构性的学习。希望阅读本书能带来更多的收获吧。</p><p>与前面的一些笔记相比，本文更加侧重于「实践」。也就是说切实地提升自己的代码能力。</p><p>Part A 包含：</p><ul><li>§ 1 深度学习简介</li><li>§ 2 预备知识：Pytorch</li><li>§ 3 深度学习基础<ul><li>线性回归，Softmax 回归，多层感知机三类基本模型</li><li>权重衰减和 Dropout 两类应对过拟合的方法</li></ul></li><li>§ 4 深度学习计算<ul><li>构造 Pytorch 模型的方式</li><li>模型参数的访问、初始化与共享</li><li>自定义 Layer</li><li>读取与存储</li><li>GPU 计算</li></ul></li></ul><h2 id="深度学习简介"><a href="#深度学习简介" class="headerlink" title="深度学习简介"></a>深度学习简介</h2><ul><li><strong>机器学习与深度学习的关系</strong></li></ul><p><strong>机器学习</strong>研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。</p><p>在机器学习的众多研究方向中，<strong>表征学习关注如何自动找出表示数据的合适方式</strong>，以便更好地将输入变换为正确的输出。</p><p>而本书要重点探讨的<strong>深度学习是具有多级表示的表征学习方法</strong>。</p><p>在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。</p><ul><li><strong>深度学习的一个外在特点：End-to-end</strong></li></ul><p>深度学习的一个外在特点是<strong>端到端的训练</strong>。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。</p><p>比如说，计算机视觉科学家之前曾一度将特征抽取与机器学习模型的构建分开处理，像是Canny边缘探测和SIFT特征提取曾占据统治性地位达10年以上，但这也就是人类能找到的最好方法了。</p><p>当深度学习进入这个领域后，这些特征提取方法就被性能更强的自动优化的逐级过滤器替代了。</p><h2 id="预备知识-Pytorch"><a href="#预备知识-Pytorch" class="headerlink" title="预备知识: Pytorch"></a>预备知识: Pytorch</h2><h3 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h3><ul><li><strong>对 Tensor 的操作：Tensor 的创建</strong></li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>Tensor(*sizes)</code></td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center"><code>tensor(data,)</code></td><td style="text-align:center">类似 <code>np.array</code> 的构造函数</td></tr><tr><td style="text-align:center"><code>ones(*sizes)</code></td><td style="text-align:center">全 1 Tensor</td></tr><tr><td style="text-align:center"><code>zeros(*sizes)</code></td><td style="text-align:center">全 0 Tensor</td></tr><tr><td style="text-align:center"><code>eye(*sizes)</code></td><td style="text-align:center">对角线为 1，其他为 0</td></tr><tr><td style="text-align:center"><code>arange(s,e,step)</code></td><td style="text-align:center">从 s 到 e，步长为 step</td></tr><tr><td style="text-align:center"><code>linspace(s,e,steps)</code></td><td style="text-align:center">从 s 到 e，均匀切分成 steps 份</td></tr><tr><td style="text-align:center"><code>rand/randn(*sizes)</code></td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center"><code>normal(mean,std)/uniform(from,to)</code></td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center"><code>randperm(m)</code></td><td style="text-align:center">随机排列</td></tr></tbody></table></div><ul><li><strong>对 Tensor 进行操作时注意其可能的数据共享</strong></li></ul><p>如使用 <code>view()</code> 改变 Tensor 的形状的时候，注意返回的新 Tensor 与源 Tensor 虽然可能有不同的 size，但是是共享 data 的。</p><p>所以如果我们想返回一个真正新的副本（即不共享 data 内存）该怎么办呢？Pytorch 还提供了一个 <code>reshape()</code> 可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用 <code>clone</code> 创造一个副本然后再使用 <code>view</code>。</p><p>注：虽然 <code>view</code> 返回的 <code>Tensor</code> 与源 <code>Tensor</code> 是共享 <code>data</code> 的，但是依然是一个新的 <code>Tensor</code>（因为 <code>Tensor</code> 除了包含 <code>data</code> 外还有一些其他属性），二者 <code>id</code>（内存地址）并不一致。</p><p>另外一个常用的函数就是 <code>item()</code>, 它可以将一个标量 <code>Tensor</code> 转换成一个 Python Number。</p><ul><li><strong>广播机制</strong> Broadcasting</li></ul><p>当对两个形状不同的 <code>Tensor</code> 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 <code>Tensor</code> 形状相同后再按元素运算。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]]) <span class="comment"># x</span></span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]]) <span class="comment"># y</span></span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]]) <span class="comment"># x+y</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>Tensor</code> 与 <code>ndarray</code></strong> 的转换</li></ul><p>很容易用 <code>numpy()</code> 和 <code>from_numpy()</code> 将 <code>Tensor</code> 和 NumPy 中的数组相互转换。但是需要注意的一点是：两个函数所产生的 <code>Tensor</code> 和 NumPy 中的数组共享相同的内存（所以它们之间的转换很快），改变其中一个时另一个也会改变。</p><p>与之相对比，还有一个常用的将 NumPy 中的 array 转换成 <code>Tensor</code> 的方法就是 <code>torch.tensor()</code>, 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的 <code>Tensor</code>和原来的数据不再共享内存。</p><p>所有在CPU上的 <code>Tensor</code>（除了 <code>CharTensor</code>）都支持与 NumPy 数组相互转换。</p><ul><li><strong><code>Tensor</code> on GPU</strong></li></ul><p>用方法 <code>to()</code> 可以将 <code>Tensor</code> 在 CPU 和 GPU（需要硬件支持）之间相互移动。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以下代码只有在PyTorch GPU版本上才会执行</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># GPU</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 等价于 .to(&quot;cuda&quot;)</span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># to()还可以同时更改数据类型</span></span><br></pre></td></tr></table></figure><h3 id="Autograd"><a href="#Autograd" class="headerlink" title="Autograd"></a>Autograd</h3><ul><li><strong><code>Tensor</code> 的 <code>requires_grad</code> 与 <code>Function</code></strong></li></ul><p>上一节介绍的 <code>Tensor</code> 是这个包的核心类，如果将其属性 <code>requires_grad</code> 设置为 <code>True</code>，它将开始追踪（track）在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。</p><p>完成计算后，可以调用 <code>backward()</code> 来完成所有梯度计算。此 <code>Tensor</code> 的梯度将累积到 <code>.grad</code> 属性中。</p><p>如果不想要被继续追踪，可以调用 <code>.detach()</code> 将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用 <code>with torch.no_grad()</code> 将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（<code>requires_grad=True</code>）的梯度。</p><p><code>Function</code> 是另外一个很重要的类。<code>Tensor</code> 和 <code>Function</code> 互相结合就可以构建一个记录有整个计算过程的有向无环图。每个 <code>Tensor</code> 都有一个 <code>grad_fn</code> 属性，该属性即创建该 <code>Tensor</code> 的 <code>Function</code> , 就是说该 <code>Tensor</code> 是不是通过某些运算得到的，若是，则 <code>grad_fn</code> 返回一个与这些运算相关的对象，否则是 None。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x.grad_fn)</span><br><span class="line"><span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x + <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y)</span><br><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line">&lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x7fbb003b4250</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x.is_leaf, y.is_leaf)</span><br><span class="line"><span class="literal">True</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = y * y * <span class="number">3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out = z.mean()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(z, out)</span><br><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward0&gt;) tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure><p>注意 x 是直接创建的，所以它没有 <code>grad_fn</code>, 而 y 是通过一个加法操作创建的，所以它有一个为 <code>&lt;AddBackward&gt;</code> 的 <code>grad_fn</code>。像 x 这种直接创建的称为叶子节点，叶子节点对应的 <code>grad_fn</code> 是 <code>None</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">2</span>, <span class="number">2</span>) <span class="comment"># 缺失情况下默认 requires_grad = False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(a.requires_grad) <span class="comment"># False</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 .requires_grad_() 来用 in-place 的方式改变 requires_grad 属性</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">tensor([[-<span class="number">0.0261</span>,  <span class="number">0.6281</span>],</span><br><span class="line">        [ <span class="number">1.1572</span>,  <span class="number">6.8756</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(a.requires_grad) <span class="comment"># True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(b.grad_fn)</span><br><span class="line">&lt;SumBackward0 <span class="built_in">object</span> at <span class="number">0x7fba80387730</span>&gt;</span><br></pre></td></tr></table></figure><ul><li><strong><code>backward()</code></strong></li></ul><p>因为 <code>out</code> 是一个标量，所以调用 <code>backward()</code> 时不需要指定求导变量：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>out.backward() <span class="comment"># 等价于 out.backward(torch.tensor(1.))</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x.grad) <span class="comment"># Out 关于 x 的梯度, d(out)/dx</span></span><br><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure><p>本质上，反向传播的过程是在计算一系列 Jacobi 矩阵的乘积。</p><p>注意：grad 在反向传播过程中是累加的，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以<strong>一般在反向传播之前需把梯度清零</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 再来反向传播一次，注意grad是累加的</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out2 = x.<span class="built_in">sum</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out2.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x.grad)</span><br><span class="line">tensor([[<span class="number">5.5000</span>, <span class="number">5.5000</span>],</span><br><span class="line">        [<span class="number">5.5000</span>, <span class="number">5.5000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out3 = x.<span class="built_in">sum</span>()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.grad.data.zero_()</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>out3.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(x.grad)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><p>注：PyTorch 的 <code>backward</code> 为什么有一个 <code>grad_variables</code> 参数？</p><p>假设 x 经过一番计算得到 y，那么 <code>y.backward(w)</code> 求的不是 y 对 x 的导数，而是 <code>l = torch.sum(y*w)</code> 对 x 的导数。w 可以视为 y 的各分量的权重，也可以视为遥远的损失函数 l 对 y 的偏导数（这正是函数说明文档的含义）。特别地，若 y 为标量，w 取默认值 1.0，才是按照我们通常理解的那样，求 y 对 x 的导数。</p><ul><li><strong>中断梯度传播</strong></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span> </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y2 = x ** <span class="number">3</span></span><br><span class="line">y3 = y1 + y2</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(y1, y1.requires_grad) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(y2, y2.requires_grad) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(y3, y3.requires_grad) <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"><span class="literal">True</span></span><br><span class="line">tensor(<span class="number">1.</span>, grad_fn=&lt;PowBackward0&gt;) <span class="literal">True</span></span><br><span class="line">tensor(<span class="number">1.</span>) <span class="literal">False</span></span><br><span class="line">tensor(<span class="number">2.</span>, grad_fn=&lt;ThAddBackward&gt;) <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>可以看到，上面的 <code>y2</code> 是没有 <code>grad_fn</code> 而且 <code>y2.requires_grad=False</code> 的，而 <code>y3</code> 是有 <code>grad_fn</code> 的。如果我们将<code>y3</code>对<code>x</code>求梯度的话：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y3.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure><p>正如我们所理解的，$y_3=y_1+y_2=x^2+x^3$，其中 $y_2$ 的梯度不被回传，因此$\frac{\mathrm{d} y_3}{\mathrm{d} x}=2x $</p><p>此外，如果我们想要修改 <code>Tensor</code> 的数值，但是又不希望被 <code>autograd</code> 记录（即不会影响反向传播），那么我么可以对 <code>tensor.data</code> 进行操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.data) <span class="comment"># 还是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(x.data.requires_grad) <span class="comment"># 但是已经是独立于计算图之外</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># 更改data的值也会影响tensor的值</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor([<span class="number">1.</span>])</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">tensor([<span class="number">100.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">2.</span>])</span><br></pre></td></tr></table></figure><h2 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h2><h3 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h3><h4 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h4><p>首先，回归问题的输出是连续值，而分类问题的输出是离散值，这是二者的区别。</p><ul><li><strong>模型定义</strong>：假设我们采集的样本数为 $n$，索引为 $i$ 的样本的特征为 $x_1^{(i)}$ 和 $x_2^{(i)}$，标签为 $y^{(i)}$。对于索引为 $i$ 的房屋，线性回归模型的房屋价格预测表达式为 $\hat{y}^{(i)}=x_1^{(i)}w_1+x_2^{(i)}w_2+b$</li><li><strong>损失函数</strong>：$\ell^{(i)}(w_1,w_2,b)=\frac{1}{2}(\hat{y}^{(i)}−y^{(i)})^2$, $\ell(w_1,w_2,b)=\frac{1}{n}{\textstyle \sum_{i=1}^{n}}\ell^{(i)}(w_1,w_2,b)=\frac{1}{n}{\textstyle \sum_{i=1}^{n}}\frac{1}{2}(x_1^{(i)}w_1+x_2^{(i)}w_2+b−y^{(i)})^2$</li></ul><p>在模型训练中，我们希望找出一组模型参数，记为 $w_1^{<em>}$, $w_2^{</em>}$, $b^{<em>}$，来使训练样本平均损失最小：，来使训练样本平均损失最小： $w_1^{</em>}$, $w_2^{<em>}$, $b^{</em>} = \underset{w_1, w_2, b}{\arg\min} \ell(w_1, w_2, b) $</p><ul><li><strong>优化算法</strong></li></ul><p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作<strong>解析解</strong>。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作<strong>数值解</strong>。</p><p>在求数值解的优化算法中，<strong>小批量随机梯度下降</strong>（Mini-batch SGD, mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）$\mathcal{B} $，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数 learning_rate $\eta$ 的乘积作为模型参数在本次迭代的减小量。</p><p>在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：</p><script type="math/tex; mode=display">w_1 \gets w_1−\frac{η}{|\mathcal{B}|}\sum_{i\in B}\frac{\partial \ell^{(i)}(w_1,w_2,b)}{\partial w_1}=w_1−\frac{η}{|\mathcal{B}|}\sum_{i\in B}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b−y^{(i)}), \\w_2 \gets w_2−\frac{η}{|\mathcal{B}|}\sum_{i\in B}\frac{\partial \ell^{(i)}(w_1,w_2,b)}{\partial w_1}=w_1−\frac{η}{|\mathcal{B}|}\sum_{i\in B}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b−y^{(i)}), \\b \gets b−\frac{η}{|\mathcal{B}|}\sum_{i\in B}\frac{\partial \ell^{(i)}(w_1,w_2,b)}{\partial b}=b−\frac{η}{|\mathcal{B}|}\sum_{i\in B}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b−y^{(i)}).</script><p>在上式中，$\left | \mathcal{B} \right | $ 代表每个小批量中的样本个数（批量大小，batch size），$\eta$ 称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。</p><h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><ul><li>生成数据集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples, num_inputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">labels += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=labels.size()), dtype=torch.<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><ul><li>读取数据</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="comment"># 将训练数据的特征和标签组合</span></span><br><span class="line">dataset = Data.TensorDataset(features, labels)</span><br><span class="line"><span class="comment"># 随机读取小批量</span></span><br><span class="line">data_iter = Data.DataLoader(dataset, batch_size, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">    <span class="built_in">print</span>(X, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">tensor([[-<span class="number">2.7723</span>, -<span class="number">0.6627</span>],</span><br><span class="line">        [-<span class="number">1.1058</span>,  <span class="number">0.7688</span>],</span><br><span class="line">        [ <span class="number">0.4901</span>, -<span class="number">1.2260</span>],</span><br><span class="line">        [-<span class="number">0.7227</span>, -<span class="number">0.2664</span>],</span><br><span class="line">        [-<span class="number">0.3390</span>,  <span class="number">0.1162</span>],</span><br><span class="line">        [ <span class="number">1.6705</span>, -<span class="number">2.7930</span>],</span><br><span class="line">        [ <span class="number">0.2576</span>, -<span class="number">0.2928</span>],</span><br><span class="line">        [ <span class="number">2.0475</span>, -<span class="number">2.7440</span>],</span><br><span class="line">        [ <span class="number">1.0685</span>,  <span class="number">1.1920</span>],</span><br><span class="line">        [ <span class="number">1.0996</span>,  <span class="number">0.5106</span>]]) </span><br><span class="line"> tensor([ <span class="number">0.9066</span>, -<span class="number">0.6247</span>,  <span class="number">9.3383</span>,  <span class="number">3.6537</span>,  <span class="number">3.1283</span>, <span class="number">17.0213</span>,  <span class="number">5.6953</span>, <span class="number">17.6279</span>,</span><br><span class="line">         <span class="number">2.2809</span>,  <span class="number">4.6661</span>])</span><br><span class="line"><span class="comment"># 可以看到，对迭代器进行迭代每次拿到的数据也是以 batch 的形式封装成 array</span></span><br></pre></td></tr></table></figure><ul><li>定义模型</li></ul><p>首先，导入 <code>torch.nn</code> 模块。实际上，“nn”是neural networks（神经网络）的缩写。</p><p>顾名思义，该模块定义了大量神经网络的层。之前我们已经用过了 <code>autograd</code>，而 <code>nn</code> 就是利用 <code>autograd</code> 来定义模型。</p><p><code>nn</code> 的核心数据结构是 <code>Module</code>，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。</p><p>一个 <code>nn.Module</code> 实例应该包含一些层以及返回输出的前向传播（forward）方法。下面先来看看如何用 <code>nn.Module</code> 实现一个线性回归模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(n_feature, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># forward 定义前向传播</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="built_in">print</span>(net) <span class="comment"># 使用print可以打印出网络的结构</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Output:</span></span><br><span class="line">LinearNet(</span><br><span class="line">  (linear): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>事实上我们还可以用<code>nn.Sequential</code>来更加方便地搭建网络，<code>Sequential</code>是一个有序的容器，网络层将按照在传入<code>Sequential</code>的顺序依次被添加到计算图中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 写法一</span></span><br><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 此处还可以传入其他层</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法二</span></span><br><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># net.add_module ......</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 写法三</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">          (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br><span class="line">          <span class="comment"># ......</span></span><br><span class="line">]))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="built_in">print</span>(net[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">Sequential(</span><br><span class="line">  (linear): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line">Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>可以通过 <code>net.parameters()</code> 来查看模型所有的可学习参数，此函数将返回一个生成器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.0277,  0.2771]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.3395], requires_grad=True)</span><br></pre></td></tr></table></figure><p>注意：<code>torch.nn</code> <strong>仅支持输入一个 batch 的样本</strong>，而不支持单个样本输入，如果只有单个样本，可使用 <code>input.unsqueeze(0)</code> 来添加一维。</p><p>附：<code>unsqueeze()</code> 的使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn((<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[[-<span class="number">0.6444</span>, -<span class="number">0.5408</span>, -<span class="number">0.6239</span>],</span><br><span class="line">         [-<span class="number">0.8880</span>, -<span class="number">0.7358</span>,  <span class="number">0.7287</span>],</span><br><span class="line">         [ <span class="number">1.1660</span>,  <span class="number">1.3125</span>, -<span class="number">3.4676</span>],</span><br><span class="line">         [-<span class="number">1.4620</span>, -<span class="number">0.1572</span>, -<span class="number">0.4755</span>],</span><br><span class="line">         [ <span class="number">0.6389</span>, -<span class="number">2.4514</span>,  <span class="number">0.3339</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="number">0</span>) <span class="comment"># 最外层加壳</span></span><br><span class="line">tensor([[[[-<span class="number">0.6444</span>, -<span class="number">0.5408</span>, -<span class="number">0.6239</span>],</span><br><span class="line">          [-<span class="number">0.8880</span>, -<span class="number">0.7358</span>,  <span class="number">0.7287</span>],</span><br><span class="line">          [ <span class="number">1.1660</span>,  <span class="number">1.3125</span>, -<span class="number">3.4676</span>],</span><br><span class="line">          [-<span class="number">1.4620</span>, -<span class="number">0.1572</span>, -<span class="number">0.4755</span>],</span><br><span class="line">          [ <span class="number">0.6389</span>, -<span class="number">2.4514</span>,  <span class="number">0.3339</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="number">1</span>) <span class="comment"># 次外层元素</span></span><br><span class="line">tensor([[[[-<span class="number">0.6444</span>, -<span class="number">0.5408</span>, -<span class="number">0.6239</span>],</span><br><span class="line">          [-<span class="number">0.8880</span>, -<span class="number">0.7358</span>,  <span class="number">0.7287</span>],</span><br><span class="line">          [ <span class="number">1.1660</span>,  <span class="number">1.3125</span>, -<span class="number">3.4676</span>],</span><br><span class="line">          [-<span class="number">1.4620</span>, -<span class="number">0.1572</span>, -<span class="number">0.4755</span>],</span><br><span class="line">          [ <span class="number">0.6389</span>, -<span class="number">2.4514</span>,  <span class="number">0.3339</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="number">2</span>)</span><br><span class="line">tensor([[[[-<span class="number">0.6444</span>, -<span class="number">0.5408</span>, -<span class="number">0.6239</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">0.8880</span>, -<span class="number">0.7358</span>,  <span class="number">0.7287</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">1.1660</span>,  <span class="number">1.3125</span>, -<span class="number">3.4676</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">1.4620</span>, -<span class="number">0.1572</span>, -<span class="number">0.4755</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.6389</span>, -<span class="number">2.4514</span>,  <span class="number">0.3339</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="number">3</span>)</span><br><span class="line">tensor([[[[-<span class="number">0.6444</span>],</span><br><span class="line">          [-<span class="number">0.5408</span>],</span><br><span class="line">          [-<span class="number">0.6239</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">0.8880</span>],</span><br><span class="line">          [-<span class="number">0.7358</span>],</span><br><span class="line">          [ <span class="number">0.7287</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">1.1660</span>],</span><br><span class="line">          [ <span class="number">1.3125</span>],</span><br><span class="line">          [-<span class="number">3.4676</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">1.4620</span>],</span><br><span class="line">          [-<span class="number">0.1572</span>],</span><br><span class="line">          [-<span class="number">0.4755</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.6389</span>],</span><br><span class="line">          [-<span class="number">2.4514</span>],</span><br><span class="line">          [ <span class="number">0.3339</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.unsqueeze(<span class="number">4</span>)</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;&lt;stdin&gt;&quot;</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">IndexError: Dimension out of <span class="built_in">range</span> (expected to be <span class="keyword">in</span> <span class="built_in">range</span> of [-<span class="number">4</span>, <span class="number">3</span>], but got <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.unsqueeze(-<span class="number">1</span>) <span class="comment"># 最内层</span></span><br><span class="line">tensor([[[[-<span class="number">0.6444</span>],</span><br><span class="line">          [-<span class="number">0.5408</span>],</span><br><span class="line">          [-<span class="number">0.6239</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">0.8880</span>],</span><br><span class="line">          [-<span class="number">0.7358</span>],</span><br><span class="line">          [ <span class="number">0.7287</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">1.1660</span>],</span><br><span class="line">          [ <span class="number">1.3125</span>],</span><br><span class="line">          [-<span class="number">3.4676</span>]],</span><br><span class="line"></span><br><span class="line">         [[-<span class="number">1.4620</span>],</span><br><span class="line">          [-<span class="number">0.1572</span>],</span><br><span class="line">          [-<span class="number">0.4755</span>]],</span><br><span class="line"></span><br><span class="line">         [[ <span class="number">0.6389</span>],</span><br><span class="line">          [-<span class="number">2.4514</span>],</span><br><span class="line">          [ <span class="number">0.3339</span>]]]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.squeeze() <span class="comment"># 一直到最内层</span></span><br><span class="line">tensor([[-<span class="number">0.6444</span>, -<span class="number">0.5408</span>, -<span class="number">0.6239</span>],</span><br><span class="line">        [-<span class="number">0.8880</span>, -<span class="number">0.7358</span>,  <span class="number">0.7287</span>],</span><br><span class="line">        [ <span class="number">1.1660</span>,  <span class="number">1.3125</span>, -<span class="number">3.4676</span>],</span><br><span class="line">        [-<span class="number">1.4620</span>, -<span class="number">0.1572</span>, -<span class="number">0.4755</span>],</span><br><span class="line">        [ <span class="number">0.6389</span>, -<span class="number">2.4514</span>,  <span class="number">0.3339</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.squeeze(<span class="number">1</span>) <span class="comment"># 某一层</span></span><br><span class="line">tensor([[[-<span class="number">0.6444</span>, -<span class="number">0.5408</span>, -<span class="number">0.6239</span>],</span><br><span class="line">         [-<span class="number">0.8880</span>, -<span class="number">0.7358</span>,  <span class="number">0.7287</span>],</span><br><span class="line">         [ <span class="number">1.1660</span>,  <span class="number">1.3125</span>, -<span class="number">3.4676</span>],</span><br><span class="line">         [-<span class="number">1.4620</span>, -<span class="number">0.1572</span>, -<span class="number">0.4755</span>],</span><br><span class="line">         [ <span class="number">0.6389</span>, -<span class="number">2.4514</span>,  <span class="number">0.3339</span>]]])</span><br></pre></td></tr></table></figure><ul><li>初始化模型参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias, val=<span class="number">0</span>)  <span class="comment"># 也可以直接修改bias的data: net[0].bias.data.fill_(0)</span></span><br></pre></td></tr></table></figure><ul><li>定义损失函数</li></ul><p>PyTorch 在 <code>nn</code> 模块中提供了各种损失函数，这些损失函数可看作是一种特殊的层，PyTorch 也将这些损失函数实现为 <code>nn.Module</code> 的子类。我们现在使用它提供的均方误差损失作为模型的损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><ul><li>定义优化算法</li></ul><p>同样，我们也无须自己实现小批量随机梯度下降算法。<code>torch.optim</code> 模块提供了很多常用的优化算法比如 SGD、Adam 和 RMSProp 等。下面我们创建一个用于优化 <code>net</code> 所有参数的优化器实例，并指定学习率为 0.03 的小批量随机梯度下降（SGD）为优化算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br><span class="line"><span class="built_in">print</span>(optimizer)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">SGD (</span><br><span class="line">Parameter Group 0</span><br><span class="line">    dampening: 0</span><br><span class="line">    lr: 0.03</span><br><span class="line">    momentum: 0</span><br><span class="line">    nesterov: False</span><br><span class="line">    weight_decay: 0</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>我们还可以为不同子网络设置不同的学习率，这在 fine-tune 时经常用到。例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer =optim.SGD([</span><br><span class="line">                <span class="comment"># 如果对某个参数不指定学习率，就使用最外层的默认学习率</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet1.parameters()&#125;, <span class="comment"># lr=0.03</span></span><br><span class="line">                &#123;<span class="string">&#x27;params&#x27;</span>: net.subnet2.parameters(), <span class="string">&#x27;lr&#x27;</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">            ], lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure><p>有时候我们不想让学习率固定成一个常数，那如何调整学习率呢？主要有两种做法。一种是修改 <code>optimizer.param_groups</code> 中对应的学习率，另一种是更简单也是较为推荐的做法 —— <strong>新建优化器</strong>，由于optimizer十分轻量级，构建开销很小，故而可以构建新的 optimizer。但是后者对于使用动量的优化器（如 Adam），会丢失动量等状态信息，可能会造成损失函数的收敛出现震荡等情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调整学习率</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">&#x27;lr&#x27;</span>] *= <span class="number">0.1</span> <span class="comment"># 学习率为之前的0.1倍</span></span><br></pre></td></tr></table></figure><ul><li>训练模型</li></ul><p>通过调用 <code>optim</code> 实例的 <code>step</code> 函数来迭代模型参数。按照小批量随机梯度下降的定义，我们在 <code>step</code> 函数中指明批量大小，从而对批量中样本梯度求平均。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss: %f&#x27;</span> % (epoch, l.item()))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch 1, loss: 0.000457</span><br><span class="line">epoch 2, loss: 0.000081</span><br><span class="line">epoch 3, loss: 0.000198</span><br></pre></td></tr></table></figure><h4 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h4><p>线性回归模型适用于<strong>输出为连续值</strong>的情景。</p><p>在另一类情景中，模型输出可以是一个像图像类别这样的<strong>离散值</strong>。对于这样的离散值预测问题，我们可以使用诸如 softmax 回归在内的<strong>分类模型</strong>。</p><p>和线性回归不同，softmax 回归的<strong>输出单元从一个变成了多个</strong>，且引入了 softmax 运算使输出更适合离散值的预测和训练。本节以 softmax 回归模型为例，介绍神经网络中的分类模型。</p><p>softmax 回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax 回归的<strong>输出值个数等于标签里的类别数</strong>。</p><ul><li>模型定义<script type="math/tex; mode=display">o_1=x_1w_{11}+x_2w_{21}+x_3w_{31}+x_4w_{41}+b_1, \\o_1=x_1w_{12}+x_2w_{22}+x_3w_{32}+x_4w_{42}+b_1, \\o_1=x_1w_{13}+x_2w_{23}+x_3w_{33}+x_4w_{43}+b_1. \\\\[2ex]\hat{y}_1,\hat{y}_2,\hat{y}_3=\mathrm{softmax}(o_1,o_2,o_3)</script></li></ul><p>其中：</p><script type="math/tex; mode=display">\hat{y}_1=\frac{\mathrm{exp}⁡(o_1)}{\sum_{i=1}^{3}\mathrm{exp}⁡(o_i)},\hat{y}_2=\frac{\mathrm{exp}⁡(o_2)}{\sum_{i=1}^{3}\mathrm{exp}⁡(o_i)},\hat{y}_3=\frac{\mathrm{exp}⁡(o_3)}{\sum_{i=1}^{3}\mathrm{exp}⁡(o_i)}.</script><ul><li>损失函数：Cross Entropy<script type="math/tex; mode=display">H(\mathbf{y}^{(i)},\mathbf{\hat{y}}^{(i)})=−\sum_{j=1}^{q}y_j^{(i)}\log_⁡{\hat{y}_j^{(i)}},</script>交叉熵只关心<strong>对正确类别</strong>的预测概率，因为只要其值足够大，就可以确保分类结果正确。</li></ul><p>假设训练数据集的样本数为 n，交叉熵损失函数定义为 $\ell(\mathbf{\Theta})=\frac{1}{n}\sum_{i=1}^{n}H(\mathbf{y}^{(i)},\mathbf{\hat{y}}^{(i)}).$ 其中$\mathbf{\Theta}$代表模型参数。</p><p>注意：这里的交叉熵是 H = - [ 实际值 * log(预测值) ] 的求和。而 log(预测值) 却又可以替换为是预测时的 logits oi，二者之间只差了一个系数。因此背后实现可以直接用 logits 参与简化计算。</p><blockquote><p>数据集与相关包的介绍：Fashion-MNIST</p><p>本节我们将使用 torchvision 包，它是服务于 PyTorch 深度学习框架的，主要用来构建计算机视觉模型。 torchvision 主要由以下几部分构成：</p><ol><li><code>torchvision.datasets</code>: 一些加载数据的函数及常用的数据集接口；</li><li><code>torchvision.models</code>: 包含常用的模型结构（含预训练模型），例如 AlexNet、VGG、ResNet 等；</li><li><code>torchvision.transforms</code>: 常用的图片变换，例如裁剪、旋转等；</li><li><code>torchvision.utils</code>: 其他的一些有用的方法。</li></ol></blockquote><h4 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h4><ul><li>仅添加隐藏层：即便再添加更多的隐藏层，将线性隐藏层间彼此相接依然只能与仅含输出层的单层神经网络等价。</li></ul><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。</p><p>解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。</p><p>这个非线性函数被称为激活函数（activation function）。下面我们介绍几个常用的激活函数。</p><ul><li>$Relu(x)=\mathrm{max}(0,x).$</li><li>$sigmoid(x)=\sigma(x)=\frac{1}{1+e^{−x}}.$</li><li>$\tanh(x)=\frac{1−\exp⁡(−2x)}{1+\exp⁡(−2x)}.$</li></ul><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。</p><h3 id="应对过拟合"><a href="#应对过拟合" class="headerlink" title="应对过拟合"></a>应对过拟合</h3><h4 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h4><ul><li><strong>权重衰减</strong>是一种应对过拟合的方法</li><li>权重衰减等价于 $L_2$ 范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。我们先描述 $L_2$ 范数正则化，再解释它为何又称权重衰减。</li></ul><p>$L_2$ 范数正则化在模型原损失函数基础上添加 $L_2$ 范数惩罚项，从而得到训练所需要最小化的函数。</p><p>$L_2$ 范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。即定义新的 Loss Function 为：</p><script type="math/tex; mode=display">\ell(w,b)+\frac{\lambda}{2n}\left \| \mathbf w \right \| ^2</script><p>为什么 $L_2$ Regularization 能起到“权重衰减”的作用呢？我们考虑 Optimizer 的迭代方式…</p><script type="math/tex; mode=display">w_1 \gets (1−\frac{\eta \lambda }{\left | \mathcal{B} \right |})w_1−\frac{η}{|\mathcal{B}|}\sum_{i\in B}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b−y^{(i)}), \\w_2 \gets (1−\frac{\eta \lambda }{\left | \mathcal{B} \right |})w_2−\frac{η}{|\mathcal{B}|}\sum_{i\in B}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b−y^{(i)}).</script><p>这是因为如果我们对损失函数求梯度，就必然带有了和原有的 $w$ 有关的一项。然后我们在做优化迭代的过程中，$L_2$ 范数正则化令权重 $w_1$ 和 $w_2$ 先自乘小于 1 的数，再减去不含惩罚项的梯度。因此，$L_2$ 范数正则化又叫权重衰减。</p><p>权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。实际场景中，我们有时也在惩罚项中添加偏差元素的平方和。</p><h5 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h5><ul><li>定义 L2 范数惩罚项</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2_penalty</span>(<span class="params">w</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (w**<span class="number">2</span>).<span class="built_in">sum</span>() / <span class="number">2</span></span><br></pre></td></tr></table></figure><ul><li>Train (From scratch)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot</span>(<span class="params">lambd</span>):</span></span><br><span class="line">    w, b = init_params()</span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            <span class="comment"># 添加了L2范数惩罚项</span></span><br><span class="line">            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)</span><br><span class="line">            l = l.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> w.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                w.grad.data.zero_()</span><br><span class="line">                b.grad.data.zero_()</span><br><span class="line">            l.backward()</span><br><span class="line">            d2l.sgd([w, b], lr, batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                 <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2 norm of w:&#x27;</span>, w.norm().item())</span><br></pre></td></tr></table></figure><ul><li>Train (Simple)</li></ul><p>这里我们直接在构造优化器实例时通过 <code>weight_decay</code> 参数来指定权重衰减超参数。默认下，PyTorch 会对权重和偏差同时衰减。我们可以分别对权重和偏差构造优化器实例，从而只对权重衰减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fit_and_plot_pytorch</span>(<span class="params">wd</span>):</span></span><br><span class="line">    <span class="comment"># 对权重参数衰减。权重名称一般是以weight结尾</span></span><br><span class="line">    net = nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.weight, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    nn.init.normal_(net.bias, mean=<span class="number">0</span>, std=<span class="number">1</span>)</span><br><span class="line">    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) <span class="comment"># 对权重参数衰减</span></span><br><span class="line">    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  <span class="comment"># 不对偏差参数衰减</span></span><br><span class="line"></span><br><span class="line">    train_ls, test_ls = [], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            l = loss(net(X), y).mean()</span><br><span class="line">            optimizer_w.zero_grad()</span><br><span class="line">            optimizer_b.zero_grad()</span><br><span class="line"></span><br><span class="line">            l.backward()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对两个 optimizer 实例分别调用 step 函数，从而分别更新权重和偏差</span></span><br><span class="line">            optimizer_w.step()</span><br><span class="line">            optimizer_b.step()</span><br><span class="line">        train_ls.append(loss(net(train_features), train_labels).mean().item())</span><br><span class="line">        test_ls.append(loss(net(test_features), test_labels).mean().item())</span><br><span class="line">    d2l.semilogy(<span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), train_ls, <span class="string">&#x27;epochs&#x27;</span>, <span class="string">&#x27;loss&#x27;</span>,</span><br><span class="line">                 <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>), test_ls, [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2 norm of w:&#x27;</span>, net.weight.data.norm().item())</span><br></pre></td></tr></table></figure><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>除了前一节介绍的权重衰减以外，深度学习模型常常使用丢弃法（Dropout）来应对过拟合问题。丢弃法有一些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted dropout）。</p><p>当对某个隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为 $p$，那么有 $p$ 的概率其输出 $h_i$ 会被清零，有 $1 - p$ 的概率 $h_i$ 会除以 $1 − p$ 做拉伸。</p><p>丢弃概率是丢弃法的超参数。具体来说，设随机变量 $\xi_i$ 为 0 和 1 的概率分别为 $p$ 和 $1 − p$ 。使用丢弃法时我们计算新的隐藏单元 $h_i^′=\frac{\xi_i}{1−p}h_i$，由于 $E(\xi_i)=1−p$，因此</p><script type="math/tex; mode=display">E(h_i^′)=\frac{E(\xi_i)}{1−p}h_i=h_i</script><p>即<strong>丢弃法不改变其输入的期望值</strong>。</p><ul><li>实现（Simple）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens1),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob1), <span class="comment"># Here</span></span><br><span class="line">        nn.Linear(num_hiddens1, num_hiddens2), </span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Dropout(drop_prob2), <span class="comment"># Here</span></span><br><span class="line">        nn.Linear(num_hiddens2, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    nn.init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><blockquote><ul><li><strong>数值稳定性</strong></li></ul><p>深度模型有关数值稳定性的典型问题是衰减（vanishing）和爆炸（explosion）。</p><p>当神经网络的层数较多时，模型的数值稳定性容易变差。假设一个层数为 $L$ 的多层感知机的第 $l$ 层 $\mathbf H^{(l)}$ 的权重参数为 $\mathbf W^{(l)}$，输出层 $\mathbf H^{(l)}$ 的权重参数为 $\mathbf W^{(l)}$。</p><p>为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射 ϕ(x)=x。</p><p>给定输入 X，多层感知机的第 l 层的输出 $\mathbf H^{(l)}=\mathbf{XW}^{(1)}\mathbf{W}^{(2)}\dots\mathbf{W}^{(l)}$。</p><p>此时，如果层数l较大，$\mathbf H^{(l)}$ 的计算可能会出现衰减或爆炸。</p><p>举个例子，假设输入和所有层的权重参数都是标量，如权重参数为 0.2 和 5，多层感知机的第 30 层输出为输入 $\mathbf X$ 分别与 $0.2^{30}\approx 1\times10^{−21}$（衰减）和 $5^{30}\approx9\times 10^{20}$（爆炸）的乘积。</p><p>类似地，当层数较多时，梯度的计算也更容易出现衰减或爆炸。</p><ul><li><strong>模型初始化</strong></li></ul><p>在神经网络中，通常需要随机初始化模型参数。下面我们来解释这样做的原因。</p><p>考虑多层感知机模型。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。</p><p>在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。</p><p>之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有 1 个隐藏单元在发挥作用。</p><p>因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。</p><p>PyTorch 中 <code>nn.Module</code> 的模块参数都采取了较为合理的初始化策略。</p></blockquote><h2 id="深度学习计算"><a href="#深度学习计算" class="headerlink" title="深度学习计算"></a>深度学习计算</h2><h3 id="模型构造"><a href="#模型构造" class="headerlink" title="模型构造"></a>模型构造</h3><ul><li>可以通过继承<code>Module</code>类来构造模型，重载 <code>__init__</code> 函数和 <code>forward</code> 函数。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># 声明带有模型参数的层，这里声明了两个全连接层</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="comment"># 调用 MLP 父类 Module 的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数</span></span><br><span class="line">        <span class="comment"># 参数，如“模型参数的访问、初始化和共享”一节将介绍的模型参数 params</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__(**kwargs)</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>) <span class="comment"># 隐藏层</span></span><br><span class="line">        self.act = nn.ReLU()</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)  <span class="comment"># 输出层</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义模型的前向计算，即如何根据输入 x 计算返回所需要的模型输出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        a = self.act(self.hidden(x))</span><br><span class="line">        <span class="keyword">return</span> self.output(a)</span><br></pre></td></tr></table></figure><ul><li><code>Sequential</code>、<code>ModuleList</code>、<code>ModuleDict</code>类都继承自<code>Module</code>类。</li></ul><p><strong><code>Sequential</code></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = MySequential(</span><br><span class="line">        nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(<span class="number">256</span>, <span class="number">10</span>), </span><br><span class="line">        )</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">net(X)</span><br></pre></td></tr></table></figure><p><strong><code>ModuleList</code></strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleList([nn.Linear(<span class="number">784</span>, <span class="number">256</span>), nn.ReLU()])</span><br><span class="line">net.append(nn.Linear(<span class="number">256</span>, <span class="number">10</span>)) <span class="comment"># # 类似 List 的 append 操作</span></span><br><span class="line"><span class="built_in">print</span>(net[-<span class="number">1</span>])  <span class="comment"># 类似 List 的索引访问</span></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># net(torch.zeros(1, 784)) # 会报 NotImplementedError</span></span><br></pre></td></tr></table></figure><p><code>ModuleList</code>仅仅是一个储存各种模块的列表，这些模块之间没有联系也没有顺序（所以不用保证相邻层的输入输出维度匹配），而且没有实现 <code>forward</code> 功能需要自己实现，所以上面执行 <code>net(torch.zeros(1, 784))</code> 会报<code>NotImplementedError</code>；而 <code>Sequential</code> 内的模块需要按照顺序排列，要保证相邻层的输入输出大小相匹配，内部 <code>forward</code> 功能已经实现。此外，<code>ModuleList</code> 不同于一般的 Python 的 <code>list</code>，加入到 <code>ModuleList</code> 里面的所有模块的参数会被自动添加到整个网络中。</p><p><strong><code>ModuleDict</code></strong></p><p><code>ModuleDict</code> 接收一个子模块的字典作为输入, 然后也可以类似字典那样进行添加访问操作。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.ModuleDict(&#123;</span><br><span class="line">    <span class="string">&#x27;linear&#x27;</span>: nn.Linear(<span class="number">784</span>, <span class="number">256</span>),</span><br><span class="line">    <span class="string">&#x27;act&#x27;</span>: nn.ReLU(),</span><br><span class="line">&#125;)</span><br><span class="line">net[<span class="string">&#x27;output&#x27;</span>] = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># 添加</span></span><br><span class="line"><span class="built_in">print</span>(net[<span class="string">&#x27;linear&#x27;</span>]) <span class="comment"># 访问</span></span><br><span class="line"><span class="built_in">print</span>(net.output)</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="comment"># net(torch.zeros(1, 784)) # 会报NotImplementedError</span></span><br></pre></td></tr></table></figure><p>和 <code>ModuleList</code> 一样，<code>ModuleDict</code> 实例仅仅是存放了一些模块的字典，并没有定义 <code>forward</code> 函数需要自己定义。同样，<code>ModuleDict</code> 也与Python的 <code>Dict</code> 有所不同，<code>ModuleDict</code> 里的所有模块的参数会被自动添加到整个网络中。</p><ul><li>与<code>Sequential</code>不同，<code>ModuleList</code>和<code>ModuleDict</code>并没有定义一个完整的网络，它们只是将不同的模块存放在一起，需要自己定义<code>forward</code>函数。</li><li>虽然<code>Sequential</code>等类可以使模型构造更加简单，但直接继承<code>Module</code>类可以极大地拓展模型构造的灵活性。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FancyMLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(FancyMLP, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">        self.rand_weight = torch.rand((<span class="number">20</span>, <span class="number">20</span>), requires_grad=<span class="literal">False</span>) <span class="comment"># 不可训练参数（常数参数）</span></span><br><span class="line">        self.linear = nn.Linear(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="comment"># 使用创建的常数参数，以及 nn.functional 中的 relu 函数和 mm 函数</span></span><br><span class="line">        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 复用全连接层。等价于两个全连接层共享参数</span></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        <span class="comment"># 控制流，这里我们需要调用 item 函数来返回标量进行比较</span></span><br><span class="line">        <span class="keyword">while</span> x.norm().item() &gt; <span class="number">1</span>:</span><br><span class="line">            x /= <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> x.norm().item() &lt; <span class="number">0.8</span>:</span><br><span class="line">            x *= <span class="number">10</span></span><br><span class="line">        <span class="keyword">return</span> x.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><h3 id="模型参数的访问、初始化与共享"><a href="#模型参数的访问、初始化与共享" class="headerlink" title="模型参数的访问、初始化与共享"></a>模型参数的访问、初始化与共享</h3><p>本节用到的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">4</span>, <span class="number">3</span>), nn.ReLU(), nn.Linear(<span class="number">3</span>, <span class="number">1</span>))  <span class="comment"># pytorch 已进行默认初始化</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line">X = torch.rand(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">Y = net(X).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">4</span>, out_features=<span class="number">3</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): ReLU()</span><br><span class="line">  (<span class="number">2</span>): Linear(in_features=<span class="number">3</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><strong>访问模型参数</strong></li></ul><p>对于 <code>Sequential</code> 实例（派生自 <code>Module</code>）中含模型参数的层，我们可以通过 <code>Module</code> 类的 <code>parameters()</code> 或者 <code>named_parameters</code> 方法来访问所有参数（以迭代器的形式返回），后者除了返回参数 <code>Tensor</code> 外还会返回其名字。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">type</span>(net.named_parameters()))</span><br><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">generator</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">&gt;&gt;&gt; <span class="title">for</span> <span class="title">name</span>, <span class="title">param</span> <span class="title">in</span> <span class="title">net</span>.<span class="title">named_parameters</span>():</span></span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(name, param.size())</span><br><span class="line">...</span><br><span class="line"><span class="number">0.</span>weight torch.Size([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="number">0.</span>bias torch.Size([<span class="number">3</span>])</span><br><span class="line"><span class="number">2.</span>weight torch.Size([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="number">2.</span>bias torch.Size([<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>接下来访问单层的参数。对于使用 <code>Sequential</code> 类构造的神经网络，我们可以通过方括号 <code>[]</code> 来访问网络的任一层。索引 0 表示隐藏层为 <code>Sequential</code> 实例最先添加的层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> name, param <span class="keyword">in</span> net[<span class="number">0</span>].named_parameters():</span><br><span class="line"><span class="meta">... </span>    <span class="built_in">print</span>(name, param.size(), <span class="built_in">type</span>(param))</span><br><span class="line">...</span><br><span class="line">weight torch.Size([<span class="number">3</span>, <span class="number">4</span>]) &lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">parameter</span>.<span class="title">Parameter</span>&#x27;&gt;</span></span><br><span class="line"><span class="class"><span class="title">bias</span> <span class="title">torch</span>.<span class="title">Size</span>(<span class="params">[<span class="number">3</span>]</span>) &lt;<span class="title">class</span> &#x27;<span class="title">torch</span>.<span class="title">nn</span>.<span class="title">parameter</span>.<span class="title">Parameter</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure><p>返回的 <code>param</code> 的类型为 <code>torch.nn.parameter.Parameter</code>，其实这是 <code>Tensor</code> 的子类，和 <code>Tensor</code> 不同的是如果一个 <code>Tensor</code> 是 <code>Parameter</code>，那么它会自动被添加到模型的参数列表里。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__(**kwargs)</span><br><span class="line">        self.weight1 = nn.Parameter(torch.rand(<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">        self.weight2 = torch.rand(<span class="number">20</span>, <span class="number">20</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">n = MyModel()</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> n.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br><span class="line">    </span><br><span class="line">Output:</span><br><span class="line">weight1</span><br></pre></td></tr></table></figure><ul><li><strong>初始化模型参数</strong></li></ul><p>PyTorch 中 <code>nn.Module</code> 的模块参数都采取了较为合理的初始化策略，但我们经常需要使用其他方法来初始化权重。</p><p>PyTorch 的 <code>init</code> 模块里提供了多种预设的初始化方法。在下面的例子中，我们将权重参数初始化成均值为 0、标准差为 0.01 的正态分布随机数，并依然将偏差参数清零。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"><span class="number">0.</span>weight tensor([[ <span class="number">0.0030</span>,  <span class="number">0.0094</span>,  <span class="number">0.0070</span>, -<span class="number">0.0010</span>],</span><br><span class="line">        [ <span class="number">0.0001</span>,  <span class="number">0.0039</span>,  <span class="number">0.0105</span>, -<span class="number">0.0126</span>],</span><br><span class="line">        [ <span class="number">0.0105</span>, -<span class="number">0.0135</span>, -<span class="number">0.0047</span>, -<span class="number">0.0006</span>]])</span><br><span class="line"><span class="number">2.</span>weight tensor([[-<span class="number">0.0074</span>,  <span class="number">0.0051</span>,  <span class="number">0.0066</span>]])</span><br></pre></td></tr></table></figure><p>下面使用常数来初始化权重参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;bias&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init.constant_(param, val=<span class="number">0</span>)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line"><span class="number">0.</span>bias tensor([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line"><span class="number">2.</span>bias tensor([<span class="number">0.</span>])</span><br></pre></td></tr></table></figure><p>有时候我们需要的初始化方法并没有在 <code>init</code> 模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它。</p><p>首先参考 normal_ 的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normal_</span>(<span class="params">tensor, mean=<span class="number">0</span>, std=<span class="number">1</span></span>):</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">return</span> tensor.normal_(mean, std)</span><br></pre></td></tr></table></figure><p>类似地，我们可以实现自定义的初始化方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weight_</span>(<span class="params">tensor</span>):</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        tensor.uniform_(-<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">        tensor *= (tensor.<span class="built_in">abs</span>() &gt;= <span class="number">5</span>).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;weight&#x27;</span> <span class="keyword">in</span> name:</span><br><span class="line">        init_weight_(param)</span><br><span class="line">        <span class="built_in">print</span>(name, param.data)</span><br></pre></td></tr></table></figure><p>此外，我们还可以通过直接改变这些参数的 <code>data</code> 来达到改写模型参数值的同时不会影响梯度的效果。</p><ul><li><strong>共享模型参数</strong></li></ul><p>在有些情况下，我们希望在多个层之间共享模型参数。</p><p>共享模型参数的方法: <code>Module</code> 类的 <code>forward</code> 函数里多次调用同一个层。</p><p>此外，如果我们传入 <code>Sequential</code> 的模块是同一个 <code>Module</code> 实例的话参数也是共享的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">linear = nn.Linear(<span class="number">1</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">net = nn.Sequential(linear, linear) </span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    init.constant_(param, val=<span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(name, param.data)</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br><span class="line"><span class="number">0.</span>weight tensor([[<span class="number">3.</span>]]) <span class="comment"># 这里只输出了一次参数列表，证明二者共享</span></span><br></pre></td></tr></table></figure><p>因为模型参数里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的。</p><h3 id="自定义-Layer"><a href="#自定义-Layer" class="headerlink" title="自定义 Layer"></a>自定义 Layer</h3><p>本节将介绍如何使用 <code>Module</code>来自定义层，从而可以被重复调用。</p><ul><li><strong>不含模型参数的自定义层</strong></li></ul><p>事实上，自定义层和自定义模型类似，因为我们可以直接把一个 Packed 的模型视为是一个 Layer。</p><p>举个例子，下面的 <code>CenteredLayer</code> 类通过继承 <code>Module</code> 类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了 <code>forward</code> 函数里。这个层里不含模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CenteredLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CenteredLayer, self).__init__(**kwargs)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x - x.mean()</span><br></pre></td></tr></table></figure><p>然后，我们就可以实例化这个 Layer，然后做 Forward Feeding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">layer = CenteredLayer()</span><br><span class="line">layer(torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], dtype=torch.<span class="built_in">float</span>))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor([-<span class="number">2.</span>, -<span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>])</span><br></pre></td></tr></table></figure><ul><li><strong>含模型参数的自定义层</strong></li></ul><p>之前我们介绍了 <code>Parameter</code> 类其实是 <code>Tensor</code> 的子类，如果一个 <code>Tensor</code> 是 <code>Parameter</code>，那么它会自动被添加到模型的参数列表里。 // 在这里可以推测这个参数列表是 <code>nn.Module</code> 的数据成员…?</p><p>所以在自定义含模型参数的层时，我们应该将参数定义成 <code>Parameter</code>。</p><p>除了直接定义成 <code>Parameter</code> 类外，还可以使用 <code>ParameterList</code> 和 <code>ParameterDict</code> 分别定义参数的列表和字典。</p><ul><li><strong><code>ParameterList</code></strong></li></ul><p><code>ParameterList</code>接收一个 <code>Parameter</code> 实例的列表作为输入然后得到一个参数列表，使用的时候可以用索引来访问某个参数，另外也可以使用 <code>append</code> 和 <code>extend</code> 在列表后面新增参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterList([nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>)])</span><br><span class="line">        self.params.append(nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.params)):</span><br><span class="line">            x = torch.mm(x, self.params[i])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net = MyDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">MyDense(</span><br><span class="line">  (params): ParameterList(</span><br><span class="line">      (0): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (1): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (2): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (3): Parameter containing: [torch.FloatTensor of size 4x1]</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><ul><li><strong><code>ParameterDict</code></strong></li></ul><p><code>ParameterDict</code> 接收一个 <code>Parameter</code> 实例的字典作为输入然后得到一个参数字典，然后可以按照字典的规则使用了。</p><p>例如使用 <code>update()</code> 新增参数，使用 <code>keys()</code> 返回所有键值，使用 <code>items()</code> 返回所有键值对等等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDictDense</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyDictDense, self).__init__()</span><br><span class="line">        self.params = nn.ParameterDict(&#123;</span><br><span class="line">                <span class="string">&#x27;linear1&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">4</span>)),</span><br><span class="line">                <span class="string">&#x27;linear2&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line">        &#125;)</span><br><span class="line">        self.params.update(&#123;<span class="string">&#x27;linear3&#x27;</span>: nn.Parameter(torch.randn(<span class="number">4</span>, <span class="number">2</span>))&#125;) <span class="comment"># 新增</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, choice=<span class="string">&#x27;linear1&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> torch.mm(x, self.params[choice])</span><br><span class="line"></span><br><span class="line">net = MyDictDense()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">MyDictDense(</span><br><span class="line">  (params): ParameterDict(</span><br><span class="line">      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]</span><br><span class="line">      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]</span><br><span class="line">      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>于是我们可以根据不同的 key 进行不同的 forward feeding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear1&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear2&#x27;</span>))</span><br><span class="line"><span class="built_in">print</span>(net(x, <span class="string">&#x27;linear3&#x27;</span>))</span><br><span class="line"></span><br><span class="line">Output:</span><br><span class="line">tensor([[<span class="number">1.5082</span>, <span class="number">1.5574</span>, <span class="number">2.1651</span>, <span class="number">1.2409</span>]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">tensor([[-<span class="number">0.8783</span>]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">tensor([[ <span class="number">2.2193</span>, -<span class="number">1.6539</span>]], grad_fn=&lt;MmBackward&gt;)</span><br></pre></td></tr></table></figure><h3 id="读取与存储"><a href="#读取与存储" class="headerlink" title="读取与存储"></a>读取与存储</h3><p>到目前为止，我们介绍了如何处理数据以及如何构建、训练和测试深度学习模型。</p><p>然而在实际中，我们有时需要把训练好的模型部署到很多不同的设备。</p><p>在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。</p><ul><li><strong>读写 <code>Tensor</code></strong></li></ul><p>我们可以直接使用 <code>save</code> 函数和 <code>load</code> 函数分别存储和读取 <code>Tensor</code>。</p><p><code>save</code> 使用 Python 的 pickle 库将对象进行序列化，然后将序列化的对象保存到硬盘。</p><p>使用 <code>save</code> 可以保存各种对象，包括 <code>nn.Module</code>, <code>Tensor</code>, <code>dict</code> 等等。</p><p>而 <code>load</code> 使用 unpickle 工具将 pickle 的对象文件反序列化为内存。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">3</span>)</span><br><span class="line">torch.save(x, <span class="string">&#x27;x.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">x2 = torch.load(<span class="string">&#x27;x.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(x2) <span class="comment"># tensor([1., 1., 1.])</span></span><br><span class="line"></span><br><span class="line">y = torch.zeros(<span class="number">4</span>)</span><br><span class="line">torch.save([x, y], <span class="string">&#x27;xy.pt&#x27;</span>)</span><br><span class="line">xy_list = torch.load(<span class="string">&#x27;xy.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(xy_list) <span class="comment"># [tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]</span></span><br><span class="line"></span><br><span class="line">torch.save(&#123;<span class="string">&#x27;x&#x27;</span>: x, <span class="string">&#x27;y&#x27;</span>: y&#125;, <span class="string">&#x27;xy_dict.pt&#x27;</span>)</span><br><span class="line">xy = torch.load(<span class="string">&#x27;xy_dict.pt&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(xy) <span class="comment"># &#123;&#x27;x&#x27;: tensor([1., 1., 1.]), &#x27;y&#x27;: tensor([0., 0., 0., 0.])&#125;</span></span><br></pre></td></tr></table></figure><ul><li><strong>读写模型</strong></li></ul><p>PyTorch 中保存和加载训练模型有两种常见的方法:</p><ol><li>仅保存和加载模型参数(<code>state_dict</code>)；</li><li>保存和加载整个模型。</li></ol><p><strong>保存和加载模型的 <code>state_dict()</code> 成员（Recommended）</strong></p><p>保存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH) <span class="comment"># 推荐的文件后缀名是 pt 或 pth</span></span><br></pre></td></tr></table></figure><p>加载：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br></pre></td></tr></table></figure><h3 id="GPU-计算"><a href="#GPU-计算" class="headerlink" title="GPU 计算"></a>GPU 计算</h3><p>到目前为止，我们一直在使用 CPU 计算。</p><p>对复杂的神经网络和大规模的数据来说，使用 CPU 来计算可能不够高效。</p><p>在本节中，我们将介绍如何使用单块 NVIDIA GPU 来计算。</p><p>可以通过 <code>nvidia-smi</code> 命令来查看显卡信息。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Wed Jan 26 11:48:28 2022</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 457.49       Driver Version: 457.49       CUDA Version: 11.1     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce GTX 1650   WDDM  | 00000000:01:00.0 Off |                  N/A |</span><br><span class="line">| N/A   43C    P8     4W /  N/A |    359MiB /  4096MiB |      8%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br></pre></td></tr></table></figure><ul><li><strong>计算设备</strong></li></ul><p>PyTorch 可以指定用来存储和计算的设备，如使用内存的 CPU 或者使用显存的 GPU。</p><p>默认情况下，PyTorch 会将数据创建在内存，然后利用 CPU 来计算。</p><p>用 <code>torch.cuda.is_available()</code> 查看 GPU 是否可用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">torch.cuda.is_available() <span class="comment"># 输出 True</span></span><br></pre></td></tr></table></figure><p>GPU 的相关信息查询：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.device_count() <span class="comment"># 查看 GPU 数量，输出 1</span></span><br><span class="line">torch.cuda.current_device() <span class="comment"># 查看当前 GPU 索引号，输出 0</span></span><br><span class="line">torch.cuda.get_device_name(<span class="number">0</span>) <span class="comment"># 根据索引号查看 GPU 名字，输出 &#x27;GeForce GTX 1050&#x27;</span></span><br></pre></td></tr></table></figure><ul><li><strong><code>Tensor</code> 的 GPU 计算</strong></li></ul><p>默认情况下，<code>Tensor</code> 会被存在内存上。因此，之前我们每次打印 <code>Tensor</code> 的时候看不到 GPU 相关标识。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># tensor([1, 2, 3])</span></span><br></pre></td></tr></table></figure><p>使用 <code>.cuda()</code> 可以将CPU上的 <code>Tensor</code> 转换（复制）到GPU上。</p><p>如果有多块GPU，我们用 <code>.cuda(i)</code>来表示第 i 块 GPU 及相应的显存（i 从 0 开始）且 <code>cuda(0)</code> 和 <code>cuda()</code> 等价。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = x.cuda(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span></span><br></pre></td></tr></table></figure><p>可以通过 <code>Tensor</code> 的 <code>device</code> 属性来查看该 <code>Tensor</code> 所在的设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.device) <span class="comment"># device(type=&#x27;cuda&#x27;, index=0)</span></span><br></pre></td></tr></table></figure><p>可以直接在创建的时候就指定设备。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], device=device)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]).to(device)</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># tensor([1, 2, 3], device=&#x27;cuda:0&#x27;)</span></span><br></pre></td></tr></table></figure><p>如果对在 GPU 上的数据进行运算，那么结果还是存放在 GPU 上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x**<span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y) <span class="comment"># tensor([1, 4, 9], device=&#x27;cuda:0&#x27;)</span></span><br></pre></td></tr></table></figure><p>需要注意的是，<strong>存储在不同位置中的数据是不可以直接进行计算的</strong>。即存放在 CPU 上的数据不可以直接与存放在 GPU 上的数据进行运算，位于不同 GPU 上的数据也是不能直接进行计算的。</p><ul><li><strong>模型的 GPU 计算</strong></li></ul><p>同 <code>Tensor</code> 类似，PyTorch 模型也可以用类似的方式转移到 GPU 上。</p><ul><li><code>.cuda(i)</code></li><li><code>.cpu()</code></li><li><code>.to(device)</code></li></ul><p>我们也可以通过检查模型的参数的 <code>device</code> 属性来查看存放模型的设备。</p><p>同样的，需要保证模型输入的 <code>Tensor</code> 和模型都在同一设备上，否则会报错。</p>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
          <category> 理论/机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>「QRCode 标准阅读」#1 构成及数据编码</title>
      <link href="/2022/04/17/qrcode/"/>
      <url>/2022/04/17/qrcode/</url>
      
        <content type="html"><![CDATA[<h2 id="基础描述及结构（6-1、6-3）"><a href="#基础描述及结构（6-1、6-3）" class="headerlink" title="基础描述及结构（6.1、6.3）"></a>基础描述及结构（6.1、6.3）</h2><h3 id="基础描述（5-3、6-1）"><a href="#基础描述（5-3、6-1）" class="headerlink" title="基础描述（5.3、6.1）"></a>基础描述（5.3、6.1）</h3><ul><li>块位置：左上角为原点 (0, 0) 向下x+，向右y+</li><li>版本表示：Version V-E（其中V是版本号，E是纠错等级）</li><li>数据表示：黑块-1 白块-0（可以带背景全部反色）</li><li>大小：从版本1到版本40依次是 21x21 ～ 177x177（每增加一个版本，边长增加4）</li><li>支持的最多字符数（版本40）<ul><li>数字模式：7089</li><li>字母模式：4296</li><li>字节模式：2953</li><li>日文模式：1817</li></ul></li><li>纠错等级允许的恢复比例<ul><li>L：7%</li><li>M：15%</li><li>Q：25%</li><li>H：30%</li></ul></li></ul><h3 id="二维码结构（6-3）"><a href="#二维码结构（6-3）" class="headerlink" title="二维码结构（6.3）"></a>二维码结构（6.3）</h3><ul><li>功能图案（function patterns）<ul><li>特征符（finder pattern）7x7黑圈 5x5白圈 3x3黑块</li><li>分割线（separator）在特征符周围的一圈全白区域</li><li>时序图案（timing patterns）第7行第7列的两条黑白条纹</li><li>对齐图案（alignment patterns）版本1无，版本2-6 1个，版本7-13 6个……（附录E）</li><li>静默区（quiet zone）至少4个单位宽</li></ul></li><li>编码区域（encoding region）<ul><li>格式信息（format information）左上角分割线外一圈，左下角分割线右侧，右上角分割线下侧</li><li>版本信息（version information）版本7后才有，在左下分割线上侧，右上分割线左侧</li><li>数据及纠错码区域</li></ul></li></ul><p><img src="https://s2.loli.net/2022/04/17/WC3vjUgRtysr5ED.png" alt="structure"></p><h2 id="数据编码（7-4）"><a href="#数据编码（7-4）" class="headerlink" title="数据编码（7.4）"></a>数据编码（7.4）</h2><h3 id="数据序列（7-4-1）"><a href="#数据序列（7-4-1）" class="headerlink" title="数据序列（7.4.1）"></a>数据序列（7.4.1）</h3><p>默认的 ECI 模式下，比特流以模式标识符开始。如果不是默认 ECI 模式，则需要从 ECI 头开始：</p><ul><li>（4 bits）ECI 模式标识符</li><li>（8/16/24 bits）ECI Designator</li></ul><p>比特流的剩余部分由下面几部分组成：</p><ul><li>（4 bits）模式标识符</li><li>字符数量标识符（长度见下第二个表）</li><li>数据比特流</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">模式</th><th style="text-align:center">标识符</th><th style="text-align:center">说明</th></tr></thead><tbody><tr><td style="text-align:center">ECI</td><td style="text-align:center">0111</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">数字模式</td><td style="text-align:center">0001</td><td style="text-align:center">只包含数字0-9，3个数字 10 bits</td></tr><tr><td style="text-align:center">字母数字模式</td><td style="text-align:center">0010</td><td style="text-align:center">45个字符，0-9A-Z 及9个符号 空格$%*+-./:，2个字符 11 bits</td></tr><tr><td style="text-align:center">字节模式</td><td style="text-align:center">0100</td><td style="text-align:center">每个字符 8 bits</td></tr><tr><td style="text-align:center">日本汉字模式</td><td style="text-align:center">1000</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">结构添加模式</td><td style="text-align:center">0011</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th style="text-align:center">版本</th><th style="text-align:center">数字模式字符数量标识符长度</th><th style="text-align:center">字母模式…</th><th style="text-align:center">字节模式…</th><th style="text-align:center">日文模式…</th></tr></thead><tbody><tr><td style="text-align:center">1~9</td><td style="text-align:center">10</td><td style="text-align:center">9</td><td style="text-align:center">8</td><td style="text-align:center">8</td></tr><tr><td style="text-align:center">10～26</td><td style="text-align:center">12</td><td style="text-align:center">11</td><td style="text-align:center">16</td><td style="text-align:center">10</td></tr><tr><td style="text-align:center">27～40</td><td style="text-align:center">14</td><td style="text-align:center">13</td><td style="text-align:center">16</td><td style="text-align:center">12</td></tr></tbody></table></div><h3 id="ECI-模式（7-4-2）"><a href="#ECI-模式（7-4-2）" class="headerlink" title="ECI 模式（7.4.2）"></a>ECI 模式（7.4.2）</h3><p>ECI 模式即使用某些特定的字符映射来把字符转换为比特流</p><p>而且都使用字节模式来表示数据（即在 ECI 头后的模式标识符为字节模式的 0100）</p><p>每个 ECI 都有一个六位数编号（assignment value），可能占 1、2、3 个 codewords，具体标识方式见下表（占1个 codewords 时开头一定是0，占2个时开头一定是10，占3个时开头一定是110）表中 xxxxxxxx 表示编号的二进制</p><div class="table-container"><table><thead><tr><th style="text-align:center">ECI Assignment Value</th><th style="text-align:center">Codewords values</th></tr></thead><tbody><tr><td style="text-align:center">000000 ～ 000127</td><td style="text-align:center">0xxxxxxx</td></tr><tr><td style="text-align:center">000000 ～ 016383</td><td style="text-align:center">10xxxxxx xxxxxxxx</td></tr><tr><td style="text-align:center">000000 ～ 999999</td><td style="text-align:center">110xxxxx xxxxxxxx xxxxxxxx</td></tr></tbody></table></div><p>而且 ECI 模式下中途可以更换 ECI 指示器，一个 5C（01011100）表示换新的 ECI，后面要接6个 codewords 即6个数字（十六进制30～39）表示编号，而不是用上表中的表示方法。而 5C 正常情况下表示 \ ，所以表示 \ 这个原数据需要用两个 5C</p><ul><li><strong>例子 1</strong><ul><li>使用 <code>ISO/IEC 8859-7（ECI 000009）</code> 来表示希腊字母 <code>ΑΒΓΔΕ</code>（该 ECI 下表示为十六进制 A1 A2 A3 A4 A5）</li><li>比特流：<ul><li>ECI 标识符：<code>0111</code></li><li>ECI 编号：<code>00001001</code></li><li>字节模式标识符：<code>0100</code></li><li>字符数量：<code>00000101</code>（5个字符）</li><li>数据：<code>10100001</code> <code>10100010</code> <code>10100011</code> <code>10100100</code> <code>10100101</code></li></ul></li><li>所以最终的比特流：<code>0111</code> <code>00001001</code> <code>0100</code> <code>00000101</code> <code>10100001</code> <code>10100010</code> <code>10100011</code> <code>10100100</code> <code>10100101</code></li></ul></li><li><strong>例子 2（14.3）</strong><ul><li>需要编码的数据：ABC\123456<ul><li>数据流中十六进制（字节模式标识符0100后）：41 42 43 5C 5C 31 32 33 34 35 36</li></ul></li><li>需要编码的数据：ABC&lt;后接 <code>ECI 123456</code> 下的数据……&gt;<ul><li>数据流中十六进制（字节模式标识符0100后）：41 42 43 5C 31 32 33 34 35 36 ……</li></ul></li></ul></li></ul><h3 id="数字模式（7-4-3）"><a href="#数字模式（7-4-3）" class="headerlink" title="数字模式（7.4.3）"></a>数字模式（7.4.3）</h3><p>输入的数字字符串（因为开头可以是0）要被分成3个一组，每组会转换为 10 bits 的二进制串（999 -&gt; <code>1111100111</code>）。剩余不到3个的部分，如果剩2个数字，则将其转换为 7 bits 的二进制串（99 -&gt; <code>1100011</code>）如果剩1个数字，则将其转换为 4 bits 的二进制串（9 -&gt; <code>1001</code>）</p><p>然后开头加上数字模式标识符 <code>0001</code> 和数量标识符（字符个数转为二进制，并开头补0至长度，长度由版本决定，见上 7.4.1 部分的第二个表）</p><ul><li><strong>例子</strong><ul><li>数据内容： <code>01234567</code>（保留开头0）</li><li>数据流部分：<ul><li>数字模式标识符： <code>0001</code></li><li>数量标识符： <code>0000001000</code>（8，且版本1下规定为 10 bits）</li><li>数据：<ul><li>012 -&gt; <code>0000001100</code></li><li>345 -&gt; <code>0101011001</code></li><li>67 -&gt; <code>1000011</code></li></ul></li></ul></li><li>完整数据比特流： <code>0001</code> <code>0000001000</code> <code>0000001100</code> <code>0101011001</code> <code>1000011</code></li></ul></li></ul><p>数字模式下的比特流长度为：</p><script type="math/tex; mode=display">B = M + C + 10 \times \left \lfloor \frac{D}{3} \right \rfloor + R</script><p>其中 M 为 4，C 为数量标识符长度（版本1～9为 10，版本10～26为 12，版本27～40为 14），D为输入字符个数，R为剩余部分（若 D mod 3 = 0 则为 0，若 D mod 3 = 1 则为 4，若 D mod 3 = 2 则为 7）</p><h3 id="字母数字模式（7-4-4）"><a href="#字母数字模式（7-4-4）" class="headerlink" title="字母数字模式（7.4.4）"></a>字母数字模式（7.4.4）</h3><p>数字字母模式（Alphanumeric mode）下支持的编码字符有45个，把它们从0编号至44。其中 0-9 对应数字 0-9，10-35 对应字母 A-Z，36-44 对应9个符号：</p><p><img src="https://s2.loli.net/2022/04/17/UtIqfcXo7NkgOYz.png" alt="alnum.png"></p><p>输入的字符先按照上表转换为数值，然后分为两个一组，每一组内把 第一个数值 × 45 + 第二个数值，再转换为长度为 11 bits 的二进制串（最大为 44×45+44=2024 -&gt; <code>11111101000</code>）。如果字符长度为奇数，则会剩余出一个字符，需要将其值转换为长度为 6 bits 的二进制串（最大为 11 -&gt; <code>101100</code>）</p><p>然后开头加上字母数字模式标识符 <code>0010</code> 和数量标识符（长度由 7.4.1 第二个表规定）</p><ul><li><strong>例子</strong><ul><li>数据内容：AC-42</li><li>数据流部分：<ul><li>字母数字模式标识符： <code>0010</code></li><li>数量标识符： <code>000000101</code>（5，且版本1下规定长度为9）</li><li>数据：AC-42 -&gt; 10 12 41 4 2 -&gt; (10 12)(41 4)(2)<ul><li>10 12 -&gt; 10*45+12=462 -&gt; <code>00111001110</code></li><li>41 4 -&gt; 41*45+4=1849 -&gt; <code>11100111001</code></li><li>2 -&gt; 2 -&gt; <code>000010</code></li></ul></li></ul></li><li>完整数据比特流： <code>0010</code> <code>000000101</code> <code>00111001110</code> <code>11100111001</code> <code>000010</code></li><li>字母数字模式下的比特流长度为：</li></ul></li></ul><script type="math/tex; mode=display">B = M + C + 11 \times \left \lfloor \frac{D}{2} \right \rfloor + 6 \times (D mod 2)</script><p>其中 M 为 4，C 为数量标识符长度，D 为原数据长度</p><h3 id="字节模式（7-4-5）"><a href="#字节模式（7-4-5）" class="headerlink" title="字节模式（7.4.5）"></a>字节模式（7.4.5）</h3><p>字节模式（Byte mode）下把每个字符根据 <code>Latin-1（ISO/IEC 8859-1）</code> 编码成 8 bits（1字节），直接接在字节模式标识符 <code>0100</code> 和数量标识符（长度由 7.4.1 第二个表规定）的后面。</p><p><img src="https://s2.loli.net/2022/04/17/zRFdfvsejuLYayD.png" alt="Latin-1"></p><p>字节模式下的比特流长度：</p><script type="math/tex; mode=display">B = M + C + 8 \times D</script><p>其中 M 为 4，C 为数量标识符长度，D 为原数据长度</p><p><strong>中文编码</strong><br>中文在转换成比特流的时候也使用字节模式，需要用 UTF-8 编码，每个字符会被编码成 3 个字节</p><h3 id="混合模式（7-4-7）"><a href="#混合模式（7-4-7）" class="headerlink" title="混合模式（7.4.7）"></a>混合模式（7.4.7）</h3><p>一个二维码的数据流中也可以使用多种模式，且不需要特别表示。更换新的模式时只需要正常添加 模式标识符+数量标识符+数据 即可</p><p><img src="https://s2.loli.net/2022/04/17/93CQrZiuOqMmnbk.png" alt="multi"></p><ul><li><strong>例子</strong><ul><li>原始数据：123测试</li><li>数据流：<ul><li>数字模式：<ul><li>标识符： <code>0001</code></li><li>数量标识符： <code>0000000011</code>（3，长度10）</li><li>数据：123 -&gt; <code>0001111011</code></li></ul></li><li>字节模式：测试 -&gt; E6 B5 8B / E8 AF 95<ul><li>标识符： <code>0100</code></li><li>数量标识符： <code>00000110</code>（6，长度8）</li><li>数据：<ul><li>测 -&gt; <code>11100110</code> <code>10110101</code> <code>10001011</code></li><li>试 -&gt; <code>11101000</code> <code>10101111</code> <code>10010101</code></li></ul></li></ul></li></ul></li><li>完整数据比特流： <code>0001</code> <code>0000000011</code> <code>0001111011</code> <code>0100</code> <code>00000110</code> <code>11100110</code> <code>10110101</code> <code>10001011</code> <code>11101000</code> <code>10101111</code> <code>10010101</code></li></ul></li></ul><h3 id="结束符（7-4-9）"><a href="#结束符（7-4-9）" class="headerlink" title="结束符（7.4.9）"></a>结束符（7.4.9）</h3><p>在数据的末尾要填充4个0作为结束符，如果容量不足的话可以缩短或省略<br>即能填下则加4个0，填不下则能加几个0就加几个0</p><h3 id="填充-padding-bits（7-4-10）"><a href="#填充-padding-bits（7-4-10）" class="headerlink" title="填充 padding bits（7.4.10）"></a>填充 padding bits（7.4.10）</h3><p>转换后的数据比特流还需要填充至二维码的数据容量</p><ol><li>首先先用 <code>0</code> 补充比特流长度到 8 的整数倍</li><li>然后用 <code>11101100</code> 和 <code>00010001</code> 交替填补到二维码数据容量<br>具体的数据容量由版本号和纠错等级决定，且数据容量（比特）一定为8的倍数，完整数据见文档的 33～36 页（整个 pdf 的第 41～44 页）</li></ol><blockquote><p>注：这个地方 QRazyBox 网站存在 bug，有时无法正常识别填充的 0 比特和 padding bits（即可能把填充的 0 中前四个视为一个 terminator，把后面的 0 才视为属于 padding bits ）</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 技术/CTF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QRCode </tag>
            
            <tag> CTF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python 函数装饰器</title>
      <link href="/2022/03/26/python-func-decorators/"/>
      <url>/2022/03/26/python-func-decorators/</url>
      
        <content type="html"><![CDATA[<p>Python 函数的装饰器是<strong>修改其他函数功能的函数</strong>。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>首先，我们需要将 Python 中的<code>函数</code>作为函数对象来理解。</p><p><strong>函数名只是引用标识名</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span>(<span class="params">name=<span class="string">&quot;yasoob&quot;</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hi &quot;</span> + name</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(hi())</span><br><span class="line"><span class="comment"># output: &#x27;hi yasoob&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 我们甚至可以将一个函数赋值给一个变量，比如</span></span><br><span class="line">greet = hi</span><br><span class="line"><span class="comment"># 我们这里没有在使用小括号，因为我们并不是在调用hi函数</span></span><br><span class="line"><span class="comment"># 而是在将它放在greet变量里头。我们尝试运行下这个</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(greet())</span><br><span class="line"><span class="comment"># output: &#x27;hi yasoob&#x27;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 如果我们删掉旧的hi函数，看看会发生什么！</span></span><br><span class="line"><span class="keyword">del</span> hi</span><br><span class="line"><span class="built_in">print</span>(hi())</span><br><span class="line"><span class="comment">#outputs: NameError</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(greet())</span><br><span class="line"><span class="comment">#outputs: &#x27;hi yasoob&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>函数的返回值可以是函数对象</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span>(<span class="params">name=<span class="string">&quot;yasoob&quot;</span></span>):</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greet</span>():</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;now you are in the greet() function&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">welcome</span>():</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;now you are in the welcome() function&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> name == <span class="string">&quot;yasoob&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> greet</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> welcome</span><br><span class="line">    </span><br><span class="line">a = hi()</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment">#outputs: &lt;function greet at 0x7f2143c01500&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#上面清晰地展示了`a`现在指向到hi()函数中的greet()函数</span></span><br><span class="line"><span class="comment">#现在试试这个</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(a())</span><br><span class="line"><span class="comment">#outputs: now you are in the greet() function</span></span><br></pre></td></tr></table></figure><p><strong>函数的参数可以是函数对象</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hi</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hi yasoob!&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">doSomethingBeforeHi</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;I am doing some boring work before executing hi()&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(func())</span><br><span class="line"></span><br><span class="line">doSomethingBeforeHi(hi)</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing hi()</span></span><br><span class="line"><span class="comment">#        hi yasoob!</span></span><br></pre></td></tr></table></figure><h2 id="So-what-is-a-decorator"><a href="#So-what-is-a-decorator" class="headerlink" title="So what is a decorator?"></a>So what is a decorator?</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span>(<span class="params">a_func</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span>():</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I am doing some boring work before executing a_func()&quot;</span>)</span><br><span class="line"></span><br><span class="line">        a_func()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I am doing some boring work after executing a_func()&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;I am the function which needs some decoration to remove my foul smell&quot;</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: &quot;I am the function which needs some decoration to remove my foul smell&quot;</span></span><br><span class="line"> </span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br><span class="line"><span class="comment">#now a_function_requiring_decoration is wrapped by wrapTheFunction()</span></span><br><span class="line"> </span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs:I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#        I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#        I am doing some boring work after executing a_func()</span></span><br></pre></td></tr></table></figure><p>这段代码等价于…</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Hey you! Decorate me!&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;I am the function which needs some decoration to &quot;</span></span><br><span class="line">          <span class="string">&quot;remove my foul smell&quot;</span>)</span><br><span class="line"></span><br><span class="line">a_function_requiring_decoration()</span><br><span class="line"><span class="comment">#outputs: I am doing some boring work before executing a_func()</span></span><br><span class="line"><span class="comment">#         I am the function which needs some decoration to remove my foul smell</span></span><br><span class="line"><span class="comment">#         I am doing some boring work after executing a_func()</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#the @a_new_decorator is just a short way of saying:</span></span><br><span class="line">a_function_requiring_decoration = a_new_decorator(a_function_requiring_decoration)</span><br></pre></td></tr></table></figure><p>但是，这样的代码存在的问题有，比如，<code>__name__</code>获取不到正确的函数名。</p><p>于是稍加修改，我们有了以下解释器函数的编写模板：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_new_decorator</span>(<span class="params">a_func</span>):</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">a_func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapTheFunction</span>():</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I am doing some boring work before executing a_func()&quot;</span>)</span><br><span class="line">        a_func()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;I am doing some boring work after executing a_func()&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> wrapTheFunction</span><br><span class="line"> </span><br><span class="line"><span class="meta">@a_new_decorator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_function_requiring_decoration</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Hey yo! Decorate me!&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;I am the function which needs some decoration to &quot;</span></span><br><span class="line">          <span class="string">&quot;remove my foul smell&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a_function_requiring_decoration.__name__)</span><br><span class="line"><span class="comment"># Output: a_function_requiring_decoration</span></span><br></pre></td></tr></table></figure><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logit</span>(<span class="params">func</span>):</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_logging</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(func.__name__ + <span class="string">&quot; was called&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> with_logging</span><br><span class="line"> </span><br><span class="line"><span class="meta">@logit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addition_func</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Do some math.&quot;&quot;&quot;</span></span><br><span class="line">   <span class="keyword">return</span> x + x</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">result = addition_func(<span class="number">4</span>)</span><br><span class="line"><span class="comment"># Output: addition_func was called</span></span><br></pre></td></tr></table></figure><h2 id="Decorators-with-parameters"><a href="#Decorators-with-parameters" class="headerlink" title="Decorators with parameters"></a>Decorators with parameters</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logit</span>(<span class="params">logfile=<span class="string">&#x27;out.log&#x27;</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">logging_decorator</span>(<span class="params">func</span>):</span></span><br><span class="line"><span class="meta">        @wraps(<span class="params">func</span>)</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">            log_string = func.__name__ + <span class="string">&quot; was called&quot;</span></span><br><span class="line">            <span class="built_in">print</span>(log_string)</span><br><span class="line">            <span class="comment"># 打开logfile，并写入内容</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(logfile, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                <span class="comment"># 现在将日志打到指定的logfile</span></span><br><span class="line">                opened_file.write(log_string + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapped_function</span><br><span class="line">    <span class="keyword">return</span> logging_decorator</span><br><span class="line"> </span><br><span class="line"><span class="meta">@logit()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">myfunc1()</span><br><span class="line"><span class="comment"># Output: myfunc1 was called</span></span><br><span class="line"><span class="comment"># 现在一个叫做 out.log 的文件出现了，里面的内容就是上面的字符串</span></span><br><span class="line"> </span><br><span class="line"><span class="meta">@logit(<span class="params">logfile=<span class="string">&#x27;func2.log&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc2</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"> </span><br><span class="line">myfunc2()</span><br><span class="line"><span class="comment"># Output: myfunc2 was called</span></span><br><span class="line"><span class="comment"># 现在一个叫做 func2.log 的文件出现了，里面的内容就是上面的字符串</span></span><br></pre></td></tr></table></figure><h2 id="Class-of-Decorator"><a href="#Class-of-Decorator" class="headerlink" title="Class of Decorator"></a>Class of Decorator</h2><p>By means of class inheritance, we could implement effects of different kinds of decorations.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">logit</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, logfile=<span class="string">&#x27;out.log&#x27;</span></span>):</span></span><br><span class="line">        self.logfile = logfile</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, func</span>):</span></span><br><span class="line"><span class="meta">            @wraps(<span class="params">func</span>)</span></span><br><span class="line">            <span class="function"><span class="keyword">def</span> <span class="title">wrapped_function</span>(<span class="params">*args, **kwargs</span>):</span></span><br><span class="line">                log_string = func.__name__ + <span class="string">&quot; was called&quot;</span></span><br><span class="line">                <span class="built_in">print</span>(log_string)</span><br><span class="line">                <span class="comment"># 打开logfile并写入</span></span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(self.logfile, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> opened_file:</span><br><span class="line">                    <span class="comment"># 现在将日志打到指定的文件</span></span><br><span class="line">                    opened_file.write(log_string + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">                <span class="comment"># 现在，发送一个通知</span></span><br><span class="line">                self.notify()</span><br><span class="line">                <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">            <span class="keyword">return</span> wrapped_function</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span>(<span class="params">self</span>):</span></span><br><span class="line">            <span class="comment"># logit只打日志，不做别的</span></span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">email_logit</span>(<span class="params">logit</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    一个logit的实现版本，可以在函数调用时发送email给管理员</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, email=<span class="string">&#x27;admin@myproject.com&#x27;</span>, *args, **kwargs</span>):</span></span><br><span class="line">        self.email = email</span><br><span class="line">        <span class="built_in">super</span>(email_logit, self).__init__(*args, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">notify</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># 发送一封email到self.email</span></span><br><span class="line">        <span class="comment"># 这里就不做实现了</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@logit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc1</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@email_logit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc2</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.runoob.com/w3cnote/python-func-decorators.html">https://www.runoob.com/w3cnote/python-func-decorators.html</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 技术/Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 函数修饰器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OOP 课程笔记</title>
      <link href="/2022/03/13/oop-note/"/>
      <url>/2022/03/13/oop-note/</url>
      
        <content type="html"><![CDATA[<p>《面向对象程序设计基础》课程笔记的主要部分。内含：</p><ul><li>创建与销毁</li><li>引用与复制</li><li>组合与继承</li><li>虚函数</li><li>多态与模板</li><li>STL 初步</li></ul><h2 id="Week-05-创建和销毁"><a href="#Week-05-创建和销毁" class="headerlink" title="Week 05 创建和销毁"></a>Week 05 创建和销毁</h2><h3 id="5-0-Overview"><a href="#5-0-Overview" class="headerlink" title="5.0 Overview"></a>5.0 Overview</h3><ul><li>5.1 友元</li><li>5.2 静态成员与常量成员</li><li>5.3 常量/静态/参数对象的构造与析构时机</li><li>5.4 对象的new和delete</li></ul><h3 id="5-1-友元"><a href="#5-1-友元" class="headerlink" title="5.1 友元"></a>5.1 友元</h3><ul><li>友元<ul><li>被声明为友元的函数或类，具有对出现友元声明的类的private及protected成员的访问权限，即可以访问该类的一切成员。</li><li>友元的声明只能在类内进行。</li></ul></li><li>可以声明别的类的成员函数，包括构造和析构函数，为当前类的友元。</li><li>友元的声明与当前所在域是否为private或public无关。</li></ul><h3 id="5-2-静态成员与常量成员"><a href="#5-2-静态成员与常量成员" class="headerlink" title="5.2 静态成员与常量成员"></a>5.2 静态成员与常量成员</h3><h4 id="5-2-1-static"><a href="#5-2-1-static" class="headerlink" title="5.2.1 static"></a>5.2.1 static</h4><ol><li>静态变量与静态函数</li></ol><ul><li>静态变量：使用static修饰的变量<ul><li>初始化：初次定义时需要初始化，且只能初始化一次。</li><li>静态局部变量存储在静态存储区，生命周期将持续到整个程序结束</li><li>静态全局变量是<strong>内部可链接</strong>的，作用域仅限其声明的文件，不能被其他文件所用，可以避免和其他文件中的同名变量冲突</li></ul></li><li>静态函数：使用static修饰的函数<ul><li>静态函数是<strong>内部可链接</strong>的，作用域仅限其声明的文件，不能被其他文件所用，可以避免和其他文件中的同名函数冲突</li></ul></li></ul><ol><li>静态数据成员与静态成员函数</li></ol><ul><li>静态数据成员：使用static修饰的数据成员，是隶属于类的，称为类的静态数据成员，也称“类变量”<ul><li>静态数据成员被该类的所有对象共享（即所有对象中的这个数据域处在同一内存位置）</li><li>类的静态成员（数据、函数）既可以通过对象来访问，也可以通过类名来访问，如<code>ClassName::static_var</code>或者<code>a.static_var</code>（a为ClassName类的对象）</li><li>类的静态数据成员要在实现文件中赋初值，格式为：<code>Type ClassName::static_var = Value;</code></li><li>和全局变量一样，类的静态数据成员在程序开始前初始化</li></ul></li><li>静态成员函数：在返回值前面添加static修饰的成员函数，称为类的静态成员函数<ul><li>和静态数据成员类似，类的静态成员函数既可以通过对象来访问，也可以通过类名来访问，如<code>ClassName::static_function</code>或者<code>a.static_function</code>(a为ClassName类的对象）</li></ul></li><li><strong>静态成员函数不能访问非静态成员</strong>。（原因：分配时序）</li></ul><h4 id="5-2-2-const"><a href="#5-2-2-const" class="headerlink" title="5.2.2 const"></a>5.2.2 const</h4><ol><li>常量</li></ol><ul><li>修饰变量时（如<code>const int n = 1;</code>），必须就地初始化，该变量的值在其生命周期内都不会发生变化</li><li>修饰引用/指针时（如<code>int a=1; const int&amp; b=a;</code>），不能通过该引用/指针修改相应变量的值，常用于函数参数以保证函数体中无法修改参数的值</li><li>修饰函数返回值时（如<code>const int* func() &#123;…&#125;</code>），函数返回值的内容（或其指向的内容）不能被修改</li></ul><ol><li>常量数据成员和常量成员函数</li></ol><ul><li><p>常量数据成员：使用const修饰的数据成员，称为类的常量数据成员，在对象的整个生命周期里不可更改</p><ul><li>常量数据成员可以在<ul><li>构造函数的初始化列表中被初始化</li><li>就地初始化</li><li><strong>不允许</strong>在构造函数的函数体中通过赋值来设置</li></ul></li></ul></li><li><p>常量成员函数</p><ul><li><p>成员函数也能用const来修饰，称为常量成员函数。</p></li><li><p>常量成员函数的访问权限：实现语句不能修改类的数据成员，即不能改变对象状态（内容）<br><code>ReturnType Func(…) const &#123;…&#125;</code></p></li><li><p>注意区别：<code>const ReturnType Func(…) &#123;…&#125;</code></p></li><li><p>若对象被定义为常量(<code>const ClassName a;</code>)，则它只能调用以const修饰的成员函数</p><ul><li>常量对象：对象中的“数据”不能变</li></ul></li></ul></li></ul><ol><li>常量静态变量</li></ol><ul><li>当然，我们可以定义既是常量也是静态的变量<ul><li>作为类的常量变量</li></ul></li><li>常量静态变量需要在类外进行定义，但有两个例外：int和enum类型可以就地初始化</li><li>常量静态变量和静态变量一样，满足访问权限的任意函数均可访问，但都不能修改</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">foo</span> &#123;</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span>* cs; <span class="comment">// 不可就地初始化</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> i = <span class="number">3</span>; <span class="comment">// 可以就地初始化</span></span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">int</span> j; <span class="comment">// 也可以在类外定义</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* foo::cs = <span class="string">&quot;foo C string&quot;</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> foo::j = <span class="number">4</span>;</span><br></pre></td></tr></table></figure><h3 id="5-3-常量-静态-参数对象的构造与析构时机"><a href="#5-3-常量-静态-参数对象的构造与析构时机" class="headerlink" title="5.3 常量/静态/参数对象的构造与析构时机"></a>5.3 常量/静态/参数对象的构造与析构时机</h3><h4 id="5-3-1-常量对象的构造与析构"><a href="#5-3-1-常量对象的构造与析构" class="headerlink" title="5.3.1 常量对象的构造与析构"></a>5.3.1 常量对象的构造与析构</h4><ul><li><strong>常量全局/局部对象</strong>的构造与析构时机和<strong>普通全局/局部对象</strong>相同</li><li>常量全局对象：在main()函数调用之前进行初始化，在main()函数执行完return，程序结束时，对象被析构<br>常量局部对象：在程序执行到该局部对象的代码时被初始化。在局部对象生命周期结束、即所在作用域结束后被析构</li></ul><h4 id="5-3-2-静态对象的构造与析构"><a href="#5-3-2-静态对象的构造与析构" class="headerlink" title="5.3.2 静态对象的构造与析构"></a>5.3.2 静态对象的构造与析构</h4><ol><li>静态全局对象</li></ol><ul><li>静态全局对象的构造与析构时机和普通全局对象相同</li></ul><ol><li>函数中静态对象</li></ol><ul><li>函数内部定义的静态局部对象</li><li>在程序执行到该静态局部对象的代码时被初始化，离开作用域不析构。</li><li>第二次执行到该对象代码时，不再初始化，直接使用上一次的对象。</li><li>在main()函数结束后被析构。</li></ul><ol><li>类静态对象</li></ol><ul><li>类A的对象a作为类B的静态变量</li><li>a的构造与析构表现和全局对象类似，即在main()函数调用之前进行初始化，在main()函数执行完return，程序结束时，对象被析构</li><li>和B是否实例化无关</li></ul><h4 id="5-3-3-参数对象的构造和析构"><a href="#5-3-3-参数对象的构造和析构" class="headerlink" title="5.3.3 参数对象的构造和析构"></a>5.3.3 参数对象的构造和析构</h4><h3 id="5-4-对象的new和delete"><a href="#5-4-对象的new和delete" class="headerlink" title="5.4 对象的new和delete"></a>5.4 对象的new和delete</h3><h2 id="Week-06-引用与复制"><a href="#Week-06-引用与复制" class="headerlink" title="Week 06 引用与复制"></a>Week 06 引用与复制</h2><h3 id="6-0-Overview"><a href="#6-0-Overview" class="headerlink" title="6.0 Overview"></a>6.0 Overview</h3><ul><li>6.1 常量引用</li><li>6.2 拷贝构造函数</li><li>6.3 右值引用</li><li>6.4 移动构造函数</li><li>6.5 赋值运算符</li><li>6.6 类型转换</li></ul><h3 id="6-1-常量引用"><a href="#6-1-常量引用" class="headerlink" title="6.1 常量引用"></a>6.1 常量引用</h3><ul><li>最小特权原则：给函数足够的权限去完成相应的任务，但不要给予他多余的权限。<ul><li>例如函数<code>void add(int&amp; a, int&amp; b)</code>，如果将参数类型定义为<code>int&amp;</code>，则给予该函数在函数体内修改a和b的值的权限</li><li>如果我们不想给予函数修改权限，则可以在参数中使用常量/常量引用</li><li><code>void add(const int&amp; a, const int&amp; b)</code></li><li>此时函数中仅能读取a和b的值，无法对a, b进行任何修改操作。</li></ul></li></ul><h3 id="6-2-拷贝构造函数"><a href="#6-2-拷贝构造函数" class="headerlink" title="6.2 拷贝构造函数"></a>6.2 拷贝构造函数</h3><ul><li>拷贝构造函数是一种特殊的构造函数，它的参数是语言规定的，是同类对象的常量引用</li><li><code>MyClass(const MyClass&amp;) &#123;&#125;</code></li><li>拷贝构造函数被调用的三种常见情况<ul><li>用一个类对象定义另一个新的类对象</li><li>函数调用时以类的对象为形参</li><li>函数返回类对象</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.2.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// a</span></span><br><span class="line">Test a; <span class="comment">// NO</span></span><br><span class="line"><span class="function">Test <span class="title">b</span><span class="params">(a)</span></span>; <span class="comment">//YES</span></span><br><span class="line">Test c = a; <span class="comment">//YES</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// b</span></span><br><span class="line"><span class="built_in">Func</span>(Test a)</span><br><span class="line"></span><br><span class="line"><span class="comment">// c</span></span><br><span class="line"><span class="keyword">return</span> a;</span><br></pre></td></tr></table></figure><ul><li>如果<strong>调用拷贝构造函数</strong>且当前<strong>没有</strong>给类<strong>显式定义</strong>拷贝构造函数，编译器将自动合成“<strong>隐式定义</strong>的拷贝构造函数”，其功能是<strong>调用所有数据成员的拷贝构造函数或拷贝赋值运算符</strong>。</li><li>隐式定义拷贝构造函数在遇到<strong>指针类型</strong>成员时可能会出错,导致多个指针类型的变量指向同一个地址。</li><li>拷贝构造函数的频繁调用会降低程序运行的效率，解决方法：<ul><li>使用引用/常量引用来传参或返回对象</li><li>将拷贝构造函数声明为 <code>private</code> ，或使用 <code>delete</code> 取消拷贝构造函数的隐式合成</li></ul></li></ul><h3 id="6-3-右值引用"><a href="#6-3-右值引用" class="headerlink" title="6.3 右值引用"></a>6.3 右值引用</h3><ul><li>左值和右值<ul><li>左值：可以取地址、有名字的值。</li><li>右值：不能取地址、没有名字的值; 常见于常值、函数返回值、表达式</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.3.1</span></span><br><span class="line"><span class="keyword">int</span> a = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">int</span> b = <span class="built_in">func</span>();</span><br><span class="line"><span class="keyword">int</span> c = a + b;</span><br><span class="line"><span class="comment">// 其中a、b、c为左值，1、func函数返回值、a+b的结果为右值。</span></span><br></pre></td></tr></table></figure><ul><li>右值引用<ul><li>虽然右值无法取地址，但可以被&amp;&amp;引用(右值引用)<ul><li><code>int &amp;&amp;e = a+b;</code></li></ul></li><li>右值引用无法绑定左值<ul><li><code>int &amp;&amp;e = a; //Compile Error</code></li></ul></li><li>例外：常量左值引用能也绑定右值<ul><li><code>const int &amp;e = 3;</code></li></ul></li></ul></li></ul><h3 id="6-4-移动构造函数"><a href="#6-4-移动构造函数" class="headerlink" title="6.4 移动构造函数"></a>6.4 移动构造函数</h3><ul><li>移动构造函数<ul><li>右值引用可以延续即将销毁变量的生命周期，用于构造函数可以<strong>提升处理效率</strong>，在此过程中尽可能少地进行拷贝。</li><li>使用右值引用作为参数的构造函数叫做移动构造函数。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.4.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 拷贝构造函数</span></span><br><span class="line"><span class="built_in">ClassName</span>(<span class="keyword">const</span> ClassName&amp; VariableName);</span><br><span class="line"><span class="comment">// 移动构造函数</span></span><br><span class="line"><span class="built_in">ClassName</span>(ClassName&amp;&amp; VariableName);</span><br></pre></td></tr></table></figure><ul><li>移动构造函数与拷贝构造函数最主要的差别就是类中堆内存是重新开辟并拷贝，还是直接将指针指向那块地址。</li><li>对于一些即将析构的临时类，移动构造函数直接利用了原来临时对象中的堆内存，新的对象无需开辟内存，临时对象无需释放内存，从而大大提高计算效率。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.4.2</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Test</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="keyword">int</span> * buf; <span class="comment">//// only for demo.</span></span><br><span class="line"><span class="built_in">Test</span>() &#123;</span><br><span class="line">buf = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>]; <span class="comment">//申请一块内存</span></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;Test(): this-&gt;buf @ &quot;</span> &lt;&lt; hex &lt;&lt; buf &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line">~<span class="built_in">Test</span>() &#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;~Test(): this-&gt;buf @ &quot;</span> &lt;&lt; hex &lt;&lt; buf &lt;&lt; endl;</span><br><span class="line"><span class="keyword">if</span> (buf) <span class="keyword">delete</span>[] buf;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(<span class="keyword">const</span> Test&amp; t) : <span class="built_in">buf</span>(<span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>]) &#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">10</span>; i++) </span><br><span class="line">buf[i] = t.buf[i]; <span class="comment">//拷贝数据</span></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;Test(const Test&amp;) called. this-&gt;buf @ &quot;</span></span><br><span class="line">&lt;&lt; hex &lt;&lt; buf &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">Test</span>(Test&amp;&amp; t) : <span class="built_in">buf</span>(t.buf) &#123; <span class="comment">//直接复制地址，避免拷贝</span></span><br><span class="line">cout &lt;&lt; <span class="string">&quot;Test(Test&amp;&amp;) called. this-&gt;buf @ &quot;</span></span><br><span class="line">&lt;&lt; hex &lt;&lt; buf &lt;&lt; endl;</span><br><span class="line">t.buf = <span class="literal">nullptr</span>; <span class="comment">//将t.buf改为nullptr，使其不再指向原来内存区域</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">Test <span class="title">GetTemp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">Test tmp;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;GetTemp(): tmp.buf @ &quot;</span></span><br><span class="line">&lt;&lt; hex &lt;&lt; tmp.buf &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> tmp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">(Test t)</span> </span>&#123;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;fun(Test t): t.buf @ &quot;</span></span><br><span class="line">&lt;&lt; hex &lt;&lt; t.buf &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">Test a = <span class="built_in">GetTemp</span>();</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;main() : a.buf @ &quot;</span> &lt;&lt; hex &lt;&lt; a.buf &lt;&lt; endl;</span><br><span class="line"><span class="built_in">fun</span>(a);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// g++ test.cpp --std=c++11 -fno-elide-constructors -o test</span></span><br></pre></td></tr></table></figure><ul><li><p>移动语义</p><ul><li><p><code>std::move</code>函数</p><ul><li>输入：左值（包括变量等，该左值一般不再使用）</li><li>返回值：该左值对应的右值</li></ul></li></ul></li></ul><h3 id="6-5-赋值运算符"><a href="#6-5-赋值运算符" class="headerlink" title="6.5 赋值运算符"></a>6.5 赋值运算符</h3><ul><li>拷贝复制运算符</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.5.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 区分</span></span><br><span class="line">ClassName a;</span><br><span class="line">ClassName b;</span><br><span class="line">a = b;</span><br><span class="line"></span><br><span class="line">ClassName a = b;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 前者调用</span></span><br><span class="line">ClassName&amp; <span class="keyword">operator</span>= (<span class="keyword">const</span> ClassName&amp; right) &#123;</span><br><span class="line">   <span class="keyword">if</span> (<span class="keyword">this</span> != &amp;right) &#123;<span class="comment">// 避免自己赋值给自己</span></span><br><span class="line"><span class="comment">// 将right对象中的内容拷贝到当前对象中...</span></span><br><span class="line">&#125;</span><br><span class="line">   <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>移动赋值运算符</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.5.2</span></span><br><span class="line">Test&amp; <span class="keyword">operator</span>= (Test&amp;&amp; right) &#123;</span><br><span class="line"><span class="keyword">if</span> (<span class="keyword">this</span> == &amp;right)  cout &lt;&lt; <span class="string">&quot;same obj!\n&quot;</span>;</span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line"><span class="keyword">this</span>-&gt;buf = right.buf;  <span class="comment">//直接赋值地址</span></span><br><span class="line">right.buf = <span class="literal">nullptr</span>;</span><br><span class="line">cout &lt;&lt; <span class="string">&quot;operator=(Test&amp;&amp;) called.\n&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="6-6-类型转换"><a href="#6-6-类型转换" class="headerlink" title="6.6 类型转换"></a>6.6 类型转换</h3><ol><li>在源类中定义<strong>目标类型转换运算符</strong></li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.6.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dst</span> &#123;</span> <span class="comment">//目标类Destination</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Dst</span>() &#123; cout &lt;&lt; <span class="string">&quot;Dst::Dst()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Src</span> &#123;</span> <span class="comment">//源类Source</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Src</span>() &#123; cout &lt;&lt; <span class="string">&quot;Src::Src()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">operator</span> <span class="title">Dst</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; </span><br><span class="line">cout &lt;&lt; <span class="string">&quot;Src::operator Dst() called&quot;</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">Dst</span>(); </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ol><li>在目标类中定义“源类对象作参数的构造函数”</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 6.6.2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Src</span>;</span><span class="comment">// 前置类型声明，因为在Dst中要用到Src类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dst</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Dst</span>() &#123; cout &lt;&lt; <span class="string">&quot;Dst::Dst()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">  <span class="built_in">Dst</span>(<span class="keyword">const</span> Src&amp; s) &#123; </span><br><span class="line">cout &lt;&lt; <span class="string">&quot;Dst::Dst(const Src&amp;)&quot;</span> &lt;&lt; endl; </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Src</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">Src</span>() &#123; cout &lt;&lt; <span class="string">&quot;Src::Src()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><ul><li>注意：两种自动类型转换的方法不能同时使用，使用时请任选其中一种。</li><li>禁止自动类型转换<ul><li>如果用 <code>explicit</code> 修饰类型转换运算符或类型转换构造函数，则相应的类型转换必须显式地进行</li></ul></li></ul><h2 id="Week-08-组合与继承"><a href="#Week-08-组合与继承" class="headerlink" title="Week 08 组合与继承"></a>Week 08 组合与继承</h2><h3 id="8-0-Overview"><a href="#8-0-Overview" class="headerlink" title="8.0 Overview"></a>8.0 Overview</h3><ul><li>组合</li><li>继承</li><li>成员访问权限</li><li>重写隐藏与重载</li><li>多重继承</li></ul><h3 id="8-1-组合"><a href="#8-1-组合" class="headerlink" title="8.1 组合"></a>8.1 组合</h3><ul><li>对象组合的两种实现方法：<ul><li>已有类的对象作为新类的公有数据成员，这样通过允许直接访问子对象而“提供”旧类接口</li><li>已有类的对象作为新类的私有数据成员。新类可以调整旧类的对外接口，可以不使用旧类原有的接口（相当于对接口作了转换）</li></ul></li><li>对象拷贝与赋值运算<ul><li>如果调用拷贝构造函数且没有给类显式定义拷贝构造函数，编译器将自动合成：<ul><li>对有显式定义拷贝构造函数的子对象调用该拷贝构造函数</li><li>对无显式定义拷贝构造函数的子对象采用位拷贝</li></ul></li><li>赋值的默认操作类似</li></ul></li></ul><h3 id="8-2-继承"><a href="#8-2-继承" class="headerlink" title="8.2 继承"></a>8.2 继承</h3><ul><li>基本概念<ul><li>被继承的已有类，被称为基类 <strong>base class</strong>，也称“父类”。</li><li>通过继承得到的新类，被为派生类 <strong>derived class</strong>，也称“子类”、“扩展类”。</li></ul></li><li>继承方式<ul><li>常见的继承方式：public, private<ul><li><code>class Derived : [private] Base &#123; .. &#125;;</code> 缺省继承方式</li><li><code>class Derived : public Base &#123; ... &#125;;</code></li></ul></li><li>protected 继承很少被使用<ul><li><code>class Derived : protected Base &#123; ... &#125;;</code></li></ul></li></ul></li><li>什么不能被继承？<ul><li>构造函数：创建派生类对象时，必须调用派生类的构造函数，派生类构造函数调用基类的构造函数，以创建派生对象的基类部分。C++11新增了继承构造函数的机制（使用using），但默认不继承</li><li>析构函数：释放对象时，先调用派生类析构函数，再调用基类析构函数</li><li>赋值运算符：因为赋值运算符包含一个类型为其所属类的形参</li><li>友元函数：不是类成员</li></ul></li><li>派生类对象的构造与析构过程</li><li>调用基类构造函数<ul><li>若没有显式调用，则编译器会自动生成一个对基类的默认构造函数的调用。</li><li>若想要显式调用，则<strong>只能</strong>在派生类构造函数的<strong>初始化成员列表</strong>中进行。</li></ul></li><li>继承基类构造函数<ul><li>在派生类中使用 <code>using Base::Base;</code> 来继承基类构造函数，相当于给派生类“定义”了相应参数的构造函数，如下例 8.2.1.</li></ul></li><li>当基类存在多个构造函数时，使用using会给派生类自动构造多个相应的构造函数，如下例 8.2.2.<ul><li>如果基类的某个构造函数被声明为私有成员函数，则不能在派生类中声明继承该构造函数。</li><li>如果派生类使用了继承构造函数，编译器就不会再为派生类生成默认构造函数。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 8.2.1</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Base</span>(<span class="keyword">int</span> i) : <span class="built_in">data</span>(i) &#123; cout &lt;&lt; <span class="string">&quot;Base::Base(&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derive</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> Base::Base; <span class="comment">///相当于 Derive(int i):Base(i)&#123;&#125;;</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">Derive <span class="title">obj</span><span class="params">(<span class="number">356</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 8.2.2</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> </span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Base</span>(<span class="keyword">int</span> i) : <span class="built_in">data</span>(i) &#123; cout &lt;&lt; <span class="string">&quot;Base::Base(&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125;</span><br><span class="line"><span class="built_in">Base</span>(<span class="keyword">int</span> i, <span class="keyword">int</span> j) </span><br><span class="line">&#123; cout &lt;&lt; <span class="string">&quot;Base::Base(&quot;</span> &lt;&lt; i &lt;&lt; “,<span class="string">&quot; &lt;&lt; j &lt;&lt; &quot;</span>)\n<span class="string">&quot;;&#125;</span></span><br><span class="line"><span class="string">&#125;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">class Derive : public Base &#123;</span></span><br><span class="line"><span class="string">public:</span></span><br><span class="line"><span class="string">    using Base::Base; ///相当于 Derive(int i):Base(i)&#123;&#125;;</span></span><br><span class="line"><span class="string">                     ///加上 Derive(int i, int j):Base(i，j)&#123;&#125;;</span></span><br><span class="line"><span class="string">&#125;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">int main() &#123;</span></span><br><span class="line"><span class="string">    Derive obj(356);</span></span><br><span class="line"><span class="string">Derive obj(356, 789);</span></span><br><span class="line"><span class="string">    return 0;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><ul><li>继承方式<ul><li>public 继承：基类中公有成员仍能在派生类中保持公有。（图 8.2.3）</li><li>private 继承：用基类接口实现派生类功能。（图 8.2.4）</li></ul></li></ul><p><a href="https://i.loli.net/2021/04/12/EwhXWMB2RUrOzkT.png"><img src="https://i.loli.net/2021/04/12/EwhXWMB2RUrOzkT.png" alt="image-20210412091000928"></a></p><p>（图 8.2.3）</p><p><a href="https://i.loli.net/2021/04/12/5Q9VMBnZk7YUHpq.png"><img src="https://i.loli.net/2021/04/12/5Q9VMBnZk7YUHpq.png" alt="image-20210412091010013"></a></p><p>（图 8.2.4）</p><h3 id="8-3-成员访问权限"><a href="#8-3-成员访问权限" class="headerlink" title="8.3 成员访问权限"></a>8.3 成员访问权限</h3><ul><li><p>基类中的私有成员</p><ul><li>不允许在派生类成员函数中访问</li><li>不允许派生类的对象访问它们</li></ul></li><li><p>基类中的公有成员</p><ul><li><p>允许在派生类成员函数中被访问</p></li><li><p>若是使用<code>public</code>继承方式，则成为派生类公有成员，可以被派生类的对象访问</p></li><li><p>若是使用<code>private/protected</code>继承方式，则成为派生类私有/保护成员，不能被派生类的对象访问</p><ul><li>若想让某成员能被派生类的对象访问，可在派生类 <code>public</code> 部分用关键字 <code>using</code> 声明它的名字（例 8.3.1）</li></ul></li></ul></li><li><p>基类中的保护成员</p><ul><li>与基类中的私有成员的不同在于：保护成员允许在派生类成员函数中被访问。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 8.3.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>: </span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">baseFunc</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;in Base::baseFunc()...&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derive3</span>:</span> <span class="keyword">private</span> Base &#123;<span class="comment">// B的私有继承</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">/// 私有继承时，在派生类public部分声明基类成员名字</span></span><br><span class="line">  <span class="keyword">using</span> Base::baseFunc; </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Derive3 obj3;</span><br><span class="line">  cout &lt;&lt; <span class="string">&quot;calling obj3.baseFunc()...&quot;</span> &lt;&lt; endl;</span><br><span class="line">  obj3.<span class="built_in">baseFunc</span>(); <span class="comment">//基类接口在派生类public部分声明，则派生类对象可调用</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>基类成员的访问权限<ul><li>public 继承：基类的公有成员，保护成员，私有成员作为派生类的成员时，都保持原有的状态。</li><li>private 继承：基类的公有成员，保护成员，私有成员作为派生类的成员时，都作为私有成员。</li><li>protected 继承：基类的公有成员，保护成员作为派生类的成员时，都成为保护成员，基类的私有成员仍然是私有的。</li></ul></li></ul><p><a href="https://i.loli.net/2021/04/12/Wsf9mCAqRuQMdHz.png"><img src="https://i.loli.net/2021/04/12/Wsf9mCAqRuQMdHz.png" alt="image-20210412091900016"></a></p><p>（表 8.3.2）</p><h3 id="8-4-重写隐藏与重载"><a href="#8-4-重写隐藏与重载" class="headerlink" title="8.4 重写隐藏与重载"></a>8.4 重写隐藏与重载</h3><ul><li>重载(overload)：<ul><li>目的：提供同名函数的不同实现，属于静态多态。</li><li>函数名必须相同，函数参数必须不同，作用域相同（如位于同一个类中；或同名全局函数）。</li></ul></li><li>重写隐藏(redefining)：<ul><li>目的：在派生类中重新定义基类函数，实现派生类的特殊功能。</li><li>屏蔽了基类的所有其它同名函数。（例 8.4.1）</li><li>函数名必须相同，函数参数可以不同</li><li>可以在派生类中通过 <code>using 类名::成员函数名;</code> 在派生类中“恢复”指定的基类成员函数（即去掉屏蔽），使之重新可用（例 8.4.2）</li></ul></li></ul><blockquote><p>程序编译时系统就能决定调用哪个函数，因此静态多态性又称为编译时的多态性。</p><p>多态分为两类：静态多态性和动态多态性，以前学过的函数重载和运算符重载实现的多态性属于静态多态性，在程序编译时系统就能决定调用哪个函数，因此静态多态性又称为编译时的多态性。静态多态性是通过函数的重载实现的。动态多态性是在程序运行过程中才动态地确定操作所针对的对象。它又称运行时的多态性。动态多态性是通过虚函数实现的。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 8.4.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">T</span> &#123;</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;B::f()\n&quot;</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f(&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125; <span class="comment">/// 重载</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> d)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f(&quot;</span> &lt;&lt; d &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125; <span class="comment">///重载</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(T)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f(T)\n&quot;</span>; &#125; <span class="comment">///重载</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derive</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Derive::f(&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125; <span class="comment">///重写隐藏</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Derive d;</span><br><span class="line">  d.<span class="built_in">f</span>(<span class="number">10</span>);</span><br><span class="line">  d.<span class="built_in">f</span>(<span class="number">4.9</span>);<span class="comment">/// 编译警告。执行自动类型转换。</span></span><br><span class="line">  <span class="comment">//  d.f();/// 被屏蔽，编译错误</span></span><br><span class="line">  <span class="comment">//  d.f(T());/// 被屏蔽，编译错误</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 8.4.2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">T</span> &#123;</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f()\n&quot;</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f(&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="keyword">double</span> d)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f(&quot;</span> &lt;&lt; d &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(T)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::f(T)\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derive</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">using</span> Base::f;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Derive::f(&quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot;)\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Derive d;</span><br><span class="line">  d.<span class="built_in">f</span>(<span class="number">10</span>);</span><br><span class="line">  d.<span class="built_in">f</span>(<span class="number">4.9</span>);</span><br><span class="line">  d.<span class="built_in">f</span>();</span><br><span class="line">  d.<span class="built_in">f</span>(<span class="built_in">T</span>());</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><code>using</code>关键字<ul><li>继承基类构造函数</li><li>恢复被屏蔽的基类成员函数</li><li>还可用于：<ul><li>指示命名空间，<code>using namespace std;</code></li><li>将另一个命名空间的成员引入当前命名空间<code>using std::cout; cout &lt;&lt; endl;</code></li><li>定义类型别名，<code>using a = int;</code></li></ul></li></ul></li></ul><h3 id="8-5-多重继承"><a href="#8-5-多重继承" class="headerlink" title="8.5 多重继承"></a>8.5 多重继承</h3><ul><li>派生类同时继承多个基类</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span>&#123;</span>&#125;; </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputFile</span>:</span> <span class="keyword">public</span> File&#123;&#125;; </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutputFile</span>:</span> <span class="keyword">public</span> File&#123;&#125;; </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IOFile</span>:</span> <span class="keyword">public</span> InputFile, <span class="keyword">public</span> OutputFile&#123;&#125;;</span><br></pre></td></tr></table></figure><p><a href="https://i.loli.net/2021/04/12/jCy49k6pKcM7XhE.png"><img src="https://i.loli.net/2021/04/12/jCy49k6pKcM7XhE.png" alt="image-20210412092825715"></a></p><p>（图 8.5.1）</p><ul><li>数据存储<ul><li>如果派生类D继承的两个基类A,B，是同一基类Base的不同继承，则A,B中继承自Base的数据成员会在D有两份独立的副本，可能带来数据冗余。</li></ul></li><li>二义性<ul><li>如果派生类D继承的两个基类A,B，有同名成员a，则访问D中a时，编译器无法判断要访问的哪一个基类成员。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 8.5.2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">int</span> a&#123;<span class="number">0</span>&#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MiddleA</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">addA</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;a=&quot;</span> &lt;&lt; ++a &lt;&lt; endl; &#125;;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;A::bar&quot;</span> &lt;&lt; endl; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MiddleB</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">addB</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;a=&quot;</span> &lt;&lt; ++a &lt;&lt; endl; &#125;;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">bar</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;B::bar&quot;</span> &lt;&lt; endl; &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derive</span> :</span> <span class="keyword">public</span> MiddleA, <span class="keyword">public</span> MiddleB&#123;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Derive d;</span><br><span class="line">  d.<span class="built_in">addA</span>(); <span class="comment">/// 输出 a=1。</span></span><br><span class="line">  d.<span class="built_in">addB</span>(); <span class="comment">/// 仍然输出 a=1。</span></span><br><span class="line">  cout &lt;&lt; d.a; <span class="comment">/// 编译错误，A和B都有成员a</span></span><br><span class="line">  cout &lt;&lt; d.A::a; <span class="comment">/// 输出A中的成员a的值</span></span><br><span class="line">  d.<span class="built_in">bar</span>(); <span class="comment">/// 编译错误，A和B都有成员函数bar</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Week-09-虚函数"><a href="#Week-09-虚函数" class="headerlink" title="Week 09 虚函数"></a>Week 09 虚函数</h2><h3 id="9-0-Overview"><a href="#9-0-Overview" class="headerlink" title="9.0 Overview"></a>9.0 Overview</h3><ul><li>向上类型转换</li><li>对象切片</li><li>函数调用捆绑</li><li>虚函数和虚函数表</li><li>虚函数和构造函数、析构函数</li><li>重写覆盖，override和final</li></ul><h3 id="9-1-向上类型转换"><a href="#9-1-向上类型转换" class="headerlink" title="9.1 向上类型转换"></a>9.1 向上类型转换</h3><ul><li><strong>派生类</strong>对象/引用/指针<strong>转换成基类</strong>对象/引用/指针，称为向上类型转换。只对<code>public</code>继承有效，在继承图上是上升的；对<code>private</code>、<code>protected</code>继承无效。</li><li>向上类型转换（派生类到基类）可以由编译器自动完成，是一种隐式类型转换。</li><li>凡是<strong>接受基类对象/引用/指针的地方</strong>（如函数参数），都可以<strong>使用派生类对象/引用/指针</strong>，编译器会自动将派生类对象转换为基类对象以便使用。</li></ul><h3 id="9-2-对象切片"><a href="#9-2-对象切片" class="headerlink" title="9.2 对象切片"></a>9.2 对象切片</h3><ul><li>当<strong>派生类的对象</strong><code>(不是指针或引用)</code>被通过<strong>传参或赋值</strong>的方式转换为<strong>基类的对象</strong>时，派生类的对象被<strong>切片</strong>为对应基类的子对象。<ul><li><a href="https://i.loli.net/2021/04/19/2tVMgrhZ3GPT1cI.png"><img src="https://i.loli.net/2021/04/19/2tVMgrhZ3GPT1cI.png" alt="image-20210419081125275"></a></li><li>派生类的新数据和新方法丢失（图 9.2.1）</li></ul></li><li>当派生类的<code>指针（引用）</code>被通过<strong>传参或赋值</strong>的方式转换为基类<code>指针（引用）</code>时，不会创建新的对象，但只保留基类的接口。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 9.2.2 私有继承“照此实现”</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> &#123;</span></span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> data&#123;<span class="number">0</span>&#125;;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getData</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> data; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setData</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123; data = i; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D1</span> :</span> <span class="keyword">private</span> B &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> B::getData;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    D1 d1;</span><br><span class="line">    cout &lt;&lt; d1.<span class="built_in">getData</span>();  </span><br><span class="line">    <span class="comment">// d1.setData(10) //隐藏了基类的setData函数，不可访问 </span></span><br><span class="line">    <span class="comment">// B&amp; b = d1;     //不允许私有继承的向上转换</span></span><br><span class="line">    <span class="comment">// b.setData(10); //否则可以绕过D1，调用基类的setData函数</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="9-3-函数调用捆绑"><a href="#9-3-函数调用捆绑" class="headerlink" title="9.3 函数调用捆绑"></a>9.3 函数调用捆绑</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 9.3.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Instrument</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">play</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Instrument::play&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wind</span> :</span> <span class="keyword">public</span> Instrument &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">// Redefine interface function:</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">play</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Wind::play&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">tune</span><span class="params">(Instrument&amp; i)</span> </span>&#123;</span><br><span class="line">  i.<span class="built_in">play</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Wind flute;</span><br><span class="line">  <span class="built_in">tune</span>(flute); <span class="comment">//引用的向上类型转换(传参)，编译器早绑定，无对象切片产生</span></span><br><span class="line">  Instrument &amp;inst = flute;  <span class="comment">// 引用的向上类型转换(赋值)</span></span><br><span class="line">  inst.<span class="built_in">play</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>把函数体与函数调用相联系称为<strong>捆绑</strong>(binding)。</p><ul><li>即将函数体实现代码的入口地址，与调用的函数名绑定。执行到调用代码时进入函数体内部。</li></ul></li><li><p>当捆绑在程序运行之前（由编译器和连接器）完成时，称为<strong>早捆绑</strong>(early binding)。</p><ul><li>运行之前已经决定了函数调用代码到底进入哪个函数。</li><li>上面程序中的问题是早捆绑引起的，编译器将tune中的函数调用i.play()与Instrument::play()绑定。</li></ul></li><li><p>当捆绑根据对象的实际类型(上例中即子类Wind而非Instrument)，发生在程序运行时，称为<strong>晚捆绑</strong>(late binding)，又称动态捆绑或运行时捆绑。</p><ul><li>要求在运行时能确定对象的实际类型，并绑定正确的函数。</li><li>晚捆绑只对类中的虚函数起作用，使用 virtual 关键字声明虚函数。</li></ul></li></ul><h3 id="9-4-虚函数与虚函数表"><a href="#9-4-虚函数与虚函数表" class="headerlink" title="9.4 虚函数与虚函数表"></a>9.4 虚函数与虚函数表</h3><ul><li>对于被派生类重新定义的成员函数，若它<strong>在基类中被声明为虚函数</strong>，则通过基类<strong><code>指针或引用</code></strong>调用该成员函数时，编译器将根据所指（或引用）对象的实际类型决定是调用基类中的函数，还是调用派生类重写的函数。</li><li>若某成员函数在基类中声明为虚函数，当派生类重写覆盖(同名，同参数函数)它时，无论是否声明为虚函数，该成员函数都仍然是虚函数。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 9.4.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Instrument</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">play</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Instrument::play&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wind</span> :</span> <span class="keyword">public</span> Instrument &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">play</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Wind::play&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">     <span class="comment">/// 重写覆盖(稍后：重写隐藏和重写覆盖的区别）</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">tune</span><span class="params">(Instrument&amp; ins)</span> </span>&#123;</span><br><span class="line">  ins.<span class="built_in">play</span>(); <span class="comment">/// 由于 Instrument::play 是虚函数，编译时不再直接绑定，运行时根据 ins 的实际类型调用。</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Wind flute;</span><br><span class="line">  <span class="built_in">tune</span>(flute); <span class="comment">/// 向上类型转换</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>一般来说，派生类虚函数的返回类型应该和基类相同。</p><ul><li><p>或者，是协变(Covariant)的，例如</p><ul><li>基类和派生类的指针是协变的</li><li>基类和派生类的引用是协变的</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 9.4.2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Instrument</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> Instrument&amp; <span class="title">getObj</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> *<span class="keyword">this</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Wind</span> :</span> <span class="keyword">public</span> Instrument &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"><span class="function"><span class="keyword">virtual</span> Wind&amp; <span class="title">getObj</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> *<span class="keyword">this</span>;&#125;</span><br><span class="line"><span class="comment">//Wind&amp;和Instrument&amp;协变</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>CODE</p><ul><li>虚函数表<ul><li>对象自身要包含自己实际类型的信息：用虚函数表表示。运行时通过虚函数表确定对象的实际类型。</li><li><strong>虚函数表</strong>(VTABLE)：每个包含虚函数的类用于存储虚函数地址的表(虚函数表有唯一性，即使没有重写虚函数)。</li><li>每个<strong>包含虚函数的类对象</strong>中，编译器秘密地放一个<strong>指针</strong>，称为<strong>虚函数指针</strong>(vpointer/VPTR)，指向这个类的VTABLE。</li><li>当通过基类指针做虚函数调用时，编译器静态地插入能取得这个VPTR并在VTABLE表中查找函数地址的代码，这样就能调用正确的函数并引起晚捆绑的发生。<ul><li><strong>编译</strong>期间：<strong>建立虚函数表VTABLE</strong>，记录每个类或该类的基类中所有已声明的虚函数入口地址。</li><li><strong>运行</strong>期间：<strong>建立虚函数指针VPTR</strong>，在构造函数中发生，指向相应的VTABLE。</li></ul></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 9.4.3</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun1</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;B::fun1()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun2</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;B::fun2()&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> i;</span><br><span class="line">    <span class="keyword">float</span> j;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span> :</span> <span class="keyword">public</span> B &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;D::fun1()&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;  <span class="comment">///对fun1重写覆盖，对fun2没有，则fun2使用基类的虚函数地址</span></span><br><span class="line">    <span class="keyword">double</span> k;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    B b;</span><br><span class="line">    D d;</span><br><span class="line">    B* pB = &amp;d;</span><br><span class="line">    pB-&gt;<span class="built_in">fun1</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><p><a href="https://i.loli.net/2021/04/19/LEmhf6Y7igcUnPT.png"><img src="https://i.loli.net/2021/04/19/LEmhf6Y7igcUnPT.png" alt="image-20210419083950089"></a></p><p>（图 9.4.4）</p><h3 id="9-5-虚函数与构造函数、析构函数"><a href="#9-5-虚函数与构造函数、析构函数" class="headerlink" title="9.5 虚函数与构造函数、析构函数"></a>9.5 虚函数与构造函数、析构函数</h3><ul><li>虚函数与构造函数<ul><li>当创建一个包含有虚函数的对象时，必须初始化它的VPTR以指向相应的VTABLE。设置VPTR的工作由构造函数完成。编译器在构造函数的开头秘密的插入能初始化VPTR的代码。</li><li>构造函数不能也不必是虚函数。<ul><li>不能：如果构造函数是虚函数，则创建对象时需要先知道VPTR，而在构造函数调用前，VPTR未初始化。</li><li>不必：构造函数的作用是提供类中成员初始化，调用时明确指定要创建对象的类型，没有必要是虚函数。</li></ul></li><li>在构造函数中调用一个虚函数，被调用的只是这个函数的本地版本(即当前类的版本)，即虚机制在构造函数中不工作。</li><li>初始化顺序：(与构造函数初始化列表顺序无关)<ul><li>基类初始化</li><li>对象成员初始化</li><li>构造函数体</li></ul></li><li>原因：基类的构造函数比派生类先执行，调用基类构造函数时派生类中的数据成员还没有初始化。如果允许调用实际对象的虚函数，则可能会用到未初始化的派生类成员。</li></ul></li><li>虚函数与析构函数<ul><li>析构函数能是虚的，且常常是虚的。虚析构函数仍需定义函数体。</li><li>虚析构函数的用途：当删除基类对象指针时，编译器将根据指针所指对象的实际类型，调用相应的析构函数。</li><li>若基类析构不是虚函数，则删除基类指针所指派生类对象时，编译器仅自动调用基类的析构函数，而不会考虑实际对象是不是基类的对象。这可能会导致内存泄漏。</li><li>在析构函数中调用一个虚函数，被调用的只是这个函数的本地版本，即虚机制在析构函数中不工作。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 9.5.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base1</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  ~<span class="built_in">Base1</span>() &#123; cout &lt;&lt; <span class="string">&quot;~Base1()\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived1</span> :</span> <span class="keyword">public</span> Base1 &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  ~<span class="built_in">Derived1</span>() &#123; cout &lt;&lt; <span class="string">&quot;~Derived1()\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base2</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">virtual</span> ~<span class="built_in">Base2</span>() &#123; cout &lt;&lt; <span class="string">&quot;~Base2()\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived2</span> :</span> <span class="keyword">public</span> Base2 &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  ~<span class="built_in">Derived2</span>() &#123; cout &lt;&lt; <span class="string">&quot;~Derived2()\n&quot;</span>; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  Base1* bp = <span class="keyword">new</span> Derived1;</span><br><span class="line">  <span class="keyword">delete</span> bp; <span class="comment">/// 只调用了基类的虚析构函数</span></span><br><span class="line">  Base2* b2p = <span class="keyword">new</span> Derived2;</span><br><span class="line">  <span class="keyword">delete</span> b2p; <span class="comment">/// 派生类虚析构函数调用完后调用基类的虚析构函数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Output</span></span><br><span class="line">~<span class="built_in">Base1</span>()</span><br><span class="line">~<span class="built_in">Derived2</span>()</span><br><span class="line">~<span class="built_in">Base2</span>()</span><br></pre></td></tr></table></figure><ul><li>重要原则：总是将基类的析构函数设置为虚析构函数</li></ul><h3 id="9-6-重载、重写覆盖与重写隐藏"><a href="#9-6-重载、重写覆盖与重写隐藏" class="headerlink" title="9.6 重载、重写覆盖与重写隐藏"></a>9.6 重载、重写覆盖与重写隐藏</h3><ul><li><strong>重载(overload)</strong>：<ul><li>函数名必须相同，函数参数必须不同，作用域相同(同一个类)，返回值可以相同或不同。</li></ul></li><li><strong>重写覆盖(override)</strong>：<ul><li>派生类重新定义基类中的<strong>虚函数</strong>，<strong>函数名必须相同</strong>，函数<strong>参数必须相同</strong>，<strong>返回值一般情况应相同</strong>。</li><li>派生类的虚函数表中原基类的虚函数指针会被派生类中重新定义的虚函数指针覆盖掉。</li></ul></li><li><strong>重写隐藏(redefining)</strong>：<ul><li>派生类重新定义基类中的函数，<strong>函数名相同</strong>，但是<strong>参数不同或者基类的函数不是虚函数</strong>。</li><li>虚函数表不会发生覆盖。</li></ul></li><li>重写覆盖和重写隐藏：<ul><li>相同点：<ul><li>都要求派生类定义的函数与基类同名。</li><li>都会屏蔽基类中的同名函数，即派生类的实例无法调用基类的同名函数。</li></ul></li><li>不同点：<ul><li>重写覆盖要求基类的函数是虚函数，且函数参数相同，返回值一般情况应相同；重写隐藏要求基类的函数不是虚函数或者函数参数不同。</li><li>重写覆盖会使派生类虚函数表中基类的虚函数的指针被派生类的虚函数指针覆盖。重写隐藏不会。</li></ul></li></ul></li><li>override与final关键字<ul><li>重写覆盖要满足的条件很多，很容易写错，可以使用override关键字辅助检查。</li><li>override关键字明确地告诉编译器一个函数是对基类中一个虚函数的重写覆盖，编译器将对重写覆盖要满足的条件进行检查，正确的重写覆盖才能通过编译。</li><li>如果没有override关键字，但是满足了重写覆盖的各项条件，也能实现重写覆盖。它只是编译器的一个检查，正确实现override时，对编译结果没有影响。</li></ul></li><li>不想让使用者继承？-&gt; final关键字!<ul><li>在虚函数声明或定义中使用时，final确保函数为虚且不可被派生类重写。可在继承关系链的“中途”进行设定，禁止后续派生类对指定虚函数重写。</li><li>在类定义中使用时，final指定此类不可被继承。</li></ul></li></ul><h2 id="Week-10-多态与模板"><a href="#Week-10-多态与模板" class="headerlink" title="Week 10 多态与模板"></a>Week 10 多态与模板</h2><h3 id="10-0-Overview"><a href="#10-0-Overview" class="headerlink" title="10.0 Overview"></a>10.0 Overview</h3><ul><li>纯虚函数与抽象类</li><li>向下类型转换</li><li>多重继承的虚函数表，多重继承的利弊</li><li>多态</li><li>函数模板与类模板</li></ul><h3 id="10-1-纯虚函数与抽象类"><a href="#10-1-纯虚函数与抽象类" class="headerlink" title="10.1 纯虚函数与抽象类"></a>10.1 纯虚函数与抽象类</h3><ul><li>虚函数还可以进一步声明为纯虚函数，包含纯虚函数的类，通常被称为“抽象类”。<ul><li><code>virtual 返回类型 函数名(形式参数) = 0;</code></li></ul></li><li>抽象类不允许定义对象，定义基类为抽象类的主要用途是为派生类规定共性“接口”</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.1.1</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">f</span><span class="params">()</span> </span>= <span class="number">0</span>; <span class="comment">/// 可在类外定义函数体提供默认实现。派生类通过 A::f() 调用</span></span><br><span class="line">&#125;;</span><br><span class="line">A obj; <span class="comment">/// 不准抽象类定义对象！编译不通过！</span></span><br></pre></td></tr></table></figure><ul><li>抽象类<ul><li>定义：含有至少一个纯虚函数。</li><li>特点：<ul><li>不允许定义对象。</li><li>只能为派生类提供接口。</li><li>能避免对象切片：保证只有指针和引用能被向上类型转换。</li></ul></li></ul></li><li>基类纯虚函数被派生类重写覆盖之前仍是纯虚函数。因此当继承一个抽象类时，必须实现所有纯虚函数，否则继承出的类也是抽象类。</li><li>纯虚析构函数除外<ul><li>对于纯虚析构函数而言，即便派生类中不显式实现，编译器也会自动合成默认析构函数。因此，即使派生类不覆盖纯虚析构函数，派生类可以不是抽象类，可以定义派生类对象。</li><li>回顾：虚函数与析构函数<ul><li>析构函数能是虚的，且常常是虚的。<strong>虚析构函数仍需定义函数体</strong>。</li><li>虚析构函数的用途：当删除基类对象指针时，编译器将根据指针所指对象的实际类型，调用相应的析构函数。</li></ul></li><li>析构函数也可以是纯虚函数<ul><li>纯虚析构函数仍然需要函数体</li><li>目的：使基类成为抽象类，不能创建基类的对象。如果有其他函数是纯虚函数，则析构函数不必是纯虚的。</li></ul></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.1.2</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>: </span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">Base</span>()=<span class="number">0</span>; </span><br><span class="line">&#125;;</span><br><span class="line">Base::~<span class="built_in">Base</span>() &#123;&#125; <span class="comment">/// 必须有函数体</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derive</span> :</span> <span class="keyword">public</span> Base &#123;&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base b; <span class="comment">/// 编译错误，基类是抽象类</span></span><br><span class="line">    Derive d1;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="10-2-向下类型转换"><a href="#10-2-向下类型转换" class="headerlink" title="10.2 向下类型转换"></a>10.2 向下类型转换</h3><ul><li><p>基类指针/引用转换成派生类指针/引用，则称为向下类型转换。（类层次中向下移动）</p></li><li><p>如何确保转换的正确性？</p><ul><li>如何保证基类指针指向的对象也可以被要转换的派生类的指针指向？—— 借助虚函数表进行动态类型检查！</li></ul></li><li><p>C++提供了一个特殊的显式类型转换，称为<code>dynamic_cast</code>，是一种安全类型向下类型转换。</p><ul><li>使用dynamic_cast的对象必须有虚函数，因为它使用了存储在虚函数表中的信息判断实际的类型。使用方法：<ul><li>obj_p，obj_r分别是T1类型的指针和引用<ul><li><code>T2* pObj = dynamic_cast&lt;T2*&gt;(obj_p);</code>//转换为T2指针，运行时失败返回 <code>nullptr</code></li><li><code>T2&amp; refObj = dynamic_cast&lt;T2&amp;&gt;(obj_r);</code> //转换为T2引用，运行时失败抛出 <code>bad_cast</code> 异常</li></ul></li><li>T1必须是多态类型（声明或继承了至少一个虚函数的类），否则不过编译；T2不必。T1,T2没有继承关系也能通过编译，只不过运行时会转换失败。</li></ul></li><li>如果我们知道正在处理的是哪些类型，可以使用static_cast来避免这种开销。<ul><li>static_cast在编译时静态浏览类层次，只检查继承关系。没有继承关系的类之间，必须具有转换途径才能进行转换（要么自定义，要么是语言语法支持），否则不过编译。运行时无法确认是否正确转换。</li><li>static_cast使用方法：<ul><li>obj_p，obj_r分别是T1类型的指针和引用</li><li><code>T2* pObj = static_cast&lt;T2*&gt;(obj_p);</code> //转换为T2指针</li><li><code>T2&amp; refObj = static_cast&lt;T2&amp;&gt;(obj_r);</code> //转换为T2引用</li><li>不安全：不保证转换后的目标是T2类型的，可能导致非法内存访问。</li></ul></li></ul></li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.2.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">f</span><span class="params">()</span></span>&#123;&#125;;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span> :</span> <span class="keyword">public</span> B &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> i&#123;<span class="number">2018</span>&#125;;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    D d;</span><br><span class="line">    B b;</span><br><span class="line">    <span class="comment">//    D d1 = static_cast&lt;D&gt;(b); ///未定义类型转换方式</span></span><br><span class="line">    <span class="comment">//    D d2 = dynamic_cast&lt;D&gt;(b); ///只允许指针和引用转换</span></span><br><span class="line"></span><br><span class="line">    D* pd1 = <span class="keyword">static_cast</span>&lt;D*&gt;(&amp;b);  <span class="comment">/// 有继承关系，允许转换</span></span><br><span class="line">    <span class="keyword">if</span> (pd1 != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;static_cast, B*(B) --&gt; D*: OK&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;D::i=&quot;</span> &lt;&lt; pd1-&gt;i &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/// 但是不安全：对D中成员i可能非法访问</span></span><br><span class="line"></span><br><span class="line">    D* pd2 = <span class="keyword">dynamic_cast</span>&lt;D*&gt;(&amp;b);</span><br><span class="line">    <span class="keyword">if</span> (pd2 == <span class="literal">nullptr</span>)  <span class="comment">/// 不允许不安全的转换</span></span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;dynamic_cast, B*(B) --&gt; D*: FAILED&quot;</span> &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="keyword">static_cast</span>, B*(B) --&gt; D*:OK</span><br><span class="line">&gt;&gt;&gt; D::i=<span class="number">124455624</span></span><br><span class="line">&gt;&gt;&gt; <span class="keyword">dynamic_cast</span>, B*(B) --&gt; D*: FAILED</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.2.2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">f</span><span class="params">()</span></span>&#123;&#125;;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span> :</span> <span class="keyword">public</span> B &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">int</span> i&#123;<span class="number">2018</span>&#125;;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    D d;</span><br><span class="line">    B b;</span><br><span class="line">    <span class="comment">//    D d1 = static_cast&lt;D&gt;(b); ///未定义类型转换</span></span><br><span class="line">    <span class="comment">//    D d2 = dynamic_cast&lt;D&gt;(b); ///只允许指针和引用转换</span></span><br><span class="line">    B* pb = &amp;d;</span><br><span class="line">    D* pd3 = <span class="keyword">static_cast</span>&lt;D*&gt;(pb);</span><br><span class="line">    <span class="keyword">if</span> (pd3 != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;static_cast, B*(D) --&gt; D*: OK&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;D::i=&quot;</span> &lt;&lt; pd3-&gt;i &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    D* pd4 = <span class="keyword">dynamic_cast</span>&lt;D*&gt;(pb);</span><br><span class="line">    <span class="keyword">if</span> (pd4 != <span class="literal">nullptr</span>) &#123;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;dynamic_cast, B*(D) --&gt; D*: OK&quot;</span> &lt;&lt; endl;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;D::i=&quot;</span> &lt;&lt; pd4-&gt;i &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; <span class="keyword">static_cast</span>, B*(D) --&gt; D*: OK</span><br><span class="line">&gt;&gt;&gt; D::i=<span class="number">2018</span></span><br><span class="line">&gt;&gt;&gt; <span class="keyword">dynamic_cast</span>, B*(D) --&gt; D*: OK</span><br><span class="line">&gt;&gt;&gt; D::i=<span class="number">2018</span></span><br></pre></td></tr></table></figure><ul><li>重要原则(清楚指针所指向的真正对象)：<br>1）指针或引用的向上转换总是安全的；<br>2）向下转换时用dynamic_cast，安全检查；<br>3）避免对象之间的转换。</li><li>对于基类中有虚函数的情况：<ul><li>向上类型转换：<ul><li>转换为基类指针或引用，则对应虚函数表仍为派生类的虚函数表（晚绑定）。</li><li>转换为基类对象，则对应虚函数表是基类的虚函数表（早绑定）。</li></ul></li><li>向下类型转换：<br>dynamic_cast通过虚函数表来判断是否能进行向下类型转换。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.2.3</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pet</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">virtual</span> ~<span class="built_in">Pet</span>() &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dog</span> :</span> <span class="keyword">public</span> Pet &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;dog run&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bird</span> :</span> <span class="keyword">public</span> Pet &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">fly</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;bird fly&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">action</span><span class="params">(Pet* p)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> d = <span class="keyword">dynamic_cast</span>&lt;Dog*&gt;(p);   <span class="comment">/// 向下类型转换</span></span><br><span class="line">    <span class="keyword">auto</span> b = <span class="keyword">dynamic_cast</span>&lt;Bird*&gt;(p);  <span class="comment">/// 向下类型转换</span></span><br><span class="line">    <span class="keyword">if</span> (d)  <span class="comment">/// 运行时根据实际类型表现特性</span></span><br><span class="line">        d-&gt;<span class="built_in">run</span>();</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (b)</span><br><span class="line">        b-&gt;<span class="built_in">fly</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Pet* p[<span class="number">2</span>];</span><br><span class="line">    p[<span class="number">0</span>] = <span class="keyword">new</span> Dog;   <span class="comment">/// 向上类型转换</span></span><br><span class="line">    p[<span class="number">1</span>] = <span class="keyword">new</span> Bird;  <span class="comment">/// 向上类型转换</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">        <span class="built_in">action</span>(p[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="10-3-多重继承的虚函数表与利弊"><a href="#10-3-多重继承的虚函数表与利弊" class="headerlink" title="10.3 多重继承的虚函数表与利弊"></a>10.3 多重继承的虚函数表与利弊</h3><ul><li>多重继承中的虚函数<ul><li>最多继承一个非抽象类 <strong>避免</strong> 多重继承的二义性</li><li>可以集成多个抽象类 <strong>利用</strong> 一个对象可以实现多个接口</li></ul></li></ul><h3 id="10-4-多态"><a href="#10-4-多态" class="headerlink" title="10.4 多态"></a>10.4 多态</h3><ul><li>按照基类的接口定义，调用指针或引用所指对象的接口函数，函数执行过程因对象实际所属派生类的不同而呈现不同的效果（表现），这个现象被称为“多态”。<ul><li>当利用基类指针/引用调用函数时<ul><li>虚函数在运行时确定执行哪个版本，取决于引用或指针对象的真实类型</li><li>非虚函数在编译时绑定</li></ul></li><li>当利用类的对象直接调用函数时<ul><li>无论什么函数，均在编译时绑定</li></ul></li><li>产生多态效果的条件：继承 &amp;&amp; 虚函数 &amp;&amp; (引用 || 指针)</li></ul></li><li>应用：TEMPLATE METHOD设计模式<ul><li>在接口的一个方法中定义算法的骨架</li><li>将一些步骤的实现延迟到子类中</li><li>使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤。</li></ul></li><li>模板方法是一种源代码重用的基本技术，在类库的设计实现中应用十分广泛，因为这个设计模式能有效地解决 “类库提供公共行为”与“用户定制特殊细节”之间的折中平衡。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.4.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">action</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="built_in">step1</span>();</span><br><span class="line">        <span class="built_in">step2</span>();</span><br><span class="line">        <span class="built_in">step3</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">step1</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::step1&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">step2</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::step2&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">step3</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Base::step3&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived1</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">step1</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Derived1::step1&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived2</span> :</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">step2</span><span class="params">()</span> </span>&#123; cout &lt;&lt; <span class="string">&quot;Derived2::step2&quot;</span> &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Base* ba[] = &#123;<span class="keyword">new</span> Base, <span class="keyword">new</span> Derived1, <span class="keyword">new</span> Derived2&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i) &#123;</span><br><span class="line">        ba[i]-&gt;<span class="built_in">action</span>();</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;===&quot;</span> &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">Base::step1</span><br><span class="line">Base::step2</span><br><span class="line">Base::step3</span><br><span class="line">===</span><br><span class="line">Derived1::step1</span><br><span class="line">Base::step2</span><br><span class="line">Base::step3</span><br><span class="line">===</span><br><span class="line">Base::step1</span><br><span class="line">Derived2::step2</span><br><span class="line">Base::step3</span><br><span class="line">===</span><br></pre></td></tr></table></figure><h3 id="10-5-函数模板与类模板"><a href="#10-5-函数模板与类模板" class="headerlink" title="10.5 函数模板与类模板"></a>10.5 函数模板与类模板</h3><ol><li>函数模板</li></ol><ul><li>有些算法实现与类型无关，所以可以将函数的参数类型也定义为一种特殊的“参数”，这样就得到了“函数模板”。</li><li>定义函数模板的方法<ul><li><code>template &lt;typename T&gt; ReturnType Func(Args)；</code></li><li>如：任意类型两个变量相加的“函数模板”</li><li><code>template &lt;typename T&gt; T sum(T a, T b) &#123; return a + b; &#125;</code></li><li>注：typename也可换为class</li></ul></li><li>函数模板在调用时，编译器能自动推导出实际参数的类型（这个过程叫做实例化）。<ul><li>所以，形式上调用一个函数模板与普通函数没有区别。</li><li>当多个参数的类型不一致时，无法推导：<ul><li><code>cout &lt;&lt; sum(9, 2.1);</code> //编译错误</li><li>手工指定调用类型：<code>sum&lt;int&gt;(9, 2.1);</code></li></ul></li></ul></li></ul><ol><li>类模板</li></ol><ul><li>在定义类时也可以将一些类型信息抽取出来，用模板参数来替换，从而使类更具通用性。这种类被称为“类模板”。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.5.1</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line">    T data;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span> </span>&#123; cout &lt;&lt; data &lt;&lt; endl; &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A&lt;<span class="keyword">int</span>&gt; a;</span><br><span class="line">    a.<span class="built_in">print</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>类模板中成员函数的类外定义</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.5.2</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line">    T data;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">void</span> A&lt;T&gt;::<span class="built_in">print</span>() &#123;</span><br><span class="line">    cout &lt;&lt; data &lt;&lt; endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    A&lt;<span class="keyword">int</span>&gt; a;</span><br><span class="line">    a.<span class="built_in">print</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>类模板的“模板参数”<ul><li>类型参数：使用typename或class标记</li><li>非类型参数：整数，枚举，指针（指向对象或函数），引用（引用对象或引用函数）。整数型比较常用。</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 10.5.3</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">unsigned</span> size&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">array</span> &#123;</span></span><br><span class="line">    T elems[size];</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">array&lt;<span class="keyword">char</span>, 10&gt; array0;</span><br></pre></td></tr></table></figure><ul><li>模板与多态<ul><li>模板使用泛型标记，使用同一段代码，来关联不同但相似的特定行为，最后可以获得不同的结果。模板也是多态的一种体现。</li><li>但模板的关联是在编译期处理，称为静多态。<ul><li>往往和函数重载同时使用</li><li>高效，省去函数调用</li><li>编译后代码增多</li></ul></li><li>基于继承和虚函数的多态在运行期处理，称为动多态<ul><li>运行时，灵活方便</li><li>侵入式，必须继承</li><li>存在函数调用</li></ul></li></ul></li></ul><h2 id="Week-11-模板与STL初步"><a href="#Week-11-模板与STL初步" class="headerlink" title="Week 11 模板与STL初步"></a>Week 11 模板与STL初步</h2><h3 id="11-0-Overview"><a href="#11-0-Overview" class="headerlink" title="11.0 Overview"></a>11.0 Overview</h3><ul><li><del>类模板与函数模板特化</del></li><li>命名空间</li><li>STL初步——容器与迭代器</li></ul><h3 id="11-1-命名空间"><a href="#11-1-命名空间" class="headerlink" title="11.1 命名空间"></a>11.1 命名空间</h3><ul><li>为了避免在大规模程序的设计中，以及在程序员使用各种各样的C++库时，标识符的命名发生冲突，标准C++引入了关键字namespace（命名空间），可以更好地控制标识符的作用域。</li><li>标准C++库（不包括标准C库）中所包含的所有内容（包括常量、变量、结构、类和函数等）都被定义在命名空间std（standard标准）中。</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Example 11.1.1</span></span><br><span class="line"><span class="comment">// 定义命名空间</span></span><br><span class="line"><span class="keyword">namespace</span> A &#123;</span><br><span class="line"><span class="keyword">int</span> x, y;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 使用命名空间</span></span><br><span class="line">A::x = <span class="number">3</span>;</span><br><span class="line">A::y = <span class="number">6</span>;</span><br></pre></td></tr></table></figure><ul><li>使用using声明简化命名空间使用</li><li>使用整个命名空间：所有成员都直接可用<code>using namespace A;</code> <code>x = 3; y = 6;</code></li><li>使用部分成员：所选成员可直接使用 <code>using A::x;</code> <code>x = 3; A::y = 6;</code></li><li>任何情况下，都不应出现命名冲突</li></ul><h3 id="11-2-STL初步"><a href="#11-2-STL初步" class="headerlink" title="11.2 STL初步"></a>11.2 STL初步</h3><ul><li>标准模板库（英文：Standard Template Library，缩写：STL），是一个高效的C++软件库，它被容纳于C++ 标准程序库C++ Standard Library中。其中包含4个组件，分别为算法、容器、函数、迭代器。基于模板编写。关键理念：将“在数据上执行的操作”与“要执行操作的数据”分离。</li><li>简单容器<ul><li>容器是包含、放置数据的工具。通常为数据结构。<ul><li>简单容器（simple container）</li><li>序列容器（sequence container）</li><li>关系容器（associative container）</li></ul></li></ul></li><li><code>std::pair</code></li><li><code>std::tuple</code></li><li><code>std::vector</code><ul><li>创建：<code>std::vector&lt;int&gt; x;</code></li><li>当前数组长度： <code>x.size();</code></li><li>清空： <code>x.clear();</code></li><li>在末尾添加/删除：（高速）<code>x.push_back(1); x.pop_back();</code></li><li>在中间添加/删除：（使用迭代器，低速）<code>x.insert(x.begin()+1, 5);</code> <code>x.erase(x.begin()+1);</code></li></ul></li><li>迭代器<ul><li>一种检查容器内元素并遍历元素的数据类型。</li><li>提供一种方法顺序访问一个聚合对象中各个元素, 而又不需暴露该对象的内部表示。</li><li>为遍历不同的聚合结构（需拥有相同的基类）提供一个统一的接口。</li><li>使用上类似指针。</li></ul></li><li>迭代器：失效</li><li>当迭代器不再指向本应指向的元素时，称此迭代器失效。<ul><li>vector中什么情况下会发生迭代器失效？</li><li>看作纯粹的指针<ul><li>调用insert/erase后，所修改位置之后的所有迭代器失效。（原先的内存空间存储的元素被改变）</li><li>调用push_back等修改vector大小的方法时，可能会使所有迭代器失效（Push_back到了一定程度之后，可能会造成数组的整体移动，导致所有的内存地址发生改变。）</li></ul></li></ul></li><li><code>std::list</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插入前端：</span></span><br><span class="line"> l.<span class="built_in">push_front</span>(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 插入末端：</span></span><br><span class="line"> l.<span class="built_in">push_back</span>(<span class="number">2</span>); </span><br><span class="line"><span class="comment">// 查询：</span></span><br><span class="line"> std::<span class="built_in">find</span>(l.<span class="built_in">begin</span>(), l.<span class="built_in">end</span>(), <span class="number">2</span>); <span class="comment">//返回迭代器</span></span><br><span class="line"><span class="comment">// 插入指定位置：</span></span><br><span class="line"> l.<span class="built_in">insert</span>(it, <span class="number">4</span>); <span class="comment">//it为迭代器</span></span><br></pre></td></tr></table></figure><ul><li><code>std::list</code><ul><li>不支持下标等随机访问</li><li>支持高速的在任意位置插入/删除数据</li><li>其访问主要依赖迭代器</li><li>操作不会导致迭代器失效（除指向被删除的元素的迭代器外）</li></ul></li><li><code>std::set</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 插入：</span></span><br><span class="line"> s.<span class="built_in">insert</span>(<span class="number">1</span>);</span><br><span class="line"><span class="comment">// 查询：</span></span><br><span class="line"> s.<span class="built_in">find</span>(<span class="number">1</span>);   <span class="comment">//返回迭代器</span></span><br><span class="line"><span class="comment">// 删除：</span></span><br><span class="line"> s.<span class="built_in">erase</span>(s.<span class="built_in">find</span>(<span class="number">1</span>));   <span class="comment">//导致被删除元素的迭代器失效</span></span><br><span class="line"><span class="comment">// 统计：</span></span><br><span class="line"> s.<span class="built_in">count</span>(<span class="number">1</span>);   <span class="comment">//1的个数，总是0或1</span></span><br></pre></td></tr></table></figure><ul><li><p><code>std::map</code></p><ul><li>其值类型为pair。</li><li>map中的元素key互不相同，需要key存在比较器。</li><li>可以通过下标访问（即使key不是整数）。下标访问时如果元素不存在，则创建对应元素。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
          <category> 理论/面向对象 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OOP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LaTeX 从入坑到退坑</title>
      <link href="/2022/03/01/LaTeX%20Introduction/"/>
      <url>/2022/03/01/LaTeX%20Introduction/</url>
      
        <content type="html"><![CDATA[<p>虽然 Markdown 很好用，但是生成的 pdf 文档看起来就是没有范不够正式。此外，使用 LaTeX 也是我们之后写论文的必备技能。</p><p>本教程主要涉及已对 Markdown 较为熟识之后的迁移学习。</p><h2 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h2><ul><li>Markdown 入门</li><li>Markdown 编写数学公式的方法</li></ul><h2 id="LaTeX-简介"><a href="#LaTeX-简介" class="headerlink" title="LaTeX 简介"></a>LaTeX 简介</h2><p>我们首先解决以下几个问题：</p><ul><li>LaTeX 是什么？</li><li>LaTeX 怎样运作？</li><li>我该在哪里写 LaTex？</li></ul><p>在解决上述几个问题之后，我们再详细介绍该怎么写 LaTeX 的问题。</p><h3 id="LaTeX-是什么？"><a href="#LaTeX-是什么？" class="headerlink" title="LaTeX 是什么？"></a>LaTeX 是什么？</h3><p>相信你读到这里已经做好觉悟要被灌输一系列关键词和其对应概念了。让我们开始：</p><blockquote><p>历史回溯到 Knuth 教授的巨著 The Art of Computer Programming 将要出版之际，当出版商将他们排版的书稿草样交给 Knuth 教授的时候，他对于其中复杂数学公式的排版处理十分不满。其排版之粗糙，已达到了会影响人们理解原书内容的程度。因此，对于复杂的数学和物理公式，我们急需一种能够将其在互联网上传输的编码格式，使得人能阅读的公式和机器能存储的公式之间达到一种互相转化。</p></blockquote><ul><li><strong>TeX</strong>：一种排版引擎，也是该引擎使用的标记语言的名称。引擎是指能够断行、分页的程序，标记语言是控制命令和文本结合的格式。可以类比理解成你写的 C++ 源代码或者更底层的机器指令码，如输入 <script type="math/tex">2^6</script>。</li><li><strong>LaTeX</strong>：是一个基于 TeX 的排版系统，将用户按照它的格式编写的文档解释成 TeX 引擎能理解的形式并交付给 TeX 引擎处理，再将最终结果返回给用户。可以类比理解成 g++ 编译器，将上述代码渲染为 2626。</li><li><strong>pdfTeX</strong> 与 <strong>pdfLaTeX</strong>：原版 TeX 系统生成的文件是 <code>dvi</code> 格式，而 pdfTeX 系统下生成的文件是 <code>pdf</code> 格式。</li><li><strong>XeTeX</strong> 与 <strong>XeLaTeX</strong>：上述 TeX 系统生成的字符集只支持 ASCII 字符。在 XeTeX 出现之前，我们曾使用过引用引入 CJK 宏库（解决不支持中日韩字符问题的一个库）手段来处理中文字符的问题。但是这个排版系统对所有 Unicode 字符都实现了支持。</li><li><strong>LuaTeX</strong> 与 <strong>LuaLaTeX</strong>：<code>pdfTeX</code> 系统的继承者，支持使用一些用户自定义脚本来实现之前需要写成 TeX 的功能。支持 Unicode，内联 lua，支持 OpenType。</li></ul><p>这里我们推荐使用 <code>XeTeX</code> 系统来进行我们日常的工作，我们后续的教程也围绕这个排版系统展开。为了简洁起见，我们后续不再区分上述概念，统一使用 <code>TeX</code>，<code>LaTeX</code> 来表述我们在说的这个话题。</p><h3 id="LaTeX-怎样运作？"><a href="#LaTeX-怎样运作？" class="headerlink" title="LaTeX 怎样运作？"></a>LaTeX 怎样运作？</h3><p>LaTeX 排版系统的输入是含有我们敲的文本和控制命令的 <code>tex</code> 文件，输出是一份 <code>pdf</code> 文件。我们只需要负责在 <code>tex</code> 文件中写下源码，然后剩下的编译和生成工作全部交给 LaTeX 即可。</p><p>有时我们还可以把一个 LaTeX 项目组织成一个文件夹，此时还是一份 <code>tex</code> 文件决定一个 <code>pdf</code> 的生成，但是我们还可以在这个文件夹中引入其他一些文件，如字体文件，图片文件，<code>.cls</code> 文件（文档模板类文件）等等。此外，我们还可以在这个文件夹中编写多个 <code>tex</code> 文件，以共享文件夹中的其他资源。此时不同的 <code>tex</code> 文件之间甚至可以项目包含（类比于 C++ 的 <code>#include</code> 包含）。</p><h3 id="在哪里编写-LaTeX？"><a href="#在哪里编写-LaTeX？" class="headerlink" title="在哪里编写 LaTeX？"></a>在哪里编写 LaTeX？</h3><p>我们有离线和在线两种模式来撰写 LaTeX。</p><p>离线模式就是安装一个 LaTeX 排版系统，类比我们想写 Markdown 的时候装了一个 Typora 软件一样，我们可以安装相应的软件来辅助我们工作，如：</p><ul><li>TeXworks</li><li>TeXstudio</li></ul><p>而使用这种方式安装带来的问题是可能安装包过于臃肿，优点是不用受到网络环境等等因素的干扰，也不用受到网络环境存储容量或运行时环境的限制。其安装方式在网络上搜索“LaTeX 入门”便可找到堆积如山的<a href="https://www.zhihu.com/question/62943097">教程</a>。</p><p>而我们这里推崇的方式就是使用在线方式来编写。如 <code>Overleaf</code> 在内的托管网站会将你的每个 TeX 项目组织成一个仓库的形式，并允许你在其中进行在线编辑：</p><p><img src="https://s2.loli.net/2022/02/14/JcqCKNfs31vxm2u.png" alt="image-20220213203750216"></p><p><img src="https://s2.loli.net/2022/02/13/IKAPDlYTmFEu5S1.png" alt="image-20220213204115880"></p><p>左上角是我们当前仓库的文件清单，较左侧窗口是编辑器，右侧窗口是即时预览窗口。类似于 <code>Overleaf</code> 的网站甚至还提供了仓库权限管理系统，你可以邀请其他人一起编辑，或是导入别人编辑好的模板继续你的编辑等等。值得一提的是许多学术会议都会给出他们接受的论文的模板。</p><p>鉴于 <code>Overleaf</code> 需要科学上网才能访问：</p><ul><li>某清 TUNA 协会维护了一份 <a href="https://overleaf.tsinghua.edu.cn/login">Tsinghua Overleaf</a>，需要使用清华统一认证登录;</li><li>清华大学某系某协会网络部维护了一份自己的基于 <code>Overleaf</code> 的 LaTeX 在线编辑网站 <a href="https://stu.cs.tsinghua.edu.cn/tex9/">TeX9</a>，需要使用酒井 ID 才能进行登录。</li></ul><p>我们接下来的演示便是基于 Overleaf。</p><h2 id="LaTeX-编写基础"><a href="#LaTeX-编写基础" class="headerlink" title="LaTeX 编写基础"></a>LaTeX 编写基础</h2><p>说是编写基础，接下来我们就要像介绍 Markdown 一样，先简单罗列一些简单的文档控制命令。在基础篇中我们先仅仅介绍怎样实现从 Markdown 到 LaTeX 的迁移。对于其中一些文档控制命令，我们将会在后续教程详细说明。</p><p>这里提供 CheatSheet 供查阅：</p><p><img src="https://s2.loli.net/2022/02/14/KP5yeuotcJhTlVO.png" alt="image-20220214004156979"></p><p><img src="https://s2.loli.net/2022/02/14/j3EHglvqhoetRi4.png" alt="image-20220214004207594"></p><h3 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello, World!"></a>Hello, World!</h3><p><img src="https://s2.loli.net/2022/02/13/HPCaZJUtNOj74uF.png" alt="image-20220213205714169"></p><p>上述便是一份 <code>tex</code> 文件的示例，我们推荐你新建一个项目，然后将下面我们要介绍的内容一一尝试。</p><h3 id="支持中文字符"><a href="#支持中文字符" class="headerlink" title="支持中文字符"></a>支持中文字符</h3><p>首先，我们上述已经介绍过，支持中文字符的方式有二，一种是引入 <code>CJK</code> 宏包，另一种是使用 XeLaTeX 编译器并对源码做适当修改。这里我们采用第二种方式。</p><p>首先，按下你项目左上角的 Menu 按钮，然后在 Compiler 选项中选择 XeLaTeX 选项。</p><p>然后，输入以下内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">你好，world!</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>这样我们就完成了中文字符的引入。至于 <code>documentclass</code> 是什么，我们将在后续介绍。</p><h3 id="导言与文档信息"><a href="#导言与文档信息" class="headerlink" title="导言与文档信息"></a>导言与文档信息</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\title&#123;Sample Document&#125;</span><br><span class="line">\author&#123;reta&#125;</span><br><span class="line">\date&#123;\today&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">% 这条控制命令会读取导言部分的文档相关信息</span><br><span class="line">% 并将其渲染到文档中</span><br><span class="line">% 事实上可以参考相关宏包的 Doc：</span><br><span class="line">% http://texdoc.net/texmf-dist/doc/latex/titling/titling.pdf</span><br><span class="line">\maketitle </span><br><span class="line"></span><br><span class="line">你好，world!</span><br><span class="line"></span><br><span class="line">[在这里你就开始写你的作业第一题了]</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://pic.iqy.ink/2022/03/02/1e0c7964755a9.png" alt="1646156273418.png"></p><h3 id="章节与段落"><a href="#章节与段落" class="headerlink" title="章节与段落"></a>章节与段落</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line">\title&#123;Sample Document&#125;</span><br><span class="line">\author&#123;reta&#125;</span><br><span class="line">\date&#123;\today&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\maketitle</span><br><span class="line"></span><br><span class="line">\section&#123;我是 Section 标题&#125;</span><br><span class="line"></span><br><span class="line">我是 Section 介绍。</span><br><span class="line"></span><br><span class="line">\subsection&#123;我是 Subsection 标题&#125;</span><br><span class="line">我是 Subsection 介绍。</span><br><span class="line"></span><br><span class="line">\subsubsection&#123;我是 Subsubsection 标题&#125;</span><br><span class="line">我是 Subsubsection 介绍。</span><br><span class="line"></span><br><span class="line">% \subsubsubsection&#123;不能继续套 sub 了，到底了&#125;</span><br><span class="line"></span><br><span class="line">\paragraph&#123;我是 Paragraph 标题&#125;</span><br><span class="line">我是 Paragraph 后面跟着写的东西。</span><br><span class="line">本人也是经过了深思熟虑，在每个日日夜夜思考这个问题。</span><br><span class="line">我们都知道，只要有意义，那么就必须慎重考虑。</span><br><span class="line">这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。</span><br><span class="line">要想清楚，一天掉多少根头发，到底是一种怎么样的存在。</span><br><span class="line">贝多芬曾经说过，卓越的人一大优点是：在不利与艰难的遭遇里百折不饶。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">\subparagraph&#123;我是 Subparagraph 标题&#125;</span><br><span class="line">我是 Subparagraph 后面跟着写的东西。</span><br><span class="line">这不禁令我深思既然如何，一天掉多少根头发的发生，到底需要如何做到，不一天掉多少根头发的发生，又会如何产生。 </span><br><span class="line">总结的来说， 所谓一天掉多少根头发，关键是一天掉多少根头发需要如何写。 生活中，若一天掉多少根头发出现了，我们就不得不考虑它出现了的事实。 郭沫若曾经说过，形成天才的决定因素应该是勤奋。这不禁令我深思这种事实对本人来说意义重大，相信对这个世界也是有一定意义的。 我们一般认为，抓住了问题的关键，其他一切则会迎刃而解。</span><br><span class="line"></span><br><span class="line">\subsection&#123;这是第二节&#125;</span><br><span class="line">\paragraph&#123;第二节&#125; 的首段。</span><br><span class="line">\subparagraph&#123;第二节&#125;的第二段。</span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://pic.iqy.ink/2022/03/02/d6efe811d4f2e.png" alt="1646156390864.png"></p><p>在文档类 <code>article</code>/<code>ctexart</code> 中（文档类的概念我们会在进阶篇中提供指导），我们使用这些控制序列来调整行文组织结构。他们分别是：</p><ul><li><code>\section&#123;·&#125;</code></li><li><code>\subsection&#123;·&#125;</code></li><li><code>\subsubsection&#123;·&#125;</code></li><li><code>\paragraph&#123;·&#125;</code></li><li><code>\subparagraph&#123;·&#125;</code></li></ul><h3 id="文档目录"><a href="#文档目录" class="headerlink" title="文档目录"></a>文档目录</h3><p>我们尝试在渲染区 <code>\maketitle</code> 后加入如下控制命令：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\tableofcontents</span><br></pre></td></tr></table></figure><p>没错，正如你所想的，这就会生成文档的 TOC：</p><p><img src="https://pic.iqy.ink/2022/03/02/1015a9adc35ce.png" alt="1646156495002.png"></p><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><h4 id="行内公式与行间公式"><a href="#行内公式与行间公式" class="headerlink" title="行内公式与行间公式"></a>行内公式与行间公式</h4><p>首先引入相应包 <strong>amsmath</strong>，然后我们简单介绍公式的引入：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">\documentclass[UTF8]&#123;ctexart&#125;</span><br><span class="line"></span><br><span class="line">\usepackage&#123;amsmath&#125; % 注意这里引入相应包</span><br><span class="line"></span><br><span class="line">\title&#123;Sample Document&#125;</span><br><span class="line">\author&#123;reta&#125;</span><br><span class="line">\date&#123;\today&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line"></span><br><span class="line">\maketitle</span><br><span class="line"></span><br><span class="line">\tableofcontents</span><br><span class="line"></span><br><span class="line">\section&#123;我是 Section 标题&#125;</span><br><span class="line"></span><br><span class="line">我是 Section 介绍。</span><br><span class="line"></span><br><span class="line">\subsection&#123;这个 Section 我们介绍数学公式的写法&#125;</span><br><span class="line"></span><br><span class="line">\subsubsection&#123;行内公式&#125;</span><br><span class="line"></span><br><span class="line">% 行内公式基本可以照搬 Markdown 的模式。</span><br><span class="line"></span><br><span class="line">初始处理 1 - 5 位的初始字符串集合需要处理 $18 + 18^2 + 18^3 + 18^4 + 18^5 = 2*10^6$ 的数据，因此需要 $O(T)$ 的时间，这里 $T=2*10^6$。</span><br><span class="line"></span><br><span class="line">\subsubsection&#123;行间公式&#125;</span><br><span class="line"></span><br><span class="line">% 行间公式用 $$ $$ 或者 \[ \] 来框住都可以，但在 LaTeX 中前者会改变行文的默认行间距，因此不推荐采用。</span><br><span class="line"></span><br><span class="line">\[</span><br><span class="line">\text&#123;dp&#125;[i] = \text&#123;dp&#125;[next[i]]+1, next[i] &gt; 0.</span><br><span class="line">\]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p><img src="https://pic.iqy.ink/2022/03/02/be2695e1157ac.png" alt="1646156592489.png"></p><h4 id="上下标、根式与分式"><a href="#上下标、根式与分式" class="headerlink" title="上下标、根式与分式"></a>上下标、根式与分式</h4><ul><li>上下标请使用 <code>^</code> 与 <code>_</code></li><li>根式与分式请使用 <code>\sqrt&#123;·&#125;</code> 与 <code>\frac&#123;·&#125;&#123;·&#125;</code></li><li>在行间公式和行内公式中，分式的输出效果是有差异的。如果要强制行内模式的分式显示为行间模式的大小，可以使用 <code>\dfrac</code>, 反之可以使用 <code>\tfrac</code></li></ul><h4 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h4><p>一些小的运算符，可以在数学模式下直接输入；另一些需要用控制序列生成，如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\[ \pm\; \times \; \div\; \cdot\; \cap\; \cup\;</span><br><span class="line">\geq\; \leq\; \neq\; \approx \; \equiv \]</span><br></pre></td></tr></table></figure><p>连加、连乘、极限、积分等大型运算符分别用 <code>\sum</code>, <code>\prod</code>, <code>\lim</code>, <code>\int</code> 生成。他们的上下标在行内公式中被压缩，以适应行高。我们可以用 <code>\limits</code> 和 <code>\nolimits</code> 来强制显式地指定是否压缩这些上下标。例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ \sum_&#123;i=1&#125;^n i\quad \prod_&#123;i=1&#125;^n $</span><br><span class="line">$ \sum\limits _&#123;i=1&#125;^n i\quad \prod\limits _&#123;i=1&#125;^n $</span><br><span class="line">\[ \lim_&#123;x\to0&#125;x^2 \quad \int_a^b x^2 dx \]</span><br><span class="line">\[ \lim\nolimits _&#123;x\to0&#125;x^2\quad \int\nolimits_a^b x^2 dx \]</span><br></pre></td></tr></table></figure><p>多重积分可以使用 <code>\iint</code>, <code>\iiint</code>, <code>\iiiint</code>, <code>\idotsint</code> 等命令输入。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\[ \iint\quad \iiint\quad \iiiint\quad \idotsint \]</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/02/13/VeI1zToC3JfAyK9.png" alt="image-20220213214301901"></p><h4 id="定界符"><a href="#定界符" class="headerlink" title="定界符"></a>定界符</h4><p>各种括号用 <code>()</code>, <code>[]</code>, <code>\&#123;\&#125;</code>, <code>\langle\rangle</code> 等命令表示；注意花括号通常用来输入命令和环境的参数，所以在数学公式中它们前面要加 <code>\</code>。</p><p>因为 LaTeX 中 <code>|</code> 和 <code>\|</code> 的应用过于随意，amsmath 宏包推荐用 <code>\lvert\rvert</code> 和 <code>\lVert\rVert</code> 取而代之。</p><p>为了调整这些定界符的大小，amsmath 宏包推荐使用 <code>\big</code>, <code>\Big</code>, <code>\bigg</code>, <code>\Bigg</code> 等一系列命令放在上述括号前面调整大小。</p><h4 id="省略号"><a href="#省略号" class="headerlink" title="省略号"></a>省略号</h4><p>省略号用 <code>\dots</code>, <code>\cdots</code>, <code>\vdots</code>, <code>\ddots</code> 等命令表示。<code>\dots</code> 和 <code>\cdots</code> 的纵向位置不同，前者一般用于有下标的序列。</p><h4 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h4><p><code>amsmath</code> 的 <code>pmatrix</code>, <code>bmatrix</code>, <code>Bmatrix</code>, <code>vmatrix</code>, <code>Vmatrix</code> 等环境可以在矩阵两边加上各种分隔符。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\[ \begin&#123;pmatrix&#125; a&amp;b\\c&amp;d \end&#123;pmatrix&#125; \quad</span><br><span class="line">\begin&#123;bmatrix&#125; a&amp;b\\c&amp;d \end&#123;bmatrix&#125; \quad</span><br><span class="line">\begin&#123;Bmatrix&#125; a&amp;b\\c&amp;d \end&#123;Bmatrix&#125; \quad</span><br><span class="line">\begin&#123;vmatrix&#125; a&amp;b\\c&amp;d \end&#123;vmatrix&#125; \quad</span><br><span class="line">\begin&#123;Vmatrix&#125; a&amp;b\\c&amp;d \end&#123;Vmatrix&#125; \]</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/02/14/uix67cYI4UaXvK9.jpg" alt="img"></p><h4 id="多行公式"><a href="#多行公式" class="headerlink" title="多行公式"></a>多行公式</h4><p>可以用 <code>aligned</code> 环境来实现，用 <code>&amp;</code> 实现位置对齐。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">\[</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">x = a+b+c+ \\</span><br><span class="line">+d+e+f+g+d+d+d+d+d+d+d+d+d+d+d+d \\</span><br><span class="line">+h+i \\</span><br><span class="line">+1 \\</span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">\]</span><br><span class="line"></span><br><span class="line">\[</span><br><span class="line">\begin&#123;aligned&#125;</span><br><span class="line">x &amp;= a+b+c+ \\</span><br><span class="line">&amp; +d+e+f+g+d+d+d+d+d+d+d+d+d+d+d+d \\</span><br><span class="line">&amp; +h+i \\</span><br><span class="line">&amp; +1 \\</span><br><span class="line">\end&#123;aligned&#125;</span><br><span class="line">\]</span><br></pre></td></tr></table></figure><p>效果：</p><p><img src="https://s2.loli.net/2022/02/14/oZugGY2MF78Lk9t.png" alt="image-20220214001008550"></p><p>若想要公式自带编号，可以用 <code>gather</code> 和 <code>align</code> 环境，其中 <code>gather</code> 环境将公式分行渲染，<code>align</code> 同上述 <code>aligned</code>，可以控制对齐：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;gather&#125;</span><br><span class="line">a = b+c+d \\</span><br><span class="line">x = y+z \\ </span><br><span class="line">p = a_1 + a_2 + a_3 + \dots + a_&#123;200&#125;</span><br><span class="line">\end&#123;gather&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;align&#125;</span><br><span class="line">a &amp;= b+c+d \\</span><br><span class="line">x &amp;= y+z \\</span><br><span class="line">p &amp;= a_1 + a_2 + a_3 + \dots + a_&#123;200&#125;</span><br><span class="line">\end&#123;align&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/02/14/5tZcTngyKSIsWxV.png" alt="image-20220214001355079"></p><p>若想使用分段函数，可以使用 <code>cases</code> 环境：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\[</span><br><span class="line">y= </span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">-x,\quad x\leq 0 \\</span><br><span class="line">x,\quad x&gt;0</span><br><span class="line">\end&#123;cases&#125; </span><br><span class="line">\]</span><br></pre></td></tr></table></figure><h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>数学公式是在引入了 <strong>amsmath</strong> 包之后，利用其提供的各种各样次环境来实现了较为复杂的公式的编辑。整体来说，与 Mathjax 的风格相差不大，因此迁移学习起来也十分方便。</p><p>这里我们再提供辅助工具：</p><ul><li><a href="https://mathpix.com/">https://mathpix.com/</a> 能够 OCR 手写体或是印刷体公式，而后将图片中的公式转换成 LaTeX 数学公式的代码。</li></ul><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p><code>tabular</code> 环境提供了最简单的表格功能。它用 <code>\hline</code> 命令表示横线，在列格式中用 <code>|</code> 表示竖线；用 <code>&amp;</code> 来分列，用 <code>\\</code> 来换行；每列可以采用居左、居中、居右等横向对齐方式，分别用 <code>l</code>、<code>c</code>、<code>r</code> 来表示。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;tabular&#125;&#123;|l|c|r|&#125;</span><br><span class="line"> \hline</span><br><span class="line">操作系统&amp; 发行版&amp; 编辑器\\</span><br><span class="line"> \hline</span><br><span class="line">Windows &amp; MikTeX &amp; TexMakerX \\</span><br><span class="line"> \hline</span><br><span class="line">Unix/Linux &amp; teTeX &amp; Kile \\</span><br><span class="line"> \hline</span><br><span class="line">Mac OS &amp; MacTeX &amp; TeXShop \\</span><br><span class="line"> \hline</span><br><span class="line">通用&amp; TeX Live &amp; TeXworks \\</span><br><span class="line"> \hline</span><br><span class="line">\end&#123;tabular&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/02/14/ytG5hReFbjWClwg.jpg" alt="img"></p><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p>在 LaTeX 中插入图片，有很多种方式。最好用的应当属利用 <code>graphicx</code> 宏包提供的 <code>\includegraphics</code> 命令。比如你在你的 TeX 源文件同目录下，有名为 <code>a.jpg</code> 的图片，你可以用这样的方式将它插入到输出文档中：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;</span><br><span class="line">\usepackage&#123;graphicx&#125;</span><br><span class="line">\begin&#123;document&#125;</span><br><span class="line">\includegraphics&#123;a.jpg&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure><p>想要了解更多，参见 <code>graphicx</code> 的文档：<a href="http://texdoc.net/texmf-dist/doc/latex/graphics/graphicx.pdf。">http://texdoc.net/texmf-dist/doc/latex/graphics/graphicx.pdf。</a></p><blockquote><p><strong>浮动体环境</strong></p><p>什么是浮动体环境：<code>table</code> 与 <code>figure</code>，两种浮动体环境可以替代上述的表格和图片环境，实现为表格或图片自动安排位置。</p><p>想了解更多有关浮动体环境的内容，详见<a href="https://liam.page/series/#LaTeX-中的浮动体">这里</a>。</p></blockquote><h3 id="页面设置"><a href="#页面设置" class="headerlink" title="页面设置"></a>页面设置</h3><h4 id="页边距"><a href="#页边距" class="headerlink" title="页边距"></a>页边距</h4><p>设置页边距，推荐使用 <code>geometry</code> 宏包。可以在<a href="http://texdoc.net/texmf-dist/doc/latex/geometry/geometry.pdf">这里</a>查看它的说明文档。</p><p>比如我希望，将纸张的长度设置为 20cm、宽度设置为 15cm、左边距 1cm、右边距 2cm、上边距 3cm、下边距 4cm，可以在导言区加上这样几行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;geometry&#125;</span><br><span class="line">\geometry&#123;papersize=&#123;20cm,15cm&#125;&#125;</span><br><span class="line">\geometry&#123;left=1cm,right=2cm,top=3cm,bottom=4cm&#125;</span><br></pre></td></tr></table></figure><h4 id="页眉页脚"><a href="#页眉页脚" class="headerlink" title="页眉页脚"></a>页眉页脚</h4><p>设置页眉页脚，推荐使用 <code>fancyhdr</code> 宏包。可以在<a href="http://texdoc.net/texmf-dist/doc/latex/fancyhdr/fancyhdr.pdf">这里</a>查看它的说明文档。</p><p>比如我希望，设置自定义页眉；页脚的正中写上页码；页眉和正文之间有一道宽为 0.4pt 的横线分割，可以在导言区加上如下几行：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;fancyhdr&#125;</span><br><span class="line">\pagestyle&#123;fancy&#125;</span><br><span class="line">\lhead&#123;页眉左侧&#125;</span><br><span class="line">\chead&#123;页眉中间&#125;</span><br><span class="line">\rhead&#123;页眉右侧&#125;</span><br><span class="line">\lfoot&#123;&#125;</span><br><span class="line">\cfoot&#123;\thepage&#125;</span><br><span class="line">\rfoot&#123;&#125;</span><br><span class="line">\renewcommand&#123;\headrulewidth&#125;&#123;0.4pt&#125;</span><br><span class="line">\renewcommand&#123;\headwidth&#125;&#123;\textwidth&#125;</span><br><span class="line">\renewcommand&#123;\footrulewidth&#125;&#123;0pt&#125;</span><br></pre></td></tr></table></figure><h4 id="段间距"><a href="#段间距" class="headerlink" title="段间距"></a>段间距</h4><p>我们可以通过修改长度 <code>\parskip</code> 的值来调整段间距。例如在导言区添加以下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\addtolength&#123;\parskip&#125;&#123;.4em&#125;</span><br></pre></td></tr></table></figure><p>则可以在原有的基础上，增加段间距 0.4em。如果需要减小段间距，只需将该数值改为负值即可。</p><h3 id="引用与尾注脚注"><a href="#引用与尾注脚注" class="headerlink" title="引用与尾注脚注"></a>引用与尾注脚注</h3><h4 id="交叉引用"><a href="#交叉引用" class="headerlink" title="交叉引用"></a>交叉引用</h4><p>交叉引用设置方法：</p><ul><li>给对象命名：<code>\label&#123;name&#125;</code></li><li>引用对象：<code>\ref&#123;name&#125;</code></li></ul><p>注意，在引用对象时，<code>\ref&#123;name&#125;</code> 会被替换会被引用对象的编号。举个例子，如果被引用对象在文档中是第 5 个被命名的，那么这里就会被替换为 5.</p><p>要想避免图/表/论文等等引用在计数上互相影响，你可以在命名时命名为 <code>tag:name</code> 的格式，引用时使用 <code>tag:name</code> 的格式来引用。具体来说，这些 tag 有：</p><div class="table-container"><table><thead><tr><th style="text-align:center">Tag</th><th style="text-align:center">Description</th></tr></thead><tbody><tr><td style="text-align:center"><strong><code>ch:</code></strong></td><td style="text-align:center">chapter</td></tr><tr><td style="text-align:center"><strong><code>sec:</code></strong></td><td style="text-align:center">section</td></tr><tr><td style="text-align:center"><strong><code>subsec:</code></strong></td><td style="text-align:center">subsection</td></tr><tr><td style="text-align:center"><strong><code>fig:</code></strong></td><td style="text-align:center">figure</td></tr><tr><td style="text-align:center"><strong><code>tab:</code></strong></td><td style="text-align:center">table</td></tr><tr><td style="text-align:center"><strong><code>eq:</code></strong></td><td style="text-align:center">equation</td></tr><tr><td style="text-align:center"><strong><code>lst:</code></strong></td><td style="text-align:center">code listing</td></tr><tr><td style="text-align:center"><strong><code>itm:</code></strong></td><td style="text-align:center">enumerated list item</td></tr><tr><td style="text-align:center"><strong><code>alg:</code></strong></td><td style="text-align:center">algorithm</td></tr><tr><td style="text-align:center"><strong><code>app:</code></strong></td><td style="text-align:center">appendix subsection</td></tr></tbody></table></div><h4 id="尾注脚注"><a href="#尾注脚注" class="headerlink" title="尾注脚注"></a>尾注脚注</h4><p>尾注直接在最后写就行，记得设置引用。</p><p>脚注可以使用 <code>\footnote&#123;角注内容&#125;</code> 来声明。</p><blockquote><p>想了解该如何更好地引入参考文献，请学习 BibTeX 宏包。</p><ul><li><a href="https://zh.wikipedia.org/wiki/BibTeX">https://zh.wikipedia.org/wiki/BibTeX</a></li></ul></blockquote><h3 id="列表与枚举"><a href="#列表与枚举" class="headerlink" title="列表与枚举"></a>列表与枚举</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">    \item \LaTeX&#123;&#125; 好 处 都 有 啥</span><br><span class="line">        \begin&#123;description&#125;</span><br><span class="line">            \item[好 用] 体 验 好 才 是 真 的 好</span><br><span class="line">            \item[好 看] 强 迫 症 的 福 音</span><br><span class="line">            \item[开 源] 众 人 拾 柴 火 焰 高</span><br><span class="line">        \end&#123;description&#125;</span><br><span class="line">    \item 还 有 呢?</span><br><span class="line">        \begin&#123;itemize&#125;</span><br><span class="line">            \item 好 处 1</span><br><span class="line">            \item 好 处 2</span><br><span class="line">    \end&#123;itemize&#125;</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/02/14/U3E8uCBy4RKxfSL.png" alt="image-20220214004408507"></p><h2 id="LaTeX-后续学习"><a href="#LaTeX-后续学习" class="headerlink" title="LaTeX 后续学习"></a>LaTeX 后续学习</h2><h3 id="更多宏包"><a href="#更多宏包" class="headerlink" title="更多宏包"></a>更多宏包</h3><p>宏包一般都会提供相应的文档供我们阅读使用。</p><p>这里提供查询宏包对应文档的网站：</p><ul><li><a href="https://texdoc.org/index.html">https://texdoc.org/index.html</a></li></ul><p><img src="https://s2.loli.net/2022/02/14/jkVbA8vBeY9FsUR.png" alt="image-20220214003834121"></p><h3 id="制作自己的模板"><a href="#制作自己的模板" class="headerlink" title="制作自己的模板"></a>制作自己的模板</h3><p>详见参考资料中 <code>.cls</code> 文件详解部分。我们同时推荐读者可以去多读一读其他已存在的 Template 的 <code>.cls</code> 内容。</p><h3 id="制作幻灯片"><a href="#制作幻灯片" class="headerlink" title="制作幻灯片"></a>制作幻灯片</h3><p>使用 Beamer 宏包可以制作幻灯片。详见：</p><ul><li><a href="https://www.overleaf.com/learn/latex/Beamer">https://www.overleaf.com/learn/latex/Beamer</a></li></ul><p>同时，校内也提供了一些适用于各种 pre 的 Beamer 模板。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><blockquote><p>（推荐）一份其实很短的 LaTeX 入门文档：<a href="https://liam.page/2014/09/08/latex-introduction/">https://liam.page/2014/09/08/latex-introduction/</a><br>（推荐）如何使用 LaTeX 排版论文：<a href="https://github.com/tuna/thulib-latex-talk">https://github.com/tuna/thulib-latex-talk</a><br>（<code>.cls</code> 文件详解）How to write a LaTeX class file and design your own CV： <a href="https://www.overleaf.com/learn/latex/How_to_write_a_LaTeX_class_file_and_design_your_own_CV_(Part_1">https://www.overleaf.com/learn/latex/How_to_write_a_LaTeX_class_file_and_design_your_own_CV_(Part_1</a>)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 技术/综合 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LaTeX </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《厌女》读书笔记</title>
      <link href="/2022/03/01/%E3%80%8A%E5%8E%8C%E5%A5%B3%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/03/01/%E3%80%8A%E5%8E%8C%E5%A5%B3%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>开始读《厌女：日本的女性嫌恶》（上野千鹤子著，王兰译），久闻此书大名，终于下定决心，也算是对自己学习能力的一个复健。读了几页，受益匪浅，遂决定做些记录。</p><h1 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h1><h2 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h2><p>作者提到“<strong>逃往女性，逃离女性</strong>”，对此的解释我深以为然。成年男性之所以急切地想要找寻一个女性结婚，是想要拥有一个所有物，或者说欲望发泄的对象。这里的欲望并不一定是性欲，而是包括了对女性的一切幻想，例如打理家务、相夫教子等。然而，“逃往”的终究只是对女性的幻想，在发现了现实中的女性不是这个样子之后，便开始“逃离女性”，也是“逃离家庭”。</p><p>仅以我父亲为例——在读了这一部分之后，我对他的行为有了一些了解。他单身时浪荡不羁，欠了一屁股债。随后，他与我母亲成婚，但两人相处并不愉快。显然，他期望的是一个贤惠、能干、还能赚钱的妻子，这样的形象只是他对女性的幻想。因此，他开始在外面寻觅刺激，不着家庭。他总是期望着能够找到一个满足他幻想的女性，可惜，这注定只是失败。</p><h2 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h2><p>作者提到“<strong>成为的欲望</strong>”和“<strong>拥有的欲望</strong>”。这个说法源自弗洛伊德对“生的欲动”的分解，即“自我确认/同化 （identification）”和“性欲发泄/欲望满足 （libido cathexis）”，用通俗易懂的话来讲，就是段首二词。</p><p>作者举例，一般顺性别异性恋者（以女性为例），会在家庭成长的过程中，逐渐与自己的母亲同化，即希望成为类似自己母亲的人，并且拥有男性——类似父亲，或者说母亲的丈夫的人。当然，这里说的“拥有”并不是真正主客体意味上的拥有，而只是一种简单的分类。</p><p>这里插入一点我多余的想法，部分后天跨性别的出现是否可能由于这种自我确认的失败。原定的同化目标（父亲或母亲）被认定不合格而转而寻求对另一方的同化，或者说更极端一点，对自己所见到的所有男性/女性都不满意。</p><p>根据作者的说法，如果混淆了“成为的欲望”和“拥有的欲望”，可能会导致同性恋，即对于原本希望“成为那样”的人，转变为了“想拥有的人”。如果按照这个思路的话，能否认为，后天跨性别是相反，即对于原本顺直人“想拥有”的人，转变为了“想成为”的人。这样来看的话。跨性别和同性恋还有一些可比较之处的。如果更进一步思考下去的话，天生的顺性别异性恋者，如果表达“想拥有的欲望”遭受到了阻碍的话，其就更有可能转变为“想成为的欲望”。这种阻碍可能是内生的，例如天生内向，不善表达；也有可能是外生的，例如在严厉的家庭表达受到制约。这可能也解释了为什么大多数的transfem更内向，以及同性恋相较顺性别人更多。（transmale我没有足够的观测资料）</p><p>男性社会将“客体化女性”，或者说“拥有女性”视为共识，亦即准入标准。<strong>男性同性社会性欲望</strong>指的是男性之间的竞争欲望，或者说是“想成为”的欲望。<strong>同性恋憎恶</strong>即“恐同”，是因为男性需要维护这个群体认同主体同盟的纯净性，不能允许其中混入“劣等”的“客体”。</p><blockquote><p>男人的同性社会性欲望，建立在厌女症的基础上，由同性恋憎恶来维系。</p></blockquote><h2 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h2><p><strong>东方主义</strong>指的是“关于何为东方的西方世界知识体系”，这里的“东方”指的是异国，换言之，是一群人对于另一个国度的幻想和希望。如前所述，男人对于女人的幻想也是这样一种。<strong>他者化</strong>则是将本应属于与自己同一类的物种（人类、社会等）当作不可理解的其他事物。不能理解的他者，既是充满魅力的诱惑，又是无法对自己产生影响的无力存在，这恰恰是男性对女性的暧昧态度。</p><p>人种与社会性别相同，也是建构的产物。白人为了强调自己的主体性，构造了黑人这个“劣等种族”，这与男性客体化女性的行为相同。同样，白人对黑人的研究很多时候也是对自己想象中的黑人。因此，无论怎么谈论平等，只要还存在主体与客体的区分，就不是平等。</p><p>分而治之（divide and rule）是支配统治的手段，将女性分为“圣女”与“娼妓”，前者被剥夺快感，异化为繁殖的手段，后者则只为快乐服务。这样的划分可以使男人在拥有并管制自己妻子的同时，可以在外找到娼妓。无论是地位还是人种的划分，都可以使被压迫者难以团结起来。因此，被当作“圣女”的女性，就可以合理歧视“娼妓”。然而在近代，这种分界线变得模糊了，无论是已婚女性或是中学生都可以参与到性交易中。</p><p>在我个人看来，尽管女性采取这种选择的结果依旧是被压迫与剥削，但是分界线的淡化事实上是一件好事。这意味着在未来，女性之间更容易产生共情。如果扩展开来，我们提出这样一个问题，人是否有“堕落”的自由，即人是否有自由主动选择变得“更差”。一种观点认为，如果自由不加限制，则会导致强者对弱者的剥削。这里举的例子可以是，如果允许女性自由地参与卖淫，那么一定会存在女性被强制卖淫，并收走钱款的情况出现。而如果不允许这样做，这就意味着女性很难打破阶层的隔阂，促进共情。似乎是一个两难的选择，但是所幸的是，我并非一个决策者。因此，我可以说，在现实中，我们还是应当对自由加以一定的限制——毕竟国家就是建立在暴力的基础之上，完全的自由只会在乌托邦中存在。反之，在理想的世界中，人们则拥有几乎完全的自由，对自己的一切行为负责。</p><h2 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h2><p>这一章在论述相对弱者的男性（即在婚恋市场中处于下风的男性）对自由婚恋的控诉。在互联网上。不乏这样的陈述，他们认为，自由婚恋让“强者”掠夺了性资源，自己处了下风。这种言论目前主要被攻击的点在于，其将女性视为性资源。他们对此不自知吗？我觉得不是的。有些没有说出来的人，他们只是顾虑这样会有损于自己正人君子的形象，而另一些就直接大言不惭了。绝大多数男性之所以理所当然地将女性视为性资源，因为他们“起反应”的目标根本不是作为人的女性，而只是一个解决自己欲望的符号，然后又将这个自己幻想中的符号投射到了现实女性上。</p><p>而如果继续看自由婚恋市场的状态，可以发现，魅力资源，并不如很多人所想，都是学历地位职业收入决定的。身居高位却在恋爱上屡屡碰壁的男性屡见不鲜，而在社会资源上不算领先的男性也有在情场如鱼得水的时候。</p><blockquote><p>其实，魅力资源不是由“交换价值”决定的，而是由只对消费者当事人有用的“使用价值”来测量的。性与恋爱，终究还是人与人的关系。“性的市场”的“规则放松”，意味着要求男人们也应该具备与人沟通交流的技能。</p></blockquote><p>许多被称为“性弱者”的男人，由于其缺乏与现实女性的接触，因此幻想与现实是脱节的。由于外貌是最天生的，大部分弱者会将自己无法得到女性青睐的原因归咎到外貌，而不是自己的交流能力或者学历收入之类的可以后天习得的能力。作者提出了很有意思的一个观点，一直以来，女性都是被教育如果没有男人要你你什么都不是，现在也有男性需要面对这样的的情况了，那也确实可以称得上是男女平等上的进步了。为什么男性对拥有女朋友这件事情有着如此的执念呢？因为只有拥有了一个女性，这才会被男性群体承认和接纳，才会成为男人，如前一章所述。</p><p>为什么人际交流能力现在被当做一个重要的能力提出来了呢？我个人认为，在几十年前，女性在很多场合的声音是被抹消的。尽管在共和国成立之后，妇女的地位得到了极大的提升，但是至少以我在互连网上的经历为例，在十多年前，互联网上充斥着下流段子、生殖器笑话，还有“春哥铁血真男人”。这样的东西如果在今天大行其道的话，必定是会被女性反抗的。所以有些男性认为的Good Old Days，事实上是因为女性被噤声。在从前的社会，只需要维护男性之间的人际关系，如果表现好的话自然就会有女性送上门来。现在如果想要找到女朋友的话，与女性的沟通交流是必不可少的，这一点变化容易让男性感到不悦。</p><h2 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h2><blockquote><p>性欲,是在个人内部完结的存在于大脑之中的现象。正如全美性教育信息协议会（SIECUS）所下的定义：性欲（sexuality）不是存在于“两腿之间（between the legs）”，而是存在于“两耳之间（between the ears）”，即大脑之中。所以，性欲研究（sexuality studies）其实不是关于下半身的研究。</p></blockquote><p>作者提出，性欲、性行为、性关系三个概念需要被分离看待。欲望是在个体之内完成的，与“论迹不论心”类似，无论一个人的性欲是什么，是guro或者恋童，或者作者所称的儿童性侵犯，都是与他人无关的。</p><p>性行为分为有伴侣的和无伴侣的，即与他人身体之间发生的，和自慰的区别。只要性行为涉及到了他者，那么就是一个社会关系，这个社会关系即成为性关系，这是与自慰本质的不同。既然存在社会关系，那么也就需要遵从社会关系的基本原则，也就是法律。所以，即使是在夫妻之中，强奸罪与性骚扰也是可以成立的。现在的法律，尤其是中国的，也许受到传统思想的影响，倾向于将家庭视为一个私有领域。但是从哲学的角度来说，一个人私有的只有其自己。</p><h1 id="摘抄"><a href="#摘抄" class="headerlink" title="摘抄"></a>摘抄</h1><h2 id="喜欢女人的男人的厌女症"><a href="#喜欢女人的男人的厌女症" class="headerlink" title="喜欢女人的男人的厌女症"></a>喜欢女人的男人的厌女症</h2><p><strong>misogyny</strong>，译为“厌女症”，在男性身上表现为“女性蔑视”，在女性身上则表现为“自我厌恶”，本质上都是对女性的物化，因此厌女症不代表男性不会对女性产生兴趣。恰恰相反，很多自称“性豪”的男性喜欢以发生过关系的女性数量多为荣，这表明，男性起反应的不是女人，而是女性符号，如若不然，是不可能把全体女人都纳入“女人”的范围内的。</p><p>这个所谓的“女性符号”可以通过对恋物癖的解释来进行说明。譬如超短裙，无论穿在谁的身上，哪怕是穿在男性的身上，还是会有人对超短裙发情，这个时候发情的对象表现为超短裙这个符号。恋物癖就是一种通过换喻关系置换欲望对象的符号操作，这个例子可以说明女性在现代社会已经被高度物化，以至于哪怕是女性身上的片断符号都足以让男性发生反应，而这种反应不是本能，而是高度的文化产物。而这也可以说明，在当今时代的女装男，其实也是一种男性恋物癖的产物。</p><p>下面解释一下喜欢女性的厌女症男性，好色的厌女症男性都会喜欢娼妓，因为他们并不把女性当作人，而是享受那种支配的快感。然而在这种支配的背面，是他们的潜意识里认为，男性的主体化必须要依赖他者女性来完成，就如同奴隶主的地位要通过奴隶的服从来彰显一样，每一次想要证明自己是男性时，都不得不依赖女性这种低贱卑微的动物来满足欲望，而男性对这个悖论事实的厌恶，就是厌女症。</p><p>而男性也一定期待过不需要依赖女性便能够彰显自身主体性的世界，因此就诞生了同性恋。这里的同性恋指的是刻意美化男性形象的同性恋，这就是男性试图构建的一个新世界。</p><p>女性被物化的直接后果就是，女性本身成为了男性确认自身名声、权力和地位的工具，被当作发泄的垃圾场。娼妓本身就是为了男性发泄类似愤怒的感情而存在的，这里是矛盾存在的地方，当男性把情绪发泄到女性身上的时候，是一种对女性的物化，男性或许是会有罪恶感的，然而如果女性表现得很享受，男性就不必背负罪恶感了，然而又会感叹“女人真是妖怪魔物”，用将女性妖魔化的方法，同样地将女性他者化了。总之，男性并不在乎女性是否得到快感，事实也确实如此，《新·摩尔报告》指出六成以上女人假装过性高潮。</p><p>色情文学的铁定规则是：</p><p>第一，女人是诱惑者；第二，女人最后一定被快感支配。深究这种结构，无非是为男性免去罪责：反正都是女性主动的，最后女性也获得了快感，那更没有什么罪恶感了，甚至可能还会有成就感。这也就解释了为什么色情文学中的终点不是男人的快感而是女人的快感。因为女人的快感可以成为测定男人性能力达成度的指标，也是男人对女人的性支配得以完成的地点。而这种作品中对女性的固化描写，会成为男性要求女性，或是女性不自觉地自我要求的目标：如果不能成为这样获得快感的女性，就意味着自己不够成熟。</p><p>近代男性文学中的，作为恋物癖符号的女人，是构成男人内心世界的私人空间。男人为逃避公共世界而寻向“女人”这个空间，然而真实的女人却是令人不快的他者，于是又从那里再次尝试逃离，这种逃离可以是逃离家庭，也可以是逃往家庭，在逃离之后，发现的是不能满足他们梦想的另一个他者，于是又再次逃离。男作家们在女人身上寄托他们的梦想，随心所欲地解释女人，而他们描绘的女人往往又是和真实世界有差异的。</p><p>男人将女人他者化，其实是把女人变为可以控制的他者，而非真实存在的，自由而不可控的他者，在他们的世界里，这样的他者既可以崇拜，又可以肆意羞辱。而这种他者化并不对等，女性作家早早从男人的现实中觉醒过来，于是她们并不逃往男人，而是她们自身。近代女性文学的特征是男性幻想的稀薄。</p><h2 id="男性同性社会性欲望·同性恋憎恶·厌女症"><a href="#男性同性社会性欲望·同性恋憎恶·厌女症" class="headerlink" title="男性同性社会性欲望·同性恋憎恶·厌女症"></a>男性同性社会性欲望·同性恋憎恶·厌女症</h2><p>女人的价值由男人的选择而定，然而男人的价值不是由女人的选择决定的，而是在男人世界里的霸权争斗中决定的，而女人在这种争斗中，往往作为争斗胜者的奖品。只有女人凭自身能力获得地位财富名誉后，男人才会在意女人的评价。反过来，女人世界里的霸权争斗，一定会有男人的评价介入，而男人的评价标准与女人评价女人的标准是不一致的。</p><p>为与homosexual区分，将不带性爱关系的男人之间的纽带称为男性同性社会性欲望，更准确的说法是“压抑了性存在的男人之间的纽带”。</p><p>弗洛伊德把“生的欲动”分为自我确认和性欲发泄，前者译为“同化”，后者译为“欲望满足”，或者说是“成为的欲望”和“拥有的欲望”。在一个家庭中，孩子渴望与父亲同化、拥有母亲（替代者）的人，就成为男人，反之则成为女人。因为母亲已被拥有，而寻找替代者为妻的人，就是异性恋男人。发现母亲与自己同样没有男性生殖器而渴望父亲的生殖器，寻找儿子来作为替代品，从而实现与母亲的同化的人，就是异性恋女人。当然，这个理论并没有讨论除男女之外的可能。</p><p>即，只有将成为的欲望和拥有的欲望分别投向异性双亲的人，才能成为异性恋。而同性恋者，就是这两种性欲望的分化失败了的人，将成为的欲望和拥有的欲望都指向男人的人，就成为男同性恋者。</p><p>然而，成为的欲望和拥有的欲望，常常是重叠的，因此男性同性社会性欲望中常常包含有同性恋欲望，两者是连续体。这是一件危险的事，因为成为的欲望是通过同化成为性的主题；拥有的欲望则是将欲望指向对象而将其作为性的客体，因此不能把同化对象的他者（主体）也作为性欲望的对象（客体）。</p><p>男人的历史，就可以理解为调整这两种欲望之间的关系而受苦的历史。在古希腊，异性恋对于有责任的自由民男人而言是义务，女性被看作工具，而少年爱即同性恋才是他们的权利。这种关系是不对称的，因为在插入式性交中，插入者与被插入者是一种单向的关系，插入者是性的主体，被插入者则是客体。如果自由民少年基于自由意志被插入，便被视为最高价值的性爱，奴隶没有自由选择的权利，价值就低了一等。虽然这种所谓自由意志也是引导的结果。</p><p>被插入、被得到、成为性的客体，可以被描述为“被女性化”，这是男性最恐惧的事情，即性的主体地位的失落。因此，男人的同性社会性欲望的纽带，就是相互认可的性的主体者之间的纽带。而在这个主体者世界里，如果出现了同性恋欲望，就可能相互沦为性的客体，这就会引发阶层的混淆。</p><p>由于同性恋和同性社会性欲望本身就是连续的，因此要切分开来并予以排斥更加困难。男性排斥不具有男人价值的人时，使用的词汇就是“同性恋”，将他们看作“像女人的男人”，男人对潜伏在自己集团中同性恋的恐惧，也就是对自己也许会被当作性的客体的恐惧，这就是恐同。</p><p>总结一下，男人的同性社会性欲望，建立在厌女症的基础上，由同性恋憎恶来维系。确认男人主体性的机制就是将女人客体化，使得性的主体者相互认可，因此拥有女人，就是成为性的主体的条件。</p><p>反对以上理论的论据通常是一些第三性，他们是一群被阉割或是通过女装女性化的男性，他们的存在意义就是向男性提供服务。然而在这样的现实中，只有男人才能转为“第三性”，反过来证明了性别二元制的强固。现代社会中的MtF和FtM也能佐证这一点。MtF被反对的理由通常是自愿客体化因此受到轻贱，FtM被反对的理由则是原本作为客体的他者试图成为主体，这是在挑战男人同性社会性欲望的秩序。</p><p>基于以上的理论，可以去解释一些事实。比如，战争中公开的轮奸就是男人之间增进连带感的仪式，女性在其中是祭品。另一个例子是男性之间的下流话，就是将女性当作性的客体来贬低和凌辱，然而这也格式化了男性谈论性的方式，至于未经格式化的个体经验，其语言表达一直是被压抑的。</p><p>社会学学者佐藤裕在《论歧视》中指出，“歧视需要三个人”，即：歧视就是通过将一个人他者化而与共同行动的另一个人同化的行为。如果把前者换为女人，后者换为男人，则是性歧视。比如男人A对男人C说，“女人脑子里是怎么想的，真是搞不懂”，这时女人B是否在场根本不重要，而是他想要寻求C的认同，构成“我们男人”的集体认同。排除是一种共同行为，如果C同意，则歧视达成，如果C反对，则会被攻击“你还是个男人吗”，在性别二元制下，脱离了男人世界，就等于被女性化的男人。</p><p>所谓的“男人话语”，就是一种强迫听者与固定格式同化，寻求同化后的集体认同感的话语方式。男人的性主体化需要的是认可自己为男人的男性集团，拉康所谓“欲望乃他者之欲”，指的是人们将自己渴望与之同化的对象所欲之物视为自己的欲望对象，通过这种方式使自己也立于同化对象的“欲望主体”的位置。男人是通过模仿其他男人的性欲望而成为性主体的，所以成为男人的途径是单一的。下流话作为一种固定格式，只能依赖于他者的认可存在，而不可能成为“我”这个主体的话语体系。男人之间比较勃起能力和射精次数，正是他们之中的一元化尺度。这种性的贫瘠表明，男人的性主体化本身，就是一种排除了偏离和多样性的固定格式。</p><ol><li><p><strong>如何理解主体化</strong></p></li><li><p><strong>同性恋到底是男性所排斥的还是刻意建构的</strong></p><p>所谓“不需要女性便能达成自身主体化”这句话的意思其实是，男性集团欢迎的是在同性恋中作为主体地位的自己，用耽美作品的语言来讲就是偏向于攻的一方，而第二章所说的恐同指的是害怕被客体化，这里偏重的是对受的一方的恐惧。比如早期中国会有那种，认为嫖男娼很正常，不能说这个人是同性恋，但是男娼本身会被认为是同性恋、离经叛道的思维方式。</p><p>上野似乎认为同性关系中也必然有作为主体的一方和作为客体的一方，但是同性关系本身也可以作为一种突破主客体二分的解读。这种思维也可以用来解释耽美作品对于男性支配的打破或者巩固。</p></li></ol><h2 id="性的双重标准和对女性的分离支配"><a href="#性的双重标准和对女性的分离支配" class="headerlink" title="性的双重标准和对女性的分离支配"></a>性的双重标准和对女性的分离支配</h2><p>把对方当作不可理解之物，如异人/异物/异教徒，将之从“我们”中放逐出去的方法，即是“他者化”，有人种化与性别化两种。“东方主义”就是西方人对于东方的幻想，普契尼的歌剧《东方女人》就是典型的东方主义视野中的东方女性形象，因为是不可理解的他者，因此可以肆意塑造一个能够满足西方男人自尊心的女人形象。</p><p>最近的人种研究已然证实了，人种是历史建构的产物，然而偏要创造人种的概念，把人区分开来。就如同性别是为了排除非男人（包括没能成为男人的男人）来维持男性的主体化一样，人种就是为了排除非白人而定义何为白人的装置。</p><p>厌女症是男性为了成为性主体而将对女性的蔑视作为自我确认的核心，同性恋憎恶则是对男女界限的模糊而带来的不安，男人们必须持续证明自己不是“像女人一样的男人”。而为了使厌女症不至于指向自己的母亲而带来对自己出身的精神危机，厌女症拥有了崇拜女人的另一个侧面，这就导向了性的双重标准。</p><p>性的双重标准，指的是对男人的性道德和对女人的性道德不一样。男人的好色被肯定，而女人则以无知纯洁为善。而这是不对等的，男人的好色无法在纯洁的女性身上发泄，因此需要另外的“犯规对象”。</p><p>结果就是，女人被分为圣女和荡妇、母亲与娼妓、结婚对象与玩弄对象。用于生殖的女人被剥夺了快乐，仅仅为了生殖；为了快乐的女人专为快乐服务而远离生殖，这也就是为什么带着孩子的娼妓让男人扫兴。</p><p>一个典型的例子是慰安妇，“慰安”当然是慰男人之安，对女性则是地狱般的劳动。在军中的从军护士则被树立起“圣女”的形象，正因如此，她们也开始歧视慰安妇，不愿意自己被当作性对象。当临死的士兵要求看一眼护士的胸部时，有些人答应了，但也决不愿将它看作性骚扰。由于这种女性的娼妓歧视，护士甚至不愿意承认自己受到了性侵犯。女性也开始接受男性的双重标准后，就从内部被分化了，被当作圣女的群体开始成为娼妓歧视的帮凶，浑然不知自己其实也是被物化为为男人的快乐服务的工具。比如穿上白色围裙的“国防妇人会”，就甘愿成为军国体制的帮凶，甚至帮忙监视士兵妻子的贞操。成为娼妓的女子则嘲笑于其他女性无法依赖自己生活，必须依靠男人的软弱。</p><p>但这个双重标准似乎也让男人困扰，如果对特定的女人认真，就不能看作性对象，反之亦然。因此男性的很多言行不一其实是一致的，对青楼女子当作性玩具，对未来的妻子则当作女性，按照平权思想来尊重。男性的婚姻是存在利益交换的，因此正妻需要门第家产与持家生子的功能，至于性的发泄则可以交给妾。</p><h2 id="“无人气男”的厌女症"><a href="#“无人气男”的厌女症" class="headerlink" title="“无人气男”的厌女症"></a>“无人气男”的厌女症</h2><p>男人不是被女人选上而成为男人的，而是在男人集团中被承认而成为男人的，女人只是加入的资格条件或是成为成员后的奖励。因此即使学历收入等社会条件都很优越，却连一个女人也弄不到手，这种男人的价值就会降低，也就不具有成为男人集团一员的资格。这就是处男比处女更难启齿的原因。</p><p>对雄性“败犬”而言，他们要求的女人并没有什么苛刻的标准，只要给男人面子就行，这说明他们所需要的是女性的符号而不是女性的属性，这也就是女性无论老幼美丑都有可能遭到强奸的原因之一。对他们而言，女性的存在和讨好仅仅只是为了满足他们的自尊心。“女高男低婚”也是如此，对于女方条件远在男方之上的婚姻，女性如不讨好男性是无法维持的，因为男性会用侮辱和打骂的方式维持自己的骄傲——这么高贵的女人随我打骂，还不会离开我。</p><p>正因如此，很多女性在求偶时会要求男性的收入，这可能不是因为她喜欢有钱的男人，而是经验表明，对年收入六百万的女人，没有一千万收入，男性面子就无法维持。男女关系的平衡，无论如何终究要让男人居上位，才能勉强维持。</p><h2 id="儿童性侵犯者的厌女症"><a href="#儿童性侵犯者的厌女症" class="headerlink" title="儿童性侵犯者的厌女症"></a>儿童性侵犯者的厌女症</h2><p>首先要区分几个概念。</p><p>性爱，指的是性+爱，性是欲望，爱是关系，既有伴随爱的性，也有不伴随爱甚至带有恨和侮辱的性。</p><p>性欲，是在个人内部完结的，存在于大脑之间的现象。性的反应，有时需要实物，有时只需要一些符号，有时需要加上幻想才能引发。即使这种幻想是对恋爱关系的想象，但欲望是在内部完结的，所以“我爱你与你毫无关系”是可以成立的。欲望是自由的。</p><p>性行为，是将欲望付诸行动。一种需要他者，一种则不需要。前者是性关系，后者则是“关系欠缺”的性行为，即自慰。只要有他者介入，无论何种性行为都是社会关系的一种，具有公开的性质，因此社会规则在这里是适用的，它不再自由。只有不具有他者的性行为才是私人的。</p><p>因此，色情作品仅仅只是私人的，是想象，是理论，仅使用过滤分级保护人不看的自由即可，对公共领域的实践才需要限制。然而，当色情制品或作品指向的是具体的现实的对象的时候，它又是应当被禁止的。“鬼畜系”色情漫画的消费者是通过体验加害者和被害者的落差，来体味到双重的快感，不仅与加害者同化，还与被害者的痛苦同化时，快感则变得更加复杂。</p><p>儿童性侵者为满足一己之欲，利用可以不征得同意的、无力反抗的孩子的身体，并长久控制对方，摧毁对方的自尊心，还希望对方是自己情愿，把对方当作诱惑者。</p><p>这就回到之前对于古希腊“少年爱”的论述上。之前已经讲过，男性的恐同是缘于对被客体化的憎恶，而对男性而言，有一种不需要失去主体化而可以实践同性恋的方式，就是“少年爱”，在这里，这种非对称关系被固定下来，年长者总是主体，不会因为少年的凝视而成为欲望客体。在古希腊的一些作品中也可以发现，少年大多出于尊敬和爱戴而奉献出自身，而极少自身感到快乐。</p><p>因此，儿童性侵者的内心活动也是这样，对他们而言，无论对方是男是女，儿童是最不具有反抗能力，最容易被客体化的对象。这也显示出这群人的脆弱——为了成为男性，必须取得在性中的主体地位。</p><h2 id="皇室的厌女症"><a href="#皇室的厌女症" class="headerlink" title="皇室的厌女症"></a>皇室的厌女症</h2><p>在出生之时，人的价值便依性别而定，没有比这更明显的厌女症了。以厌女症为核心机制的社会，就被称为父权制。大家都愿意在父权制下选择生男孩。而在20世纪80年代的日本，大多数人选择想要女孩，这并非因为平权观念深入人心，而是由于男孩的教育成本过高，加之对高龄化社会的不安使得人们期待生女儿来照顾老人，这些原因共同造成了孩子从“生产资源”成为了“消费资源”。</p><p>皇室中依然存在着这样的歧视，像是皇室的女子与普通人结婚后，便不具备皇籍，而皇室男子则可以继续保有。</p><h2 id="春宫画的厌女症"><a href="#春宫画的厌女症" class="headerlink" title="春宫画的厌女症"></a>春宫画的厌女症</h2><p>“女人寻求关系，男人寻求占有。”</p><p>对女性而言，杀死自己概率最高的人不是陌生人，而是丈夫或恋人。男性会因无法复合而杀人，因为不希望这个女人被别的男人占有，杀人是占有的最高形式。然而女人的嫉妒并非指向男人，而是指向夺取这个男人的女人。对男人而言，女人的背叛是对他所有权的侵犯，建立在占有女人基础上的自我会面临崩溃；对女人而言，嫉妒是以其他女人为对手围绕男人展开的竞争。</p><p>男人拥有的资源，按原始性程度排序，为暴力、权力和财力。暴力是最原始的，而暴力和权力一样都是不安定的，财力相比而言更安定，通用性也更好。这三者广义上说，是社会性的条件，所以其实也可以包括外貌这种需要社会承认的条件，而不是表面看来的身体自然条件。在男性的霸权争斗中，女人就被作为资源分配给他们，女人“发情”的对象，是男性在集团中的位置，而不是他本身，这种情欲具有文化性和社会性。</p><p>而体力地位金钱都买不到的，最强有力的资源，就是通过快乐来进行支配。无论在社会上处于多么弱势的地位，只要能在性上支配女人，便可以扭转其他一切负面因素。在色情作品中这一点体现得非常充分。</p><p>男人们愿意相信，男性性器是男人的快乐之源，女人的快乐也不可缺之。拉康将解剖意义上的男性性器转换为拉丁语phallus，视为语言中的象征支配而使之更加普遍化。既非暴力，也不是权力与财力支配，而是“性力”支配，而且让支配方自发服从，不通过强迫而是快乐让人自发地服从，这才是终极的、最安定的支配。这也就解释了古希腊与奴隶发生关系是不如少年爱高级的，因为前者是基于权力，后者则是基于快乐。</p><p>对男人而言，消费色情制品是一种仪式，其核心是让他们在被剥夺了一切社会属性后还能获取支配和占有的地位，在这里男性性器即是快乐之源。</p><p>随着知识体制的变化，近代以前的色情（ero）被近代之后的性（sexuality）取待，因此可以说“性的近代”，而不能说“近代的性”，因为不存在近代以前的性，这是福柯“性的历史”的核心之一。</p><p>春宫画的特征其一是男女性器尺寸的极端夸张和精密写实，其二是不论男女，即使其他部位被简略和格式化，但面部的愉悦却表现得很清晰，这是一种男人幻想的投影。而到了后期，强奸、紧缚开始出现，女性的面部也开始出现痛苦，错乱的重口味色情趣味越来越多，从对女性“快乐的支配”到“恐怖的支配”，是一个文化洗练程度下降的过程。如何解释这种现象呢？</p><p>之前已经说过，色情制品的基本设定就是女人是诱惑者，男人无需负责，女人因为寻求快乐而诱惑，并在服从中得到回报。因此一开始的快乐表情可以理解为，男人愿意相信她们从性行为中得到了快乐。此外色情作品中还有一种特殊的图标，被称为巴宾斯基反射，它指的是女性性高潮时手指足趾发生弯曲的身体反应，春宫画中有一种冷静地拉开距离观察女性反应的视线，这是沉溺于快乐的人不可能有的。在这样的游戏，特别是嫖客和娼妓的游戏中，沉溺于快乐的人就是输家。女人若将性行为置于金钱之外，嫖客成为情人，那么嫖客就“输了”，适度停止，才能取得支配权。</p><p>在插入者与被插入者的不对称关系上，被插入者被象征性地阉割——即女性化，并无快乐可言。男根被置于快乐的中心地位，即使这是男人的幻想。有这样一种图式，一个年轻女孩偷看其他男女的性交，一边进行自慰，由此暗示：“正常”的性交是插入式性交，自慰只是得不到正常性交的替代行为，有对象的插入式性交才是终点。而这种女性自慰能给男人带来性刺激的原因就是这里的男根缺失，而男性可以把自己的男根代入那个位置。女同性恋色情作品也是这样，假阳具的使用就是一种男根缺失，而女性试图寻找替代品，这种痴态能够激发男性的欲望。</p><p>春宫画的表象就是象征性的男根支配的定型化。所表达的不是作为身体部分的性器官，而是占据男人性幻想核心的男根符号。这种崇拜甚至达到了恋物癖程度。</p><p>当然，其实也存在相当部分以女性为消费对象的作品。在阴道高潮与阴蒂高潮的争论中，弗洛伊德认为未经历过阴道高潮的女性是不成熟的，也就是在要求插入式性交。然而我们至少可以说，这二者是同等重要的，阴道高潮并不是必需的。</p><h2 id="近代的厌女症"><a href="#近代的厌女症" class="headerlink" title="近代的厌女症"></a>近代的厌女症</h2><p>对母亲的最大侮辱是“娼妇”，即在父权制社会中没有登记注册的女人。父权制决定了女性和孩子的归属，不在男人的支配和控制下生出来的孩子是不能在社会中登记的。在各种表象中“娼妇”“未婚母亲”被妖魔化，描述为放浪的女人，即不服从男人控制，在性方面自由使用自己身体的女人。然而这些人或者为了钱，或者因为本应成为父亲的男性的不负责任而成为未婚母亲，加害者不过一贯地将责任转接到被害者身上。</p><p>对女人而言，厌女症是对自身的厌恶。例如，很多女性经常因对自己身体和外形不满意、嫌恶而感到焦虑。</p><p>厌女症不同于性别歧视，性别歧视是强调不平等，它是基于厌女症的，厌女症强调的是社会文化对女性的桎梏和压迫。它是性别歧视的核心。有的厌女症并不基于性别歧视，比如上面提到的身体焦虑，这种对女性身体的外在凝视和自我审查并不是因为人们觉得“男性天生就比女性美”，而是因为人们将女性的身体默认为可以被强加意志的物体。</p><p>这种厌恶是根植于语言体系的，从每一个近代的女性出生起就被她们所接受的东西。人在成为女人的时候，要先将“女人”这个范畴所背负的历史性的厌女症接受下来。女人不是生来而是变成女人的，而是通过接受“女人的范畴”成为女人的。语言世界先于个体而存在，每个人都只能降生于那个先已存在的语言世界中，语言不是她自己的东西，个体从他者那里接受“你是女人”的指名。如果顺应这个位置，女人就诞生了。而不能适应这个位置，对厌女症感到不满的人就是女性主义者。因此女性主义者是与自身的厌女症斗争的人，没有厌女症的女人没有必要成为女性主义者。</p><p>将强制的范畴改为选择，解放的关键就在其中。</p><h2 id="母亲与女儿的厌女症"><a href="#母亲与女儿的厌女症" class="headerlink" title="母亲与女儿的厌女症"></a>母亲与女儿的厌女症</h2><p>母亲所受的痛苦和付出的代价，希望在孩子身上得到补偿。如果是儿子，那办法很简单，出人头地即可，而女儿，在近代之前，早晚要成为别人家的人，没法期待投资的回报，而现在女儿对娘家的义务并不能免除。在近代，女人开始拥有接受高等教育的机会，拥有自己获取价值的机会。然而这让女性的处境更加艰难，因为此时女儿要回应母亲的双重期待，既要做好儿子，也要做好女儿。她们的选择扩大，反而让她们更加不自由，因为她们既要能做到像男人一样好，却又不能在地位上压过男人。</p><p>因此，在近代的女性成了母亲梦想的蓝图，母亲将压力和责任放到女儿身上，女儿则厌恶这一切。然而若要摆脱母亲的束缚，只能依靠另一个男人——类似自己的父亲一样的男人。</p><p>而母亲对女儿的态度也是复杂的，一方面母亲希望女儿出人头地回应她的期待，一方面却又把女儿视作竞争对手（对男性就不会有这种竞争意识），对于女儿的成功，母亲很难为自己的失意找到借口。因此，对女儿而言，只实现“自己获取的价值”是不够的，母亲之所以成为母亲，是因为她实现了被男人选上的价值，如果女儿没有得到男人给予的价值，母亲也不会承认。而如果女儿将这两个价值都实现，则更显得母亲心里不是滋味，女儿越幸福，母亲就越心情复杂，还伴随着女儿被别的男人夺走的丧失感。</p><p>母亲是孩子最初的爱恋对象，但女孩子却不能通过与父亲的同化将母亲作为欲望的对象。女孩子不但不能和母亲恋爱，也不能与和母亲一样性别的女性恋爱。这种爱恋对象的丧失根植于女性，为了忘却那种丧失，女孩子将“丧失对象”内化于身体之中，这就是“忧伤”。即抑郁状态。忧伤就是对所爱对象的忘却，因此女性性质本身就是抑郁的。这也可以由女性要素大多是“娴静”“节谨”来验证。</p><p>无论是回应母亲的期待还是背叛期待，只要母亲还活着，女儿就逃脱不掉母亲的束缚，母亲一直支配着女儿的人生，直到死后，女儿对母亲的怨恨表现为自责和自我厌恶。因为母亲既是压迫者，又是牺牲者。女儿厌恶不能爱上母亲的自己，对于女儿，厌女症总是包括母亲在内的自我厌恶。而解决办法只能是，双方相互告诉对方，“我不是你。”</p><h2 id="“父亲的女儿”的厌女症"><a href="#“父亲的女儿”的厌女症" class="headerlink" title="“父亲的女儿”的厌女症"></a>“父亲的女儿”的厌女症</h2><p>在一段婚姻中，妇女无时无刻不在受到来自男性的压迫和暴力，这种两性关系实际是一种不对称的权力关系。女儿目睹着这一切，但女儿拥有和母亲不一样的特权：她可以选择拒绝成为母亲那样，然后挤入父母中间，争得父亲的宠爱，然后共同压迫母亲。女儿要得到父权制社会的接纳，就必须承认一切不公正。选择母亲的语言意味着死亡，选择父亲的语言意味着被阉割，意味着压抑母亲要求公平的话语，而女儿选择了后者。就这样，女儿成为了“父亲的女儿”。</p><p>在弗洛伊德的理论里，儿子因对母亲的欲望而被父亲阉割，而女儿是早已被阉割过的，是“已被阉割过的儿子”。</p><p>然而父亲与女儿的关系不仅仅只是支配与服从，儿子与父亲存在竞争的关系，而女儿却成为父亲的诱惑者，对父亲而言，女儿是伴随禁忌的、充满魅惑的对象，女儿既属于自己又绝不能触碰。妻子是不可理解的他者，在父权制下，妻子属于旁系。女儿却带有自己的克隆，是自己亲手养大的，是“至高的恋人”，比起妻子，更愿意和女儿一起享受无上的幸福。这被称为“洛丽塔情结”或“皮格马利翁情结”。这种诱惑并非女儿自带的，而是男性自己建构的。</p><p>父亲的支配欲、权力欲和卑贱，会显露在女儿面前。屈服于女儿的诱惑，却又用谎言掩盖自己的肉欲就是卑贱的体现，因此从父亲屈服于自己建构的诱惑起，女儿就拥有了蔑视父亲的充分理由。这时，父亲仅仅只是侵犯者。因为父亲想侵犯女儿，女儿得到了蔑视父亲的理由，通过成为牺牲者，女儿获得凌驾于父亲之上的依据。这种机制也可以反过来，即：为了侮辱父亲，利用女儿自身的“诱惑者”的权力。这是女儿主动产生的欲望。</p><p>这种对父亲的侮辱，在援交少女身上也能看到，他们把嫖客视作父亲的代理人，把原来的身体供奉给男人，将属于父亲而父亲又不能玷污的身体让这些男人随意玷污，这样完成对于父亲的复仇。当然，这种自我伤害的方式反而体现了这些女儿们作为绝对弱者无路可走的绝望。这被称作“自我身份认同扩散综合症”。</p><p>“父亲的女儿”会再次生下“父亲的女儿”，继续忍受着自我厌恶和性别压抑，只要母亲依旧充当父权制的代理人，女儿与母亲的关系就不可能和谐；如果母亲忠实于自己的欲望，又会面临父权制的制裁。如前所述，女儿面临的是死亡与被阉割的二选一，要摆脱这种恶性循环，需要的是同时拒绝这两个选项。女人要放弃“母亲”“女儿”的角色，放弃父权制给女人的指定席位，母亲的解放和女儿的解放密不可分。</p><p><strong>儿子和女儿的阉割要怎么理解</strong></p><h2 id="女校文化和厌女症"><a href="#女校文化和厌女症" class="headerlink" title="女校文化和厌女症"></a>女校文化和厌女症</h2><p>男校文化，指的是男性的话语体系，也包含其附属的异性恋文化。女校文化，则是女性摆脱男性视线的世界，只有女性的世界。</p><p>女校文化的环境是有利于培养女性的领袖才能和积极性的，其中的女性也更为独立自主，而男女同校中的女性则更有“女人味”，懂得迎合男性的审美。然而女校也并非真空地带，由于女校的周围同样是男性的世界，因此女校学生也明白放学时换上裙子。同时，女校内部也存在围绕“女人味”的扭曲的霸权争斗。</p><p>女校文化有双重标准，被男人接受的标准和被女人接受的标准是不一样的。在男人世界里，价值标准是一元的，男人喜欢钱权，女人也一样喜欢。在女校文化下，男人喜欢的女人和女人喜欢的女人是不一样的。前者是女人羡慕的对象，后者则因为不符合男性凝视，反而因为不受男人喜欢而让女人感到安心。</p><p>在女校中，学业分数和女性分数往往不一致。女性分数高的女生，周围不会期待她的学业，因为她已经有资本在社会中生存。反之，女性分数匮乏的女生，则被期待用学业分数去弥补“女人味”的不足。女性分数高的女生就会去嘲笑其他女性分数低的女生“丑女”，得意于自身由男人赋予的价值。因此，貌似反抗学校文化的性早熟的少女，却成为了男人世界里的性客体。</p><p>而拥有“男子气”的女生，会逗笑别人的女生，这样的人“被女人接受的分数”很高，因此在女校中受女生的欢迎，甚至可能被奉为英雄。然而进入社会后，这些人会在异性恋制度下经历自我身份认同的危机，被女人喜欢的人是得不到男人的喜欢的。</p><p>正因为女人不喜欢受男人欢迎的女人，在女校文化中，这些女生会刻意把自己扮演得笨拙滑稽、惹人发笑，只有这样，才能让其他女性感到安全，从而在女校文化里生存下来。</p><p>女校文化的表演家就如同“变装皇后”一样，通过过度追求“女人味”的滑稽表演，让人发笑的同时，也解构了性别，使得人们在嘲笑滑稽的“女人味”的同时，意识到性别的虚构性，顺便也嘲弄了对这种虚构性发情的男性欲望。</p><p>结合之前说的，男性的“恋物癖”，可以发现，对于那些男性作为发情对象的女装男性或是跨性别者，这种行为将女性符号安排到一个非女性个体身上，反而将男性的恋物癖和高度物化女性的文化体系展示得淋漓尽致。</p><p>而若将其用于滑稽而夸张的表演，譬如长相不那么好的男性来女装，则显露出追求女性化这一行为本身的可笑之处。在这个时候，这种女装行为和前一种不一样，是不会有男性市场的，因此它的受众会转变为女性。这个“让女性意识到这种行为是可笑的”的机理是这么表述的：女性意识到的事情是，追求女性化的过程，本质上是追求女性符号的过程，而如果女性看得到符号背后的社会效力，就能够意识到这些符号是被父权制刻意建构出来的。</p><p>这两种行为都可以被解读为解构，然而也有观点认为这种行为将跨性别和男性一同客体化，收编到父权制中去。因此，解构这件事有没有效力，取决于社会的认知水平。如果社会对解构的认知流于表面，那可能它反而变成建构。比如如果社会意识不到男人发情的对象其实是女性符号，而还是觉得是对女性本身发情的话，那这件事反而成了terf口中的“加固刻板印象”。</p><p>所以最可怕的其实是这个社会，在把许多解构的行为变成建构，它把一切反对它的力量收纳进自己的势力范围中，让这个社会所有的反对的声音，都变成加固刻板印象的一块砖瓦。</p><h2 id="“东电女职员”的厌女症"><a href="#“东电女职员”的厌女症" class="headerlink" title="“东电女职员”的厌女症"></a>“东电女职员”的厌女症</h2><p>佐野真一的《东电女职员被杀事件》，主要内容包含两个部分，一部分是东电女职员卖娼事件，另一部分是杀人事件。前半部分探究一位精英白领女性走向卖娼的心理动机，后半部分追踪杀人犯——尼泊尔男性的背景和审判过程。</p><p>毕业于一流大学，供职于一六公司的高学历女性，没有任何经济困境，却主动选择街娼这种最底层的工作。佐野在解释她的行为时，用的词是“堕落”，这是基于一种长久以来的旧式思维，即女人出卖自己的性是有悖人伦的行为。</p><p>斋藤提出如下的弗洛伊德式精神学分析：敬爱父亲、也被父亲期待的长女成为“父亲的女儿”。大学时代失去父亲的长女于父亲同化，试图代替父亲的地位，却遭到母亲的蔑视和疏远，母亲溺爱妹妹，排斥姐姐。由于她试图代替父亲的位置，她开始憎恶自己女性的身体，对身体产生一种类似复仇的情绪。她清楚母亲对父亲的依赖，建立在母亲自我性欲望的压抑上。每天卖娼后，晚上还回到母亲和妹妹的家中，这是凸显母亲社会性无能的极好方式。自我惩罚，也就是对母亲的惩罚。</p><p>而若是被父亲支配，憎恶父亲的女儿，也需要通过卖娼来玷污本应属于父亲的身体。不论是自罚还是他罚，最终都只能通过自伤来完成。因为女儿的地位最低，她只能去惩罚更弱者——也就是她的身体。相反，儿子的攻击就表现为更单纯的对他者的伤害。</p><p>如上一章所述，现代女性面临的困境是，必须要同时实现个人的价值和男人赋予的价值，如果不能同时实现，就会被认为是失败的，东电女职员的例子便是，即使个人价值能够实现，可实现不了男人赋予的价值，就只能通过出卖自己，主动被客体化来试图获取男人的认同，得到男人的价值。而女校文化中又说到，被女人认可的价值和被男人认可的价值又是不能同时实现的。这便是现代女性面临的困局。</p><ol><li><p><strong>个人的价值和被女人认可的价值有什么联系吗</strong></p><p>个人的价值更偏向于一种社会性的，大家都可以认可的价值，比如成绩，比如事业。</p></li></ol><p>卖娼的反面是买娼，男人在为女人开价的同时，也是对自己的性欲标了同样的价钱，是对为了性欲的满足不得不依赖女人到了如此地步的男人的一种嘲笑。妻子的性是无偿的，而对娼妓而言，再卑贱的性也绝不能是无偿的。对于把自己的性不断降价的女人，男人一边轻蔑，一边把她们圣化为“拯救男人的女菩萨”，即使女人没有一点拯救的意愿，男人也会找这样的借口，把自己的负罪感投射到她们身上，这是男人在性上的双重标准——将女人分为生殖的和用来快乐的——给他们自己造成的困扰。</p><p>如果女性给自己不断降价，就成为了一个单纯的性器官，不管她们有多难堪，男人可以闭上眼睛想别的女人，或者抱着虐待的心态，总之都会付钱给她。这就如同慰安妇，被剥夺了一切人格，只剩下性器官。</p><p>女人给自己标多少价格，其实也就是说，在她们眼里男人的性欲也就值那个价格。不要钱的女人，等于把自己和男人的性欲一起扔进下水道里。而要钱的女人则是宣告：没有钱，你连自己的性欲都满足不了，这是女人对男人的复仇。当然，男人也憎恶女人，一边把女人还原为性器官，一边又不得不依赖这样的女人满足性欲——这就是男性厌女症的矛盾之处——曾在第一部分论述过的。从这个角度说，女性拿钱的行为反而彰显了自己的独立性——我的身体只属于我自己，只有在这点时间之内，才能让别人随意摆弄。</p><p>职业娼妓给自己带上故事，是为了增加性的附加价值。而名流通过显示“我只对带有附加价值的女人发情”，向别的男人展示自己的性欲和那些不花钱的性欲不一样，自己的性欲也是高级的。因此男人的性欲需要女人来标价，男人会花大把钱来养妻子，这是在夸耀，满足我性欲的女人需要花很多钱来修缮，那笔钱同时也衡量着丈夫的地位。而女人也一样，女人会大把消费，来显示自己并非把自己贱卖的女人，女人也通过男人来确认自己的价值。</p><p>通过这个也可以理解少女援交的原理，少女在社会上“被禁止触碰的身体”或是名门女校的身份，本就是巨大的附加价值，因此动机之一是“想趁着自己还值钱捞一笔”，动机之二则是，在学校和家庭得不到认可的少女，希望通过男人确认自己的价值。</p><p>如果没有男人不择手段这个条件，性产业就不会成立，男人必须抹去女性的个体差异，仅仅对着超短裙、裸体甚至性器官这样的女性符号便能发情，这并非动物的本能，而是文化产物，买娼的男人买的是女人这个符号，在这里其实没有他者的存在，所以买娼就是自慰行为的一种。而女人呢？女人把自己作为他人所属的物品出售，通过成为物体，她们解构对物体发情的男人，将其解构为单纯的性欲。通过自主成为男人性欲的对象，女人将男人还原为单纯的性欲或是性器官，正如男人所做的一样，女人“舍身”对男人进行复仇，而在这个过程中确认自己的主体性，这就是东电女职员上瘾的原因——她体会到了复仇的胜利感。男人憎恶娼妓，娼妓轻蔑嫖客。</p><p>在父权制的压迫下，女人被定义为“能激起男人性欲望的人”，女人的存在价值就是成为性欲的对象，思春期的定义即是“意识到身体并非属于自己，而是被他人观看、成为他人快乐道具的时期”，与年龄无关。当女人不再是男人的欲望对象时，女人就不是女人了。</p><p>之前我们说，女人的价值分为个人的和男人给予的，可很多时候这两个价值又是统一的，作为“父亲的女儿”，希望出人头地，作为女人，渴望被男人看上，这两种价值的认可者都是男性。</p><h2 id="女人的厌女症-厌女症的女人"><a href="#女人的厌女症-厌女症的女人" class="headerlink" title="女人的厌女症/厌女症的女人"></a>女人的厌女症/厌女症的女人</h2><p>厌女症对女人而言就是自我厌恶，但其实也有方法摆脱自我厌恶，方法就是把自己当做女人中的例外，把其他女人他者化。有两种策略，一种是成为特权精英，被男人当做“名誉男人”的女强人策略；一种是主动退出女人的范畴，逃脱被估价的女人，即丑女策略。通过站在例外的位置，站在了产出厌女症的父权制一方，并协助这种体制的强化和再生产。而因为她们成为了局外人，所以可以安心地把“真女人”的内幕毫不留情地抖露出来。</p><ol><li><p><strong>既然所有的女人都是以男人为归属，相互为潜在的竞争对手，女人之间的友情，男女之间的友情为什么得以成立？</strong></p><p>女人是以男性为归属，这是男权社会的结构性问题，但这不代表所有女性都会彼此敌视，比如恩格斯有提到无产阶级之间就存在纯粹的性爱。</p></li></ol><h2 id="权力的色情化"><a href="#权力的色情化" class="headerlink" title="权力的色情化"></a>权力的色情化</h2><p>福柯列举了四项近代之后的“性欲望的装置”：</p><p>一、 儿童的性教育化，指儿童的性成为管理对象，尤其指对手淫行为的禁止成为儿童教育规训的一项内容。</p><p>二、 女性身体的歇斯底里化，指女性的身体被视为性身体，对性欲的压抑被视为导致“神经病女人”的原因。</p><p>三、 性欲倒错的精神病理化，指除异性间性器官接触以外的多种性爱方式被视为错乱反常的快乐，通过精神病理学将之视为异常。性欲倒错包括同性恋，同性恋在中世纪被视为道德上的越轨，在近代则被视为精神医学上的病理现象，成为治疗矫正的对象。</p><p>四、 生殖行为的社会化，指夫妻以异性恋配偶为正统，作为生殖单位被置于社会的管理控制之下。</p><p>经由这四项对性的管理，达到社会对个人生命的管理，“生命权力”（bio-power）由此形成。四类人群成为控制的对象：“手淫的儿童”“歇斯底里的女人”“反常性欲者”“马尔萨斯主义的夫妻”（马尔萨斯——生育控制之意）。</p><p>正统异性恋夫妻被置于特权地位，这带来两种变化，原本存在婚姻内外的性，被限定在夫妻之间，另一种变化是，在夫妻关系中，性爱原非不必可少，但现在却被置于核心位置，即“性家庭”。</p><p>在近代以前，性关系对夫妻并非必须，正妻无子，可以认领，也可让小妾代生，妻子生的孩子不管父亲是谁，甚至在冥婚中丈夫已死，生下的孩子都记为丈夫的孩子。此时的丈夫是孩子的“社会性父亲”，“生物学父亲”是谁则不计较，婚姻仅为决定孩子归属的亲族关系的规则。（可以参考恩格斯《家庭、私有制和国家的起源》中拿破仑时代法典的内容，这是淫游制泛滥的结果）夫妻之间的“性交义务”，在近代并未写进法律，但却有一条，夫妻离婚时，对方不接受性交可以成为正当理由，这种现象被称为“夫妻关系的性化”。</p><p>权力对性的控制，通过对快乐的管理来达成，即“权力的色情化”，福柯称为“权力的感官化”，这里的感官指的就是“色情感官”，福柯认为，快乐与权力二者是相互追逐、重叠、强化的。这也就是之前提到的，基于快乐的支配模式。性是资产阶级为了区分于贵族和劳动阶级而产生的。</p><p>在四个压抑的背后，存在一个“性的隐私化”机制，把性逐出公共领域，圈入私人领域即家庭之中，家庭成为充满性意味的空间，当然，隐私化并不意味着压抑性，而是使之特权化，并与个人人格相结合。当性不能被公共领域和公权力干涉时，对强者而言就不受牵制，对弱者而言就不受保护，产生了父权支配的真空地带，“家庭的黑洞”。家庭成为“性家庭”，婚姻成为性行为的许可证，初夜是性关系的开始，夫妻没有性关系被病理化……</p><p>在之前，夫妻的契约仅仅保证性行为的权利与义务，而没有性满足的权利与义务，因此生育是目的，伴随的快乐则应当减少，可能怀孕的性器官接触得到奖励，而不能怀孕的肛交、口交、前戏被压抑禁止，甚至视为背叛上帝的行为。而在近代的性观念中，却包含夫妻关系的色情化，即性满足的权利与义务，强调夫妻之间的快乐和性满足，丈夫将快乐教给妻子并将快乐的模式刻印在她身上，使妻子不可能再从其他男人那里得到快乐，希望自己变成唯一的男人（本子常见剧情了）。</p><p>快乐取代权力，成为了终极的男性支配。权力的色情化，不应该理解为色情取代了权力，而是权力以色情的形式出现了，或是色情以权力的形式出现了。</p><p>权力的色情化，指的是支配以性爱的方式进行，色情的权力化，则指的是有人用暴力和支配的形式表达性爱，性与暴力的共同之处是，两者皆为卸下防卫装置，过度接触对方身体，二者的快感也可以是共通的，业已说明，施虐者可以与受虐者同化，获得双重快感。但虐待的快乐与性别结合起来后，角色就相对固定了，男人为施虐者，女人为受虐者，二者都以此为快乐。男人的性是能动的，女人的性是被动的。</p><p>性是可以被历史化的，也就是“去自然化”，不再将其视为“自然的”，由某些攻击性或是激素决定的。性从暴力到爱恋，形式上是多变的，跨度很大，因此不存在“本质”的性现象。我们探究的就是特定历史条件下，与性优先结合的某个特定物，即什么东西最容易和性结合。色情和性别关系本来没有结合的必要性，古希腊的色情就存在于同性之间，中世纪末期之后夫妻关系被特权化，直到近代一夫一妻制才将夫妻关系和性别关系联系起来，资产阶级的婚姻规范就是“性快乐的权利与义务”。</p><p>男性对女性“保护”的许诺，其实就是一种权力的色情化，这时“爱”成为私人领域的事情，“保护”不过是“所有”的另一种表达，却被视为爱的象征，男人的敌人是一切外部的其他男人，女人则心甘情愿接受支配，并从这种支配中逐渐习惯，享受快乐的支配。这时，女性的爱表现为服从与被拥有，这就是近代家庭中女性“照料照顾”的角色，只能用这种方式去表达爱，如果女性选择不做，则会被谴责为不是一个好妻子。</p><p>厌女症与恐同，都是权力的色情化，我们要做的就是将权力与色情分离。将权力送回原本的位置，让色情不再单一，不受权力约束的色情可以更多样化，因为它不带有控制的目的，可以更加自由。</p><p>文化不像公权力一样具有明面上的强制力，然而其潜移默化的改变则更加可怕，不知不觉中大众已经习惯了这一种控制的手段，就像吸毒的人忘记健康状况是什么感觉一样。可是那毕竟是在变化，也能改变的。改变习惯并非易事，但意识到那并非宿命，只是习惯，总是好的。</p><ol><li><p><strong>怎么理解“性是资产阶级为了区分于贵族和劳动阶级而产生的”？</strong></p><p>可以结合前述“对自己性欲的估价”来理解，男人通过给性欲附加价值，来显示自身的价值和地位，来制造阶级差分。</p></li></ol><h2 id="厌女症能够被超越吗"><a href="#厌女症能够被超越吗" class="headerlink" title="厌女症能够被超越吗"></a>厌女症能够被超越吗</h2><p>让女人“成为女人”的，是男人；让男人“成为男人”的，是男人，或者说是男人集团，女人在其中是“成为男人”的道具，或是作为“成为男人”的奖励。这里的区别是，男人集团有权力评价女人的价值，因此男人会拼命证明自己的女人的价值是更高的（如前所述的“附加价值”），来获得男人集团更大的认可。反之，不符合女人定义即“能激发男人性欲望”的女人，在男人集团的价值评价里地位就更低。在成为男人的过程中，男人集团是有主体性的，而女性是被客体化的，尽管二者在这个过程中都是必要的。所以同性恋尽管有爱情，但并不被男人集团承认。</p><p>女人，就是对“非男人的人”的标注，男性把除自己以外的群体，包括不被男人集团承认的男人，都排斥为“非男人”。异性恋秩序使男人只能以非男人作为性欲对象，如果对象是男人，就被女性化为“女人一样的男人”，因此女人也就与“属于男人”的美德与名誉区分开来，被赋予“软弱”“不坚强”这类适合于男人支配对象的属性。</p><p>位于异性恋秩序根基的欲望三角形，不是由复数的男女组成的，而是由两个欲望主体（男人）和一个欲望客体（女人）组成的。列维-斯特劳斯的“婚姻交换”理论并不把婚姻定义为一对男女的结合，而是“女性在复数亲族集团之间的移动”，是通过女人的交换建立起来的两个男人（集团）之间的纽带。异性恋秩序的核心是男性同性社会性欲望和厌女症，同时伴随同性恋憎恶。</p><p>只要权力关系的不对称存在，女性之间的女性集团即使存在，也极其弱小。因为双方同化所能得到的权力资源多寡是一目了然的，与女性集团同化远不如与男性集团同化。这也可以解释为什么女性的嫉妒并不指向背叛的男人，而是另一个女人。</p><p>男人常常会喜欢诸如好友的恋人、老师的妻子、领主的夫人等等，中世纪骑士恋爱的对象往往是已婚的贵妇人，骑士道恋爱的一个功能，往往是通过崇拜同一个女性，使骑士团这个男性共同体的纽带得以维系和强化。当然，男人之间有时会由这个而起争执，但那是男性集团内部由于争夺“资源”即被物化的女性而起的权力争执，这并不会使男人集团分裂，因为无论谁胜，失败的那一方都不会倒向女人，而是继续试图通过女人来达成自己主体化的意图。</p><p>超越厌女症的世界，就好像马克思所谓废除了阶级的世界一样，现在的我们无法想象，只能在不断的实践中给出方向。超越厌女症有两条路径，一条是女人的，一条是男人的。</p><p>对于女人，女性主义是与自己和解之途。不可否认，女性主义者生活在父权制社会中，难免会受到厌女症的影响，不如说不受到厌女症影响的女性几乎不存在，而女性主义者正是察觉到自身的厌女症而坚决与之斗争的人。如果有完全不受影响的女人而要与厌女症斗争，那女性主义就不再是为了自我解放，而成为一种局外人强加给社会的正义了，其结果必然是以多数人对少数人的压制告终。</p><p>而男性的路径是与自我厌恶的斗争。男性也存在自我厌恶，主要有两点，一者为自我否认，一者为身体蔑视。所谓身体蔑视，是指男人对自己身体的诅咒。男人锤炼身体、损伤身体，是因为他们把身体他者化了，他们被迫要显示自己是身体的主人，即主体=自我。与精神相比，身体处于劣位，因此性欲作为身体的欲望被视为肮脏的，而这种欲望又只能通过更劣等的女人才能满足，男人对身体的诅咒就会越来越深。这种诅咒也表现为“去身体化”，这种欲望有时表现为与女人身体的同化，比如女装趣味，或许就是向理想的身体同化的渴望，但不是性别越界的渴望。</p><p>当然，这种自我厌恶要与另一种区分开来，即“不够男人”的厌恶，不同于“身为男人”的厌恶，这种厌恶是由于性能力弱、不受女人喜欢、无业等问题，导致的偏离男性集团的自我厌恶，找不到自己的位置，被男性集团排挤，走向孤立。这种恐惧女性也有，来源于对身体的不自信，因此女性会去减肥、化妆、治疗不孕等。</p><p>男人的自我厌恶，来自被他者化了的身体的报复，要超越厌女症，就要停止对身体的他者化，停止成为身体及其身体性的支配者，即精神=主体。停止将于身体相关的性、怀孕、生育视为“女人领域”。正面面对自己的欲望，不要贬低以身体为媒介的亲密。如果我们接受了身体的他者性，就能接受通过身体关联的他人的存在，既不把他人视为支配控制的对象，也不视其为威胁的源泉，而是完整地接受。</p><p>女性主义否定的是“男性特性”，而非个体的男性存在，被分类为男性的人们如果希望得到完整的肯定，就必须和厌女症斗争。</p><ol><li><p><strong>怎么理解“男性把身体他者化了”？</strong></p><p>结合后文要求男性“接受自己的身体欲望”可以看出，这里的“把身体他者化”指的应该是将身体视为自己不可理解的客体，是用来客体化的，因此不承认身体的欲望属于自己，也就不承认性欲在自己这里的合理性。而又为了满足性欲而去客体化女性，甚至去嫖娼，而把这种性欲及其所导致的行为迁怒于女性或是社会。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> 日志/读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
            <tag> 社会科学 </tag>
            
            <tag> 女性主义 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>（2-9,4-9）图像处理与学术研究 课程笔记</title>
      <link href="/2022/02/24/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6/"/>
      <url>/2022/02/24/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E4%B8%8E%E5%AD%A6%E6%9C%AF%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h2 id="图像处理与学术研究"><a href="#图像处理与学术研究" class="headerlink" title="图像处理与学术研究"></a>图像处理与学术研究</h2><h3 id="Chapter-1-图像处理基础"><a href="#Chapter-1-图像处理基础" class="headerlink" title="Chapter 1 图像处理基础"></a><strong>Chapter 1</strong> 图像处理基础</h3><h4 id="1-1-概念"><a href="#1-1-概念" class="headerlink" title="1.1 概念"></a>1.1 概念</h4><ul><li><p><strong>图像</strong>是视觉信息的重要表现方式，是对客观事物相似的、生动的描述，是光能量和人类大脑相结合的产物。</p></li><li><p><strong>数字图像处理(Digital Image Processing)</strong>是利用计算机对图像进行<strong>去除噪声、增强、复原</strong><br><strong>分割、提取特征等的理论、方法和技术</strong>，是信号处理的子类，相关理论涉及通信、计算机、电子、数学、物理等多个方面，已成为一门发展迅速的综合性学科。</p></li></ul><h4 id="1-2-图像函数"><a href="#1-2-图像函数" class="headerlink" title="1.2 图像函数"></a>1.2 图像函数</h4><ul><li><p><code>A = imread(&#39;filename&#39;);</code></p><p>从 <code>filename</code> 指定的文件读取图像，并从文件内容推断出其格式。如果 <code>filename</code> 为多图像文件，则 <code>imread</code> 读取该文件中的第一个图像。下图为图像的返回结果。</p></li></ul><ul><li><p><code>imshow(&#39;filename&#39;);</code></p><p>在图窗中显示灰度图像 <code>I</code>。<code>imshow</code> 使用图像数据类型的默认显示范围，并优化图窗、坐标区和图像对象属性以便显示图像。</p></li><li><p><code>imwrite(A,filename);</code></p><p>将图像数据 <code>A</code> 写入 <code>filename</code> 指定的文件，并从扩展名推断出文件格式。<code>imwrite</code> 在当前文件夹中创建新文件。输出图像的位深取决于 <code>A</code> 的数据类型和文件格式。对于大多数格式来说：</p><ul><li>如果 <code>A</code> 属于数据类型 <code>uint8</code>，则 <code>imwrite</code> 输出 8 位值。</li><li>如果 <code>A</code> 属于数据类型 <code>uint16</code> 且输出文件格式支持 16 位数据（JPEG、PNG 和 TIFF），则 <code>imwrite</code> 将输出 16 位的值。如果输出文件格式不支持 16 位数据，则 <code>imwrite</code> 返回错误。</li><li>如果 <code>A</code> 是灰度图像或者属于数据类型 <code>double</code> 或 <code>single</code> 的 RGB 彩色图像，则 <code>imwrite</code> 假设动态范围是 [0,1]，并在将其作为 8 位值写入文件之前自动按 255 缩放数据。如果 <code>A</code> 中的数据是 <code>single</code>，则在将其写入 GIF 或 TIFF 文件之前将 <code>A</code> 转换为 <code>double</code>。</li><li>如果 <code>A</code> 属于 <code>logical</code> 数据类型，则 <code>imwrite</code> 会假定数据为二值图像并将数据写入位深为 1 的文件（如果格式允许）。BMP、PNG 或 TIFF 格式以输入数组形式接受二值图像。</li></ul><p>如果 <code>A</code> 包含索引图像数据，则应另外指定 <code>map</code> 输入参数。</p></li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%example</span></span><br><span class="line">I_rgb = imread(<span class="string">&quot;1.jpg&quot;</span>);</span><br><span class="line">imwrite(I_rgb,<span class="string">&#x27;2.jpg&#x27;</span>);</span><br></pre></td></tr></table></figure><ul><li><p><code>I = rgb2gray(&#39;filename&#39;);</code></p><p><code>rgb2gray</code> 函数通过消除色调和饱和度信息，同时保留亮度，来将 RGB 图像转换为灰度图。</p></li></ul><ul><li><p><code>th = graythresh(I);</code></p><p> 使用 Otsu 方法 [1] 根据灰度图像 <code>I</code> 计算全局阈值 <code>T</code>。</p><blockquote><p>[1] Otsu, N., “A Threshold Selection Method from Gray-Level Histograms.” IEEE Transactions on Systems, Man, and Cybernetics. Vol. 9, No. 1, 1979, pp. 62–66.</p><p>Otsu 方法选择一个阈值，使阈值化的黑白像素的类内方差最小化。全局阈值 <code>T</code> 可与 <code>imbinarize</code>结合使用以将灰度图像转换为二值图像。</p></blockquote></li></ul><ul><li><p><code>BW = im2bw(I,level);</code></p><p>基于阈值将图像转换为二值图像 <code>BW(Black and White)</code> 。方法是将输入图像中亮度大于 <code>level</code> 的所有像素替换为值 <code>1</code>（白色），将所有其他像素替换为值 <code>0</code>（黑色）。</p></li></ul><ul><li><p><code>subplot(m,n,p)</code></p><p>将当前图窗划分为 <code>m</code>×<code>n</code> 网格，并在 <code>p</code> 指定的位置放置图像。MATLAB 按行号对子图位置进行编号。第一个子图是第一行的第一列，第二个子图是第一行的第二列，依此类推。</p></li></ul><h4 id="1-3-图像分类"><a href="#1-3-图像分类" class="headerlink" title="1.3 图像分类"></a>1.3 图像分类</h4><ul><li><p><strong>灰色图像</strong></p><p>灰色图像使用<code>灰度级</code>来表示像素点的明暗程度，数值是介于 0 到 255 之间的整数。</p></li><li><p><strong>黑白图像</strong></p><p>黑色图像的二维矩阵中每一个点，呈现黑色<code>0</code>或白色<code>1</code>。</p></li><li><p><strong>彩色图像</strong></p></li></ul><p><img src="https://img-blog.csdnimg.cn/20190711231129454.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hbmh1YWliZWlhbg==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>  彩色图像又叫 RGB 图像，由三个<code>m</code>×<code>n</code>的二维矩阵构成，矩阵中每一个点的值是介于0 到 255 之间的整数，从前到后依次是红分量、绿分量和蓝分量。</p><ul><li><strong>彩色图像转换到灰度图像</strong></li></ul><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">I_rgb = imread(<span class="string">&#x27;filename_1&#x27;</span>);</span><br><span class="line">I_r = I_rgb(:,:,<span class="number">1</span>);<span class="comment">% R</span></span><br><span class="line">I_g = I_rgb(:,:,<span class="number">2</span>); <span class="comment">% G</span></span><br><span class="line">I_b = I_rgb(:,:,<span class="number">3</span>);<span class="comment">% B</span></span><br><span class="line">I_GRAY = <span class="number">0.299</span>*I_r + <span class="number">0.587</span>*I_g + <span class="number">0.114</span>*I_b;</span><br><span class="line">imshow(I_GRAY);</span><br><span class="line">imwrite(I_GRAY,<span class="string">&#x27;filename_2&#x27;</span>);</span><br></pre></td></tr></table></figure><p>  <code>I_GRAY = 0.2989*I_r + 0.5870*I_g + 0.1140*I_b;</code>三个系数和为一，心理学家发现取这三个系数更加符合人的感官。</p><p>  故灰度图像到彩色图像、黑白图像到灰度图像的过程是不可逆的，过程丢失了关键的数据。</p><h4 id="1-4-图像大小"><a href="#1-4-图像大小" class="headerlink" title="1.4 图像大小"></a>1.4 图像大小</h4><ul><li><p>数据单位</p><ul><li>1<strong>byte</strong> （字节） = 8<strong>bit</strong>（比特/位）</li><li>对于黑白图像存储每一个像素点需要1 bit，对于灰度图像这个大小是1 byte = 8 bit，而对于彩色图像这个值是3 byte。</li></ul></li><li>分辨率<ul><li>图像总像素的多少，如 640 × 480 = 307200，代表30万像素分辨率。</li><li>拍摄分辨率：<code>4K</code>:4096 × 2160 像素，<code>1080p</code>:1920 × 1080 像素</li></ul></li><li>图像压缩：JPG是压缩后的图像数据，BMP是未压缩的。</li></ul><h4 id="1-5-图像格式"><a href="#1-5-图像格式" class="headerlink" title="1.5 图像格式"></a>1.5 图像格式</h4><ul><li><p><strong>BMP</strong></p><p><strong>BMP</strong>取自位图Bitmap的缩写，也称为<strong>DIB</strong>（与<a href="https://zh.wikipedia.org/wiki/设备">设备</a>无关的<strong>位图</strong>），是一种独立于<a href="https://zh.wikipedia.org/wiki/显示器">显示器</a>的<a href="https://zh.wikipedia.org/wiki/位图">位图</a><a href="https://zh.wikipedia.org/wiki/数字图像">数字图像</a>文件格式。BMP文件通常是不<a href="https://zh.wikipedia.org/wiki/图像压缩">压缩</a>的，所以它们通常比同一幅图像的压缩图像文件格式要大很多。例如，一个800×600的24位几乎占据1.4<a href="https://zh.wikipedia.org/wiki/百萬位元組">MB</a>空间。因此它们通常不适合在<a href="https://zh.wikipedia.org/wiki/因特网">因特网</a>或者其他低速或者有容量限制的<a href="https://zh.wikipedia.org/wiki/媒介">介质</a>上进行传输。</p><p>图像通常保存的<a href="https://zh.wikipedia.org/wiki/颜色深度">颜色深度</a>有2（1bit）、16（4bit）、256（8bit）、65536（16bit）和1670万（24bit）种颜色（其中bit是表示每点所用的数据位）。</p></li><li><p><strong>GIF</strong></p><p><strong>图像互换格式</strong>（英语：Graphics Interchange Format，简称<strong>GIF</strong>）是一种<a href="https://zh.wikipedia.org/wiki/位图">位图</a><a href="https://zh.wikipedia.org/wiki/图形文件格式">图形文件格式</a>，以8bit色深（即256种颜色）重现<a href="https://zh.wikipedia.org/wiki/真彩色">真彩色</a>的图像。它实际上是一种<a href="https://zh.wikipedia.org/wiki/数据压缩">压缩</a>文档，采用<a href="https://zh.wikipedia.org/wiki/LZW">LZW</a>压缩算法进行编码，压缩率在50%左右，有效地减少了图像文件在网络上传输的时间。它是目前<a href="https://zh.wikipedia.org/wiki/全球資訊網">万维网</a>广泛应用的网络传输图像格式之一。</p></li><li><p><strong>JEPG</strong></p><p><strong>JPEG</strong>或称<strong>JPG</strong>，是一种针对照片影像而广泛使用的<a href="https://zh.wikipedia.org/wiki/有损数据压缩">有损压缩</a>标准方法，以牺牲一部分图像数据达到较高的压缩率，由<strong>联合图像专家小组</strong>（英语：<strong>J</strong>oint <strong>P</strong>hotographic <strong>E</strong>xperts <strong>G</strong>roup）开发。</p><p>由于JPEG优良的品质，被广泛应用于互联网和数码相机领域，网站上80%的图像都采用了.JPEG压缩标准。</p></li><li><p><strong>TIFF</strong></p><p><strong>标签图像文件格式</strong>（<strong>Tagged Image File Format</strong>，简写为<strong>TIFF</strong>）是一种灵活的<a href="https://zh.wikipedia.org/wiki/栅格图像">位图</a>格式，主要用来存储包括照片和艺术图在内的图像。</p><p>TIFF最初的设计目的是为了1980年代中期桌面扫描仪厂商达成一个公用的扫描图像文件格式，而不是每个厂商使用自己专有的格式。在刚开始的时候，TIFF只是一个二值图像格式，因为当时的桌面扫描仪只能处理这种格式。随着扫描仪的功能愈来愈强大，并且桌面计算机的磁盘空间越来越大，TIFF逐渐支持灰阶图像和彩色图像。</p></li><li><p><strong>PNG</strong></p><p><strong>便携式网络图形</strong>（英语：<strong>P</strong>ortable <strong>N</strong>etwork <strong>G</strong>raphics，<strong>PNG</strong>）是一种支持从LZ77派生的<a href="https://zh.wikipedia.org/wiki/无损压缩">无损压缩</a>算法的<a href="https://zh.wikipedia.org/wiki/位图">位图</a>图形格式，支持索引、<a href="https://zh.wikipedia.org/wiki/灰度">灰度</a>、<a href="https://zh.wikipedia.org/wiki/RGB">RGB</a>三种颜色方案以及<a href="https://zh.wikipedia.org/wiki/Alpha通道">Alpha通道</a>等特性。PNG的开发目标是改善并取代<a href="https://zh.wikipedia.org/wiki/GIF">GIF</a>作为适合网络传输的格式而不需专利许可，所以被广泛应用于<a href="https://zh.wikipedia.org/wiki/互联网">互联网</a>及其他方面上。</p></li></ul><h3 id="Chapter-2-图像分割技术"><a href="#Chapter-2-图像分割技术" class="headerlink" title="Chapter 2 图像分割技术"></a>Chapter 2 图像分割技术</h3><h4 id="2-1-研究背景"><a href="#2-1-研究背景" class="headerlink" title="2.1 研究背景"></a>2.1 研究背景</h4><p>人脸识别、指纹识别、车牌识别</p><p>图像分割是图像处理到图像分析的关键步骤。</p><p><strong>基于阈值的图像分割</strong></p><ul><li>直方图法、迭代法和OTSU算法</li></ul><p><strong>基于区域的图像分割算法</strong></p><ul><li>把具有某种相似性的像素连通，将图像分成很多一致性强的小区域，再按照一定规则将小区域融合成大区域，达到分割图像的目的</li></ul><p><strong>基于边缘检测的图像分割算法</strong></p><ul><li>检测图像的边缘信息实现对图像的分割</li></ul><p><strong>基于特定理论的图像分割算法</strong></p><h4 id="2-2-直方图法"><a href="#2-2-直方图法" class="headerlink" title="2.2 直方图法"></a>2.2 直方图法</h4><ul><li>直方图（Histogram）是在图像中像素灰度级与对应灰度级像素的二维统计关系。</li></ul><h5 id="2-2-1-单阈值分割法"><a href="#2-2-1-单阈值分割法" class="headerlink" title="2.2.1 单阈值分割法"></a>2.2.1 单阈值分割法</h5><ul><li><p>又称<strong>全局阈值法</strong>，即在整幅图中使用一个阈值</p></li><li><p>整幅图像分成两个区域，即目标（黑/白色）和背景（白/黑色）</p><script type="math/tex; mode=display">g(x, y)=\left\{\begin{array}{ll}1 & f(x, y) \geq \mathbb{T} \\0 & f(x, y)<\mathbb{T}\end{array}\right.</script></li><li><p>对于目标和背景对比较明显的图像，其灰度直方图为双峰形状，可选择两峰之前的<strong>波谷对应的像素值</strong>作为全局阈值。</p></li><li>对于物体和背景对不明显的图像，直方图为单峰形状，采用全局阈值法不太合适。</li></ul><h5 id="2-2-2-双阈值分割法"><a href="#2-2-2-双阈值分割法" class="headerlink" title="2.2.2 双阈值分割法"></a>2.2.2 双阈值分割法</h5><script type="math/tex; mode=display">g(x, y)=\left\{\begin{array}{ll}1 & \mathbb{T} 1 \leq f(x, y) \leq \mathbb{T} 2 \\0 & \text { Else }\end{array}\right.</script><p>处理方法：<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n</span><br><span class="line"><span class="keyword">if</span> I_gray(<span class="built_in">i</span>,<span class="built_in">j</span>) &gt; <span class="number">50</span> &amp;&amp; I_gray(<span class="built_in">i</span>,<span class="built_in">j</span>) &lt; <span class="number">150</span></span><br><span class="line">I_BW(<span class="built_in">i</span>,<span class="built_in">j</span>) = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">I_BW(<span class="built_in">i</span>,<span class="built_in">j</span>) = o;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">figure</span>;</span><br><span class="line">imshow(I_BE);</span><br></pre></td></tr></table></figure></p><h5 id="2-2-3-多个阈值分割法"><a href="#2-2-3-多个阈值分割法" class="headerlink" title="2.2.3 多个阈值分割法"></a>2.2.3 多个阈值分割法</h5><script type="math/tex; mode=display">g(x, y)=\left\{\begin{array}{ll}g_{1} & \mathbb{T}  1 \leq f(x, y) \leq \mathbb{T}  2 \\g_{2} & \mathbb{T}  3 \leq f(x, y) \leq \mathbb{T}  4 \\& \ldots \\g_{n} & \text { Else }\end{array}\right.</script><h5 id="2-2-4-半阈值分割法"><a href="#2-2-4-半阈值分割法" class="headerlink" title="2.2.4 半阈值分割法"></a>2.2.4 半阈值分割法</h5><script type="math/tex; mode=display">g(x,y)= \left\{\begin{matrix}   1 & f(x,y) \ge T \\  f(x,y) & f(x,y) < T\end{matrix}\right. \\</script><script type="math/tex; mode=display">g(x,y)= \left \{ \begin{matrix}  f(x,y) & f(x,y) < T \\  255 & f(x,y) \ge 255\end{matrix}\right.</script><h4 id="2-3-迭代法"><a href="#2-3-迭代法" class="headerlink" title="2.3 迭代法"></a>2.3 迭代法</h4><p>迭代法是通过迭代的方法来求最佳阈值，具有一定的自适应性</p><ul><li>步骤一：</li></ul><script type="math/tex; mode=display">T_{1}=\frac{\sum_{i=1}^{m} \sum_{j=1}^{n} I (i, j)}{m \times n}</script><p>设定阈值精度参数 T ，并选取一个初始阈值 T （通常可选取图像平均灰度值）</p><ul><li>步骤二：</li></ul><p>用阈值 T_1分割图像。将图像分成两部分：G_1由灰度值大于T_1的像素组成，G_2由灰度值小于或等于T_1的像素组成。</p><ul><li>步骤三：</li></ul><p>计算G_1和G_2中所有像素的平均值u_1和u_2。</p><ul><li>步骤四：</li></ul><p>计算新的阈值 T_1 = (u_1 + u_2) / 2</p><ul><li>步骤五：</li></ul><p>如果 |T_2 - T_1| &lt; T_0 则推出T_2为最佳阈值；否则，将T_2赋值给T_1，并重复执行步骤直到获得最佳阈值。</p><h4 id="2-4-OSTU法"><a href="#2-4-OSTU法" class="headerlink" title="2.4 OSTU法"></a>2.4 OSTU法</h4><p><strong>OTSU法</strong>是在1980年日本人OTSU提出的，又称为<strong>最大类间方差法</strong>。基本思想是将直方图处理分割成两组，当被分成两类的方差为最大时，决定阈值。<br>当对象和背景的灰度值的差异具有一定大小的时候，OTSU法是很有效的。</p><ul><li>设原始灰度图像灰度级为L，灰度级为i的像素点数为n_i，则图像的全部像素数为:</li></ul><script type="math/tex; mode=display">N = n_{0}+n_{1}+···+n_{L-1}</script><ul><li>归一化直方图：</li></ul><script type="math/tex; mode=display">p_{i}=\frac{n_i}{\N}\\\sum_{i=0}^{L-1}p_{i}=1</script><ul><li>按灰度级用阈值 t 划分为两类：</li></ul><script type="math/tex; mode=display">C_{0}=(0,1,2,···,t)\\C_{1}=(t+1,t+2,···,L-1)</script><ul><li>因此这两类的出现概率以及总均分值分别有下列各式：</li></ul><script type="math/tex; mode=display">ω_{0}=P_{r}(C_{0})=\sum_{i=0}^{t} p_{i}\\ω_{1}=P_{r}(C_{1})=\sum_{i=t+1}^{L-1} p_{i}\\μ_{T}(t)=\sum_{i=0}^{L-1} ip_{i}</script><h4 id="2-5-数字识别"><a href="#2-5-数字识别" class="headerlink" title="2.5 数字识别"></a>2.5 数字识别</h4><ul><li>一种古旧的算法<ul><li>字符轮廓定义：将数字的整体轮廓分解为顶部、底部、左侧和右侧四个方向的轮廓特征来描述。</li><li>结构基元：利用轮廓一阶微分变化趋势，定义构成字符轮廓的 5 个基本基元。竖直、左斜、右斜、圆弧和突变。</li></ul></li></ul><h3 id="Chapter-3-图像水印技术"><a href="#Chapter-3-图像水印技术" class="headerlink" title="Chapter 3 图像水印技术"></a>Chapter 3 图像水印技术</h3><h4 id="3-1-研究背景"><a href="#3-1-研究背景" class="headerlink" title="3.1 研究背景"></a>3.1 研究背景</h4><p>图像水印(Image Watermark)：在不影响图像内容、价值和使用的前提下，通过一定的规则将一些身份识别信息嵌入图像内容中，并且嵌入后的图像不能被人的视觉系统察觉或注意到，只有通过专用的提取算法或检测器才能提取。</p><p>图像水印算法包括：水印生成、水印嵌入和水印检测。</p><ul><li>水印生成</li></ul><p><strong>伪随机序列</strong>：具有类似白噪声的性质，可以人为产生，如混沌序列，二维条形码。</p><p><strong>有特定含义的水印信息</strong>：具有特定含义的字符串或图像。</p><ul><li>水印嵌入</li></ul><p><strong>空间域算法</strong>和<strong>频域算法</strong>，后者是主流。</p><p>水印嵌入前，宿主图像需要通过 DCT 和 DWT 等变换从空域变换到频域。</p><ul><li>水印检测</li></ul><p>水印检测是水印嵌入的逆过程。</p><p><strong>明检测</strong>：在提取水印的过程中需要提供宿主图像。</p><p><strong>盲检测</strong>：在提取水印的过程中不需要宿主图像。该算法是主流，操作方便，实用性强。</p><h4 id="3-2-LSB算法"><a href="#3-2-LSB算法" class="headerlink" title="3.2 LSB算法"></a>3.2 LSB算法</h4><p><strong>最低有效位</strong>（Least Significant Bit, LSB），LSB算法是一种经典的时空域水印算法。</p><p>原理：利用像素值的 8 个比特位对像素值贡献的差异，将水印信息嵌入到像素值的最后 5-8 个位平面。</p><p>PNG文件中的图像像素数一般由RGB三基色（红、绿、蓝）组成。每种颜色占8位，取值范围从<code>0x00</code>到<code>0xFF</code>，即有 256 种颜色，共包含 256 的三次方。因此总共有 <code>16777216</code> 种颜色。</p><p>人眼可以区分大约 1000 万种不同的颜色，这意味着人眼无法区分剩下的 600 万种颜色。LSB隐写是修改 RGB 颜色分量的最低二进制位（LSB），每种颜色会有 8 位，LSB 隐写是修改像素数中的最低位，这种变化前后人眼是不会注意到的，每个像素可以携带 3 位信息。</p><p><strong>Example</strong></p><blockquote><p>PicoCTF_2017: Little School Bus</p></blockquote><p><strong>Description:</strong></p><p>Can you help me find the data in this <a href="https://wiki.bi0s.in/forensics/img/lb.bmp">Little-School-Bus</a>?</p><p><strong>Hint:</strong></p><p>Look at least significant bit encoding!!</p><p><strong>Solution</strong></p><p>As the Hint suggests the problem is related to LSB Encoding, The leftmost digit in binary is called the LSB digit</p><p>As mentioned earlier LSB encoding is done by changing the LSB bit of the colour, however, this slight variation is not noticeable. Thus by changing the LSB bit, we can hide data inside a file.</p><p><code>xxd -b ./littleschoolbus.bmp | head -n 20</code></p><p>Gives,<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">00000000: 01000010 01001101 11100010 01001011 00000010 00000000  BM.K..</span><br><span class="line">00000006: 00000000 00000000 00000000 00000000 00110110 00000000  ....6.</span><br><span class="line">0000000c: 00000000 00000000 00101000 00000000 00000000 00000000  ..(...</span><br><span class="line">00000012: 11111100 00000000 00000000 00000000 11000111 00000000  ......</span><br><span class="line">00000018: 00000000 00000000 00000001 00000000 00011000 00000000  ......</span><br><span class="line">0000001e: 00000000 00000000 00000000 00000000 10101100 01001011  .....K</span><br><span class="line">00000024: 00000010 00000000 00000000 00000000 00000000 00000000  ......</span><br><span class="line">0000002a: 00000000 00000000 00000000 00000000 00000000 00000000  ......</span><br><span class="line">00000030: 00000000 00000000 00000000 00000000 00000000 00000000  ......</span><br><span class="line">00000036: 11111110 11111111 11111111 11111110 11111110 11111111  ......</span><br><span class="line">0000003c: 11111111 11111110 11111110 11111111 11111111 11111110  ......</span><br><span class="line">00000042: 11111111 11111111 11111110 11111110 11111110 11111111  ......</span><br><span class="line">00000048: 11111111 11111110 11111110 11111110 11111110 11111111  ......</span><br><span class="line">0000004e: 11111110 11111111 11111111 11111110 11111110 11111111  ......</span><br><span class="line">00000054: 11111111 11111111 11111110 11111111 11111111 11111111  ......</span><br><span class="line">0000005a: 11111111 11111110 11111111 11111111 11111110 11111111  ......</span><br><span class="line">00000060: 11111111 11111111 11111110 11111110 11111111 11111110  ......</span><br><span class="line">00000066: 11111110 11111111 11111111 11111110 11111110 11111111  ......</span><br><span class="line">0000006c: 11111110 11111111 11111110 11111111 11111111 11111110  ......</span><br><span class="line">00000072: 11111111 11111111 11111110 11111111 11111110 11111111  ......</span><br></pre></td></tr></table></figure><br>Taking the LSB bit after the many zero,<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">00000036: 11111110 11111111 11111111 11111110 11111110 11111111  ......</span><br><span class="line">0000003c: 11111111 11111110 11111110 11111111 11111111 11111110  ......</span><br><span class="line">00000042: 11111111 11111111 11111110 11111110 11111110 11111111  ......</span><br><span class="line">00000048: 11111111 11111110 11111110 11111110 11111110 11111111  ......</span><br></pre></td></tr></table></figure><br>8 bit gives</p><p><code>01100110 01101100</code></p><p>Which in ascii is <code>fl</code>?<br>Now we script ,</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">binary_data = open(&quot;littleschoolbus.bmp&quot;,&quot;rb&quot;) # Open the file binary mode</span><br><span class="line">binary_data.seek(54)  #seek to 54 bytes these bytes does not contain any data</span><br><span class="line">data = binary_data.read() # read the binary data</span><br><span class="line">l = []</span><br><span class="line">for i in data:</span><br><span class="line">    l.append(bin(i)[-1])  #make a list of LSB bit</span><br><span class="line">for i in range(0,500,8):</span><br><span class="line">    print(chr(int(&#x27;&#x27;.join(l[i:i+8]),2)),end=&#x27;&#x27;) # print the character</span><br></pre></td></tr></table></figure><p>Which gives the flag !!<br>flag{remember_kids_protect_your_headers_afb3}</p><h4 id="3-3-DCT算法"><a href="#3-3-DCT算法" class="headerlink" title="3.3 DCT算法"></a>3.3 DCT算法</h4><ol><li>对宿主图像分割成$8*8$的小图像块</li><li>对$8*8$的小块进行 DCT 变换，得系数 B</li><li>对处理后的DCT系数，进行DCT逆变换，得$8*8$的小图像块</li><li>对$8*8$的小块进行组合，得含水印图像</li></ol><h4 id="3-4-Secure-spread-spectrum-watermarking-for-multimedia"><a href="#3-4-Secure-spread-spectrum-watermarking-for-multimedia" class="headerlink" title="3.4 Secure spread spectrum watermarking for multimedia"></a>3.4 Secure spread spectrum watermarking for multimedia</h4><blockquote><p>I. J. Cox, J. Kilian, F. T. Leighton and T. Shamoon, “Secure spread spectrum watermarking for multimedia,” in IEEE Transactions on Image Processing, vol. 6, no. 12, pp. 1673-1687, Dec. 1997, doi: 10.1109/83.650120.</p></blockquote><p>1997年，Ingemar J. Cox等人首次提出了扩频（Spread Spectrum，SS）水印算法, 按照嵌入规则不同, 分为:</p><ol><li>加性扩频（Additive Spread Spectrum，ASS）水印算法</li><li>乘性扩频（Multiplicative Spread Spectrum，MSS）水印算法</li></ol><h4 id="3-5-评价指标"><a href="#3-5-评价指标" class="headerlink" title="3.5 评价指标"></a>3.5 评价指标</h4><ol><li><p><strong>透明性</strong></p><ul><li><strong>客观评价</strong> 峰值信噪比</li><li>Peak Signal-to-Noise Ratio, PSNR</li><li>它是定量客观地评价水印算法透明性的指标</li><li>它反映了水印嵌入前后图像整体的改变程度</li><li>PSNR 越小，则表示图像的质量下降越多</li><li>PSNR 越大，说明水印算法的透明性越好</li></ul></li></ol><blockquote><p>定义</p></blockquote><p>为衡量嵌入水印图像和原始图像之间的差别，定义峰值信噪比为:</p><script type="math/tex; mode=display">  PSNR = 10 \times \lg_{}{\frac{255^2}{\frac{1}{m \times n} \sum_{i=0}^{m-1}\sum_{j=0}^{n-1}(X(i, j) - X_{w}(i, j))^2}}</script><p>$X$是宿主图像, $X_{W}$是含水印图像, 保证$PSNR&gt;38 dB$，是水印透明性基本要求</p><ol><li><strong>鲁棒性</strong></li></ol><p>当含水印图像在经过常规信号处理操作后能够检测出水印的能力。常规操作包括空间滤波、JPEG压缩、剪切攻击、打印与复印、几何变形和噪声攻击等</p><ol><li><strong>安全性</strong></li></ol><p>安全性指水印能够抵抗各种破坏水印功能的行为的能力，即未授权者不能去除、嵌入和检测水印</p><p>同时，水印信息应该很难被他人所复制和伪造</p><ol><li><strong>嵌入容量</strong></li></ol><p>指在一幅图像中能够嵌入水印的数据量</p><h3 id="Chapter-4-图像加密技术"><a href="#Chapter-4-图像加密技术" class="headerlink" title="Chapter 4 图像加密技术"></a>Chapter 4 图像加密技术</h3><h4 id="4-1-基于现代密码体制的图像加密"><a href="#4-1-基于现代密码体制的图像加密" class="headerlink" title="4.1 基于现代密码体制的图像加密"></a>4.1 基于现代密码体制的图像加密</h4><blockquote><p>Wikipedia</p></blockquote><p><strong>密码学</strong>（英语：Cryptography）可分为古典密码学和现代密码学<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">现代密码学大致可被区分为数个领域。对称金钥密码学指的是传送方与接收方都拥有相同的金钥。直到1976年这都还是唯一的公开加密法。</span><br><span class="line"></span><br><span class="line">现代密码学重视区块加密法与串流加密法的研究及应用。区块加密法在某种意义上是阿伯提的多字元加密法的现代化。区块加密法取用明文的一个区块和金钥，输出相同大小的密文区块。由于讯息通常比单一区块还长，因此有了各种方式将连续的区块编织在一起。DES和AES是美国联邦政府核定的区块加密法标准（AES将取代DES）。尽管将从标准上废除，DES依然很流行（三重资料加密演算法变形仍然相当安全），被使用在非常多的应用上，从自动交易机、电子邮件到远端存取。也有许多其他的区块加密被发明、释出，品质与应用上各有不同，其中不乏被破解者。</span><br><span class="line"></span><br><span class="line">串流加密法，相对于区块加密，制造一段任意长的金钥原料，与明文依位元或字元结合，有点类似一次一密密码本（one-time pad）。输出的串流根据加密时的内部状态而定。在一些串流加密法上由金钥控制状态的变化。RC4是相当有名的串流加密法。</span><br><span class="line"></span><br><span class="line">密码杂凑函式（有时称作讯息摘要函式，杂凑函式又称杂凑函式或杂凑函式（Hash））不一定使用到金钥，但和许多重要的密码演算法相关。它将输入资料（通常是一整份档案）输出成较短的固定长度杂凑值，这个过程是单向的，逆向操作难以完成，而且碰撞（两个不同的输入产生相同的杂凑值）发生的机率非常小。</span><br><span class="line"></span><br><span class="line">讯息认证码或押码（Message authentication codes, MACs）很类似密码杂凑函式，除了接收方额外使用秘密金钥来认证杂凑值。</span><br></pre></td></tr></table></figure></p><blockquote><p>Analysis</p></blockquote><p>该方法的安全性体现在</p><ul><li>破译的成本超过加密信息的价值</li><li>破译的时间超过该信息有用的生命周期</li></ul><h4 id="4-2-基于矩阵变换的图像加密"><a href="#4-2-基于矩阵变换的图像加密" class="headerlink" title="4.2 基于矩阵变换的图像加密"></a>4.2 基于矩阵变换的图像加密</h4><blockquote><p>基本原理</p></blockquote><p>对图像矩阵进行有限次的初等矩阵变换，可有效地打乱输入明文的次序，进而有效地掩盖明文信息，达到加密的目的</p><blockquote><p>Arnoldi iteration</p></blockquote><ul><li><p>Arnold矩阵变换的基本思路，将原图像中的点从(x,y)移动到(x’,y’)从而实现了对图像中像素点的移动</p></li><li><p>用Arnold变换算法对图像中所有的像素点进行处理，就完成了一次图像的Arnold变换</p></li><li><p>Arnold 变换具有周期性</p></li><li><p>在网络平台下，图像传输涉及到接收（Bob）、发送（Alice）双方</p></li></ul><blockquote><p>Analysis</p></blockquote><ul><li>算法简单，易于实现，具有周期性</li><li>算法仅置乱像素位置，未改变像素值，所以置乱前后图像的直方图无改变，难以抵制统计攻击</li><li>矩阵变换的周期往往较短，该类算法的密钥空间小，无法抵制密钥穷尽攻击</li><li>在设计算法时，要把图像矩阵变换和其它加密手段有效地结合，在置乱像素位置的同时，进一步改变像素值</li></ul><h4 id="4-3-基于混沌的图像加密"><a href="#4-3-基于混沌的图像加密" class="headerlink" title="4.3 基于混沌的图像加密"></a>4.3 基于混沌的图像加密</h4><blockquote><p>基本原理</p></blockquote><ul><li>混沌是某些非线性系统出现的一种无法精确重复的、貌似随机的运动的现象</li><li>混沌理论、相对论和量子力学一起被称为20世纪物理学的三大的革命</li><li>混沌具有初值和参数敏感性、类随机性、非周期性和不可预测性等特征</li></ul><p><strong>算法原理</strong>：通常把混沌系统的初始值和参数视为密钥，利用密钥和混沌系统产生实数混沌序列，并量化为整数混沌序列。将其与原始图像以某种<strong>可逆</strong>的规则相互作用，实现对图像的加密</p><blockquote><p>常用混沌系统</p></blockquote><ul><li>Logistic映射</li><li>Tent映射</li></ul><h4 id="4-4-基于频域的图像加密"><a href="#4-4-基于频域的图像加密" class="headerlink" title="4.4 基于频域的图像加密"></a>4.4 基于频域的图像加密</h4><blockquote><p>基本原理</p></blockquote><ul><li>按照加密对象不同，图像加密算法可分为空间域加密和变换域加密，它们的加密对象分别为像素和变换系数</li><li>利用离散余弦变换（DCT）、小波变换（DWT）等变换可实现图像空间域和变换域之间的转换</li><li>DCT和DWT变换具有低频功率集中、高频功率较小的特性</li></ul><h4 id="4-5-基于DNA编码的图像加密"><a href="#4-5-基于DNA编码的图像加密" class="headerlink" title="4.5 基于DNA编码的图像加密"></a>4.5 基于DNA编码的图像加密</h4><blockquote><p>基本原理</p></blockquote><p>图像DNA编码时，A编码为00，T编码为01，C编码为10，G编码为11。这种编码组合总共有24个，不过符合$Watson-Crick-Rules$的编码对目前只有8种。</p><p><strong>流程</strong>:<br>在经典框架中，融入DNA编码和DNA运算<br>算法流程1：原始图像→DNA编码→DNA码置乱→DNA运算→DNA解码→加密图像<br>算法流程2：原始图像→像素置乱→ DNA编码→DNA运算→DNA解码→加密图像<br>算法流程3：原始图像→DNA编码→DNA码置乱→DNA解码→像素扩散→加密图像</p><h4 id="4-6-评价指标"><a href="#4-6-评价指标" class="headerlink" title="4.6 评价指标"></a>4.6 评价指标</h4><blockquote><p>密钥空间</p></blockquote><p>密钥空间是指密钥所有可能的取值范围。一个好密码算法的密钥空间应该足够大，以抵抗穷举攻击。目前，密码学界公认密码空间小于$2^{100}$是不安全的</p><blockquote><p>像素相关性</p></blockquote><p>图像的重要特征是相邻像素高度相关，特别是<strong>8个相邻域的像素</strong>。像素相关性指标可反映图像置乱的程度。相邻像素的相关性越大，置乱效果越差，反之置乱效果越好</p><blockquote><p>明文敏感性</p></blockquote><p>为抵制差分攻击，一个好的图像加密算法应该具有很少比特的原始图像改变将影响尽可能多的加密图像的性质</p><blockquote><p>直方图</p></blockquote><p>一个好的图像加密算法，其加密图像的直方图分布十分均匀</p><blockquote><p>密钥敏感性</p></blockquote><p>密钥敏感性指加密密钥的微小变化，将产生两幅完全不同的加密图像，或者解密密钥的微小变化，将导致无法正确解密</p><blockquote><p>信息熵</p></blockquote><p>最理想的加密图像是一个完全随机的图像，即每个灰度级都等概率出现，此时<strong>信息熵为8</strong>。因此，当加密图像的信息熵接近8时，表明加密算法具备足够安全地抵制统计攻击的能力</p><blockquote><p>加密效率</p></blockquote><p>加密耗时不仅跟<strong>图像大小</strong>有关系，而且还与程序具体的<strong>运行环境</strong>（硬件环境，如内存和硬盘大小、CPU主频；软件环境，如系统正在运行的进程数目，采用的编程语言）密切相关</p><h3 id="Chapter-5-中文科技论文检索"><a href="#Chapter-5-中文科技论文检索" class="headerlink" title="Chapter 5 中文科技论文检索"></a>Chapter 5 中文科技论文检索</h3><h4 id="5-1-中国知网"><a href="#5-1-中国知网" class="headerlink" title="5.1 中国知网"></a>5.1 中国知网</h4><ul><li>CNKI</li><li>www.cnki.net</li></ul><h5 id="5-1-1-检索方法"><a href="#5-1-1-检索方法" class="headerlink" title="5.1.1 检索方法"></a>5.1.1 检索方法</h5><ul><li>一框式检索<ul><li>多个检索词需要使用符合运算</li><li>$\times$, $+$, $-$分别表示与或非逻辑运算</li><li>复合运算符与前后两个之间需加空格</li><li>在检索结果中搜索使用“在结果中检索”</li></ul></li><li>高级检索</li><li><p>专业检索</p><ul><li><p>按照特定语法，将检索字段和检索词构成一个检索式进行检索的方式</p></li><li><p>检索字段：</p><p>SU=主题,TI=题名,KY=关键词, AB=摘要FT=全文,AU=作者,FI=第一责任人,RP=通讯作者,AF=机构, JN=文献来源RF=参考文献,YE=年,FU=基金,CLC=分类号,SN=ISSN,CN=统一刊号,IB=ISBN,CF=被引频次</p></li><li><p>匹配运算符</p></li><li><p>比较运算符</p></li></ul></li><li><p>句子检索</p><ul><li>在全文范围内的同一句和同一段话中进行检索，句子检索不支持空检</li></ul></li></ul><h4 id="5-2-万方维普"><a href="#5-2-万方维普" class="headerlink" title="5.2 万方维普"></a>5.2 万方维普</h4><ul><li>Wanfang Data Knowledge Service Platform</li><li>www.wanfangdata.com.cn</li></ul><h5 id="5-2-1-检索方法"><a href="#5-2-1-检索方法" class="headerlink" title="5.2.1 检索方法"></a>5.2.1 检索方法</h5><ul><li><p>一框式检索</p></li><li><p>高级检索</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
          <category> 理论/文化课 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
            <tag> 学术研究 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++程序设计之变量命名指南</title>
      <link href="/2022/02/21/name/"/>
      <url>/2022/02/21/name/</url>
      
        <content type="html"><![CDATA[<h3 id="算法竞赛选手，在变量名使用上，有诸多门派："><a href="#算法竞赛选手，在变量名使用上，有诸多门派：" class="headerlink" title="算法竞赛选手，在变量名使用上，有诸多门派："></a>算法竞赛选手，在<code>变量名</code>使用上，有诸多门派：</h3><ul><li><p>字母派：<code>a</code> <code>b</code> <code>c</code> <code>d</code> <code>e</code> <code>f</code> <code>g</code> 依次使用</p></li><li><p>新字母派：<code>array</code>，<code>brray</code>，<code>crray</code> 表示三个数组</p></li><li><p>新新字母派：<code>img</code>，<code>jmg</code>，<code>kmg</code> 表示三张图片</p></li><li><p>拼音派：用 <code>shanchu</code> 表示删除，用 <code>xiugai</code> 表示修改</p></li><li><p>卖萌派：<code>QAQ</code>，<code>qwq</code>，<code>TAT</code>，<code>orz</code></p></li><li><p>复读派：<code>n</code>，<code>nn</code>，<code>nnn</code></p></li><li><p>重排派：<code>next</code>，<code>extn</code>，<code>xtne</code></p></li><li><p>化合物派：<code>h2o</code>, <code>ch4</code>, <code>co2</code></p></li><li><p>下划线派：<code>_</code>，<code>__</code>，<code>___</code></p></li><li><p>脏话派：<code>f**k</code>，<code>s**t</code>，<code>t*d</code></p></li><li><p>除了循环变量一般用 i，j，k 以外，其它变量名大家就放飞自我了</p></li></ul><p>这样降低了代码可读性，一段时间以后谁也看不懂了，也不利于相互交流</p><p>这里按字母序列出一些推荐使用的变量名，仅供参考，抛砖引玉</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line">add 加</span><br><span class="line">anc, ancestor 祖先</span><br><span class="line">ans, answer 答案</span><br><span class="line">bel, belong 属于</span><br><span class="line">best 最佳的</span><br><span class="line">build 建立</span><br><span class="line">block 障碍</span><br><span class="line">ch, <span class="keyword">char</span> 字符</span><br><span class="line">check 判定</span><br><span class="line">color 颜色</span><br><span class="line">cmp, compare 比较</span><br><span class="line">cnt, count 计数器</span><br><span class="line">cur, current 当前量</span><br><span class="line">deg, degree 度数</span><br><span class="line">dep, depth 深度</span><br><span class="line">del, <span class="keyword">delete</span> 删除</span><br><span class="line">delta 增量</span><br><span class="line">diff, difference 差别</span><br><span class="line">dist, distance 距离</span><br><span class="line">div, division 除法，部分</span><br><span class="line">dp 动态规划</span><br><span class="line">edge 边</span><br><span class="line">extra 额外的</span><br><span class="line">fa, father 父亲</span><br><span class="line">factor 因子</span><br><span class="line">flag 标志</span><br><span class="line">flow 流</span><br><span class="line">from 来自</span><br><span class="line">get 得到</span><br><span class="line">Hash 哈希表（hash是保留字）</span><br><span class="line">heap 堆</span><br><span class="line">in 入</span><br><span class="line">ind, index 标号</span><br><span class="line">inq 在队列里</span><br><span class="line">inf, infinity 无穷大</span><br><span class="line">init, initialize 初始化</span><br><span class="line">insert 插入</span><br><span class="line">inv, inverse 翻转，颠倒</span><br><span class="line">last 最后一个</span><br><span class="line">len, length 长度</span><br><span class="line">lim, limit 极限</span><br><span class="line">low, lower 下边的</span><br><span class="line">mat, matrix 矩阵</span><br><span class="line">mid, middle 中间量</span><br><span class="line">mod 模</span><br><span class="line">modify 修改</span><br><span class="line">mp, map 映射</span><br><span class="line">mst 最小生成树</span><br><span class="line">mul, multiply 乘法</span><br><span class="line">node 结点</span><br><span class="line">num, number 数量</span><br><span class="line">nxt 后继（next是保留字）</span><br><span class="line">out 出</span><br><span class="line">pa, pair 对子</span><br><span class="line">pre, precursor 前驱</span><br><span class="line">prime 质数</span><br><span class="line">pos, position 位置</span><br><span class="line">prod, product 乘积</span><br><span class="line">put 放置</span><br><span class="line">que, queue 队列</span><br><span class="line">query 询问</span><br><span class="line">rank 秩</span><br><span class="line">res, result 结果</span><br><span class="line">res, residual 剩余</span><br><span class="line">scc 强连通分量</span><br><span class="line">size 大小</span><br><span class="line">split 分裂</span><br><span class="line">start 开始</span><br><span class="line">stk, stack 栈</span><br><span class="line">str 字符串</span><br><span class="line">suc, succeed 后继</span><br><span class="line">sum 和</span><br><span class="line">tim 时间（time是保留字）</span><br><span class="line">tmp, temporary 临时量</span><br><span class="line">tree 树</span><br><span class="line">to 表目的</span><br><span class="line">unite 联合</span><br><span class="line">up, upper 上边的</span><br><span class="line">update 更新</span><br><span class="line">used 使用过的</span><br><span class="line">val, value 值</span><br><span class="line">vec, vector 向量</span><br><span class="line">vis, visit 访问</span><br><span class="line">zero 零</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 理论 </category>
          
          <category> 理论/面向对象 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面向对象 </tag>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Continue blogging，2022！</title>
      <link href="/2022/01/31/continue%20blogging,2020!/"/>
      <url>/2022/01/31/continue%20blogging,2020!/</url>
      
        <content type="html"><![CDATA[<p>写于2022/1/31，刚好除夕夜</p><p>因为闲了太多天于是觉得要做点什么，看到服务器打折，想起来博客还托管在 GitHub Pages 上，那就好好整整自己的博客吧。</p><h1 id="Retamev-github-io-的开发日记"><a href="#Retamev-github-io-的开发日记" class="headerlink" title="Retamev.github.io 的开发日记"></a>Retamev.github.io 的开发日记</h1><span id="more"></span><h2 id="主题的选择"><a href="#主题的选择" class="headerlink" title="主题的选择"></a>主题的选择</h2><p><a href="https://camo.githubusercontent.com/d509893a31f8133dddd9a7fb3db3f00c36d8a1a5f6ac74d4072366902ce2bad5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7061636b6167652d6a736f6e2f762f6a65727279633132372f6865786f2d7468656d652d627574746572666c792f6d61737465723f636f6c6f723d253233316162316164266c6162656c3d6d6173746572"><img src="https://camo.githubusercontent.com/d509893a31f8133dddd9a7fb3db3f00c36d8a1a5f6ac74d4072366902ce2bad5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7061636b6167652d6a736f6e2f762f6a65727279633132372f6865786f2d7468656d652d627574746572666c792f6d61737465723f636f6c6f723d253233316162316164266c6162656c3d6d6173746572" alt="master version"></a><a href="https://camo.githubusercontent.com/6560a125081e9201316b73278edc8da47826332ec094f2167f331db6a5bf2c5c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7061636b6167652d6a736f6e2f762f6a65727279633132372f6865786f2d7468656d652d627574746572666c792f6465763f6c6162656c3d646576"><img src="https://camo.githubusercontent.com/6560a125081e9201316b73278edc8da47826332ec094f2167f331db6a5bf2c5c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7061636b6167652d6a736f6e2f762f6a65727279633132372f6865786f2d7468656d652d627574746572666c792f6465763f6c6162656c3d646576" alt="master version"></a><a href="https://camo.githubusercontent.com/53cd517a715f5c131d2d78adffcb196d9ddb11f1ae688e3cc6a480566e2ab9e3/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6865786f2d7468656d652d627574746572666c793f636f6c6f723d253039253233626630306666"><img src="https://camo.githubusercontent.com/53cd517a715f5c131d2d78adffcb196d9ddb11f1ae688e3cc6a480566e2ab9e3/68747470733a2f2f696d672e736869656c64732e696f2f6e706d2f762f6865786f2d7468656d652d627574746572666c793f636f6c6f723d253039253233626630306666" alt="https://img.shields.io/npm/v/hexo-theme-butterfly?color=%09%23bf00ff"></a><a href="https://camo.githubusercontent.com/2fa78a96d51d68835c0ec70b837093a0fac27dda205e23cfebf5a2906d3635e3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6865786f2d352e302b2d3065383363"><img src="https://camo.githubusercontent.com/2fa78a96d51d68835c0ec70b837093a0fac27dda205e23cfebf5a2906d3635e3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6865786f2d352e302b2d3065383363" alt="hexo version"></a><a href="https://camo.githubusercontent.com/773b553badb8b787a302f8ec34b7ca37f67e19efa63806678c5c8795c47303e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a65727279633132372f6865786f2d7468656d652d627574746572666c793f636f6c6f723d464635353331"><img src="https://camo.githubusercontent.com/773b553badb8b787a302f8ec34b7ca37f67e19efa63806678c5c8795c47303e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a65727279633132372f6865786f2d7468656d652d627574746572666c793f636f6c6f723d464635353331" alt="license"></a></p><p>2021 年暑假在 <code>Github</code> 闲逛，对 <code>butterfly</code> 主题一见钟情，加上时用的动态博客维护较为麻烦，便萌生了把博客搬迁到 <code>Hexo</code> 的想法。</p><p>于是最后选择了 <a href="https://hexo.io/zh-cn/">Hexo</a> + <a href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a> 的方案。</p><blockquote><p><strong>什么是 <code>Hexo</code> ？</strong></p><p><code>Hexo</code> 是一个快速的、简单的、功能强大的博客框架。你可以通过 <code>Markdown</code> 语言写文章，然后 <code>Hexo</code> 帮你生成一个带有漂亮主题的静态页面。</p><p><strong>什么是 <code>Butterfly</code> ？</strong></p><p><code>Butterfly</code> 是一个 <code>Hexo</code> 框架下的简单卡片式 UI 设计主题。搭配<code>Node.js</code>和<code>Git</code>(分布式版本控制系统)将主题挂载到你的博客仓库。</p></blockquote><h2 id="Hexo-的安装和后续操作"><a href="#Hexo-的安装和后续操作" class="headerlink" title="Hexo 的安装和后续操作"></a>Hexo 的安装和后续操作</h2><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>借助搜索引擎的帮助安装完 <code>Node</code> 以及 <code>Git</code> 后，新建一个标签为<code>blog</code>的文件夹，在留白处单击鼠标右键选择 <code>Git Bash Here</code> 对根目录执行 <code>Git</code> 操作。</p><p>输入下面的指令安装 <code>Hexo</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>安装完成后可以通过以下指令验证安装，出现版本号即为成功。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>输入以下指令生成基础文件和目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure><p>输入以下指令安装运行<code>hexo</code>的一些必要的组件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure><p>运行完，当前目录下会自动创建一些目录，如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── _config.yml</span><br><span class="line">├── package.json</span><br><span class="line">├── scaffolds/</span><br><span class="line">├── scripts/</span><br><span class="line">├── <span class="built_in">source</span>/</span><br><span class="line">└── themes/</span><br></pre></td></tr></table></figure><ul><li><code>_config.yml</code> 站点配置文件</li><li><code>package.json</code> 应用数据。从它可以看出Hexo版本信息，以及它所默认或者说依赖的一些组件。</li><li><code>scaffolds</code> 模版文件。当你创建一篇新的文章时，Hexo会依据模版文件进行创建，主要用在你想在每篇文章都添加一些共性的内容的情况下。</li><li><code>scripts</code> 用于存放 <code>JavaScript</code> 文件</li><li><code>source</code> 这个文件夹就是放文章的地方了，除了文章还有一些主要的资源，比如文章里的图片，文件等等东西。这个文件夹最好定期做一个备份，丢了它，整个站点就废了。</li><li><code>themes</code> 主题文件夹。</li></ul><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>若要修改网站标题、副标题和邮箱等个人资料，就去修改站点配置文件 <code>_config.yml</code> ，主题支持 <code>EnG</code> <code>zh-CN(简体中文)</code> <code>zh-TW(繁体中文)</code> 三种语言</p><p><img src="https://cdn.jsdelivr.net/gh/jerryc127/CDN/img/20191120000444.png" alt="Profile"></p><ol><li><p>Site 部分，也就是站点的一些参数设置：</p><ul><li><code>title</code> 网站的名字，也就是 HTML 的 title ，会显示在浏览器标签上，如果是博客一般就是写 <code>Reta的博客</code> 这样子啦。</li><li><code>subtitle</code> 站点副标题，会显示在 title 后，可以写一句代表自己的句子（？。</li><li><code>description</code> 站点描述，可以不填…不是什么都要填的！</li><li><code>author</code> 作者，填上自己的标记叭。</li><li><code>language</code> 语言，改成 <code>zh-CN</code> 。</li><li><code>timezone</code> 站点时区，默认是电脑时间，不用管。</li></ul></li><li><p>URL 部分，在这里要链接到自己的 GitHub 仓库</p><ul><li><code>url</code> 站点网址，如果氪金买了域名就填自己的域名，如果和我一样是靠 GitHub 施舍，那就填上网站对应的仓库链接，如 <code>https://retamev.github.io/</code> 。</li></ul></li><li><p>Writing 部分，在这里有一些关于你发布 <code>Post</code> 的设置。</p><ul><li><code>new_post_name</code> 新建文章默认文件名，默认值为 <code>title.md</code> ，比如你执行命令”hexo new hello”，就会默认在 <code>_post</code> 目录下创建一个 <code>hello.md</code> 的文件</li><li><code>future</code> 一个开关，它会决定你的博客中会不会出现来自未来的文字…嘿嘿。</li></ul></li><li><p>Home page setting 部分</p><ul><li><code>per_page</code> 一页显示多少篇文章，0 为不分页，默认值为 10 。</li></ul></li><li><p>Extensions 部分，在这里可以更换<strong>主题</strong>。</p><ul><li><p><code>theme</code> 在这里填上主题的名字，但是在 <code>./node_modules</code> 目录下一定要有主题文件。</p></li><li><p><code>deploy</code> 部署设置，在 <code>type</code> 处填 <code>Git</code> ，在 <code>repo或者repository</code> 处填写 GitHub 生成的链接，在 <code>branch</code> 处填 <code>main</code> 。</p><p><img src="https://s4.ax1x.com/2022/02/01/HFhcSf.png" alt="url"></p></li></ul></li></ol><h3 id="Hexo-常用指令"><a href="#Hexo-常用指令" class="headerlink" title="Hexo 常用指令"></a>Hexo 常用指令</h3><h4 id="简写指令"><a href="#简写指令" class="headerlink" title="简写指令"></a>简写指令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ hexo n <span class="string">&quot;我的第一篇文章&quot;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo new &quot;我的第一篇文章&quot; ，也等价于 $ hexo new post &quot;我的第一篇文章&quot;</span></span><br><span class="line"></span><br><span class="line">$ hexo p </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo publish</span></span><br><span class="line"></span><br><span class="line">$ hexo g </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo generate</span></span><br><span class="line"></span><br><span class="line">$ hexo s </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo server</span></span><br><span class="line"></span><br><span class="line">$ hexo d </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo deploy</span></span><br><span class="line"></span><br><span class="line">$ hexo deploy -g </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo deploy --generate</span></span><br><span class="line"></span><br><span class="line">$ hexo generate -d </span><br><span class="line"></span><br><span class="line"><span class="comment">#等价于 $ hexo generate --deploy</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#注: 以下指令无简写</span></span><br><span class="line"></span><br><span class="line">$ hexo clean</span><br><span class="line"></span><br><span class="line">$ git --version</span><br></pre></td></tr></table></figure><h4 id="指令说明"><a href="#指令说明" class="headerlink" title="指令说明"></a>指令说明</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server </span><br><span class="line"></span><br><span class="line"><span class="comment"># Hexo 会监视文件变动并自动更新，除修改站点配置文件外,无须重启服务器,直接刷新网页即可生效。</span></span><br><span class="line"></span><br><span class="line">$ hexo server -s </span><br><span class="line"></span><br><span class="line"><span class="comment">#以静态模式启动</span></span><br><span class="line"></span><br><span class="line">$ hexo server -p 5000 </span><br><span class="line"></span><br><span class="line"><span class="comment">#更改访问端口 ( 默认端口为 4000，如 localhost:4000 )</span></span><br><span class="line"></span><br><span class="line">$ hexo server -i IP</span><br><span class="line"></span><br><span class="line"><span class="comment">#自定义 IP</span></span><br><span class="line"></span><br><span class="line">$ hexo clean </span><br><span class="line"></span><br><span class="line"><span class="comment">#清除缓存 ,网页正常情况下可以忽略此条命令,执行该指令后,会删掉站点根目录下的public文件夹</span></span><br><span class="line"></span><br><span class="line">$ hexo g </span><br><span class="line"></span><br><span class="line"><span class="comment">#生成静态网页 (执行 $ hexo g后会在站点根目录下生成public文件夹, hexo会将./blog/source/下面的.md后缀的文件编译为.html后缀的文件,存放在./blog/public/路径下)</span></span><br><span class="line"></span><br><span class="line">$ hexo d </span><br><span class="line"></span><br><span class="line"><span class="comment">#将本地数据部署到远端服务器( 如 GitHub )</span></span><br><span class="line"></span><br><span class="line">$ hexo init 文件夹名称 </span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化文件夹名称</span></span><br><span class="line"></span><br><span class="line">$ npm update hexo -g</span><br><span class="line"></span><br><span class="line"><span class="comment">#升级</span></span><br><span class="line"></span><br><span class="line">$ npm install hexo -g</span><br><span class="line"></span><br><span class="line"><span class="comment">#安装</span></span><br><span class="line"></span><br><span class="line">$ node -v </span><br><span class="line"></span><br><span class="line"><span class="comment">#查看node.js版本号</span></span><br><span class="line"></span><br><span class="line">$ npm -v </span><br><span class="line"></span><br><span class="line"><span class="comment">#查看npm版本号</span></span><br><span class="line"></span><br><span class="line">$ git --version </span><br><span class="line"></span><br><span class="line"><span class="comment">#查看git版本号</span></span><br><span class="line"></span><br><span class="line">$ hexo -v </span><br><span class="line"></span><br><span class="line"><span class="comment">#查看hexo版本号</span></span><br><span class="line"></span><br><span class="line">$ hexo publish [layout] &lt;title&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment">#通过 publish 命令将草稿移动到 ./source/_posts 文件夹,如:$ hexo publish [layout] &lt;title&gt;,草稿默认是不会显示在页面中的，可在执行时加上 --draft 参数，或是把 render_drafts 参数设为 true 来预览草稿。</span></span><br></pre></td></tr></table></figure><p>作者暂时睡觉了zzz</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 技术/网站 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>如何写好更新记录？</title>
      <link href="/2021/10/20/hello-world/"/>
      <url>/2021/10/20/hello-world/</url>
      
        <content type="html"><![CDATA[<h3 id="Reta姐姐的小破站ver-2-0"><a href="#Reta姐姐的小破站ver-2-0" class="headerlink" title="Reta姐姐的小破站ver 2.0"></a>Reta姐姐的小破站ver 2.0</h3><p><a href="https://github.com/gohugoio/hugo"><img src="https://blog.coelacanthus.moe/images/hugo_badge.webp" alt="Generator is Hugo"> </a><a href="https://github.com/reuixiy/hugo-theme-meme"><img src="https://blog.coelacanthus.moe/images/meme_badge.webp" alt="Theme is MemE"> </a><a href="https://github.com/CoelacanthusHex/blog"><img src="https://blog.coelacanthus.moe/images/github_badge.webp" alt="Source on GitHub"> </a><a href="https://www.netlify.com/"><img src="https://blog.coelacanthus.moe/images/netlify_badge.webp" alt="Built on Netlify"></a></p><p>之前用 WordPress 搭的小站早就过期了，现在也不好找免费的虚拟主机薅羊毛，遂借用 <code>github.io  + Hexo</code> 搭建个“静态博客”，来记录今后的点滴。</p><p>等有空了就写博文，咕咕咕</p><escape><span id="more"></span></escape><h2 id="初步规划"><a href="#初步规划" class="headerlink" title="初步规划"></a>初步规划</h2><ul><li><p>魔改页面模板的源码</p><ul><li><p>修改文章的摘要摘取机制</p></li><li><p>增加背景图片</p></li></ul></li><li><p>翻一翻站点的配置文件</p></li><li><p>SEO 优化搜索引擎收录</p></li><li><p>侧边栏及导航栏管理</p><ul><li><p>友链</p></li><li><p>社交链接</p></li><li><p>分类</p></li><li><p>关于</p></li></ul></li><li><p>更改文章的标签和分类机制，对所有文章做一遍清洗，适当拆分/合并</p><ul><li><p>分类应有一级分类”/技术” “/日常”和其下的二级分类”/技术/Linux” “/技术/Python应用”</p></li><li><p>标签应侧重于体现文章的关键词</p></li></ul></li><li><p>写一些新博客</p><ul><li><p>将一些总结的内容放到博客上</p></li><li><p>写一些新内容</p></li><li><p>重写了 Friends 页面 </p></li><li><p>写一些 About 页面 </p></li></ul></li></ul><h2 id="更新实况"><a href="#更新实况" class="headerlink" title="更新实况"></a>更新实况</h2><h3 id="2022-1-31"><a href="#2022-1-31" class="headerlink" title="2022/1/31"></a>2022/1/31</h3><ul><li><p>重写了文章，对标签和分类做初步优化</p></li><li><p>数学公式渲染: 使用 marked 或 kramed，都会导致数学公式中的<code>_</code>被识别为斜体。</p><ul><li>解决方案: 卸载掉之前的 hexo 的 markdown 渲染器 marked，然后安装 <a href="https://github.com/hexojs/hexo-renderer-markdown-it">hexo-renderer-markdown-it</a>。如需配置其它参数，请参考 <a href="https://katex.org/docs/options.html">katex 官網</a></li></ul></li><li><p>大致做了下 SEO4</p></li><li><p>萌ICP备20220770号</p></li><li><p>注意到本站 Subtitle 可以设置成有意思的打字效果，于是添加了一些弹幕，供你消磨这几秒钟的加载时间（如果你没看清的话，<a href="retamev.github.io">点我</a>)，自认为这样能解决了本站静态资源较多带来的用户体验问题。</p><ul><li>内容: 瞎侃 + 喜欢的歌词 + 个人碎碎念</li><li>题外话：如果在 Devtools 把所有的 tips 连起来看，感觉还蛮怪的（</li></ul></li><li><p>另外，诸位虎年大吉，忙着写 bug 都忘了看春晚了(?)，还顺便通了个宵，睡觉？下次一定</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 日志 </category>
          
          <category> 日志/更新记录 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 站点日志 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
